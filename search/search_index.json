{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OpenBoost","text":"<p> A GPU-native, all-Python platform for tree-based machine learning. </p> <p> Quickstart \u2022   Models \u2022   NaturalBoost \u2022   API Reference </p>"},{"location":"#why-openboost","title":"Why OpenBoost?","text":"<p>For standard GBDT, use XGBoost/LightGBM\u2014they're highly optimized C++.</p> <p>For GBDT variants (probabilistic predictions, interpretable GAMs, custom algorithms), OpenBoost brings GPU acceleration to methods that were previously CPU-only and slow:</p> <ul> <li>NaturalBoost: 1.3-2x faster than NGBoost</li> <li>OpenBoostGAM: 10-40x faster than InterpretML EBM</li> </ul> <p>Plus: ~20K lines of readable Python. Modify, extend, and build on\u2014no C++ required.</p> XGBoost / LightGBM OpenBoost Code 200K+ lines of C++ ~20K lines of Python GPU Added later Native from day one Customize Modify C++, recompile Modify Python, reload"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import openboost as ob\n\n# Standard gradient boosting\nmodel = ob.GradientBoosting(n_trees=100, max_depth=6)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\n# Probabilistic predictions with uncertainty\nprob_model = ob.NaturalBoostNormal(n_trees=100)\nprob_model.fit(X_train, y_train)\nmean = prob_model.predict(X_test)\nlower, upper = prob_model.predict_interval(X_test, alpha=0.1)  # 90% interval\n</code></pre>"},{"location":"#features","title":"Features","text":""},{"location":"#rocket-gpu-accelerated","title":":rocket: GPU Accelerated","text":"<p>Numba CUDA kernels for histogram building and tree construction. 10-40x faster GAM training compared to CPU alternatives.</p>"},{"location":"#brain-probabilistic-predictions","title":":brain: Probabilistic Predictions","text":"<p>NaturalBoost provides full probability distributions with uncertainty quantification. 8 built-in distributions including Normal, Gamma, Tweedie, and Negative Binomial.</p>"},{"location":"#snake-all-python","title":":snake: All Python","text":"<p>~20K lines of readable, hackable code. No C++ compilation needed. Understand and modify the algorithms.</p>"},{"location":"#gear-sklearn-compatible","title":":gear: sklearn Compatible","text":"<p>Drop-in replacement for scikit-learn pipelines. Works with GridSearchCV, cross_val_score, and Pipeline.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install openboost\n\n# With GPU support\npip install \"openboost[cuda]\"\n</code></pre>"},{"location":"#whats-included","title":"What's Included","text":"Category Models Standard GBDT GradientBoosting, MultiClassGradientBoosting, DART Interpretable OpenBoostGAM, LinearLeafGBDT Probabilistic NaturalBoostNormal, LogNormal, Gamma, Poisson, StudentT, Tweedie, NegBin"},{"location":"#performance","title":"Performance","text":"<p>OpenBoost GPU-accelerates GBDT variants that were previously slow:</p> Benchmark Result NaturalBoost vs NGBoost 1.3-2x faster OpenBoostGAM vs InterpretML EBM 10-40x faster <p>For standard GBDT, XGBoost/LightGBM are faster. OpenBoost's value is in the variants.</p>"},{"location":"#who-is-openboost-for","title":"Who Is OpenBoost For?","text":"<ul> <li>Kaggle Competitors - Probabilistic predictions that XGBoost can't do</li> <li>ML Researchers - Prototype new algorithms in Python</li> <li>Startups - Ship interpretable models fast</li> <li>Students - Actually understand how gradient boosting works</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<p>Train-many optimization: Industry workloads often train many models (hyperparameter tuning, CV, per-segment models). XGBoost optimizes for one model fast. OpenBoost plans to enable native optimization for training many models efficiently.</p>"},{"location":"#license","title":"License","text":"<p>Apache 2.0</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to OpenBoost will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#100-2026-01-20","title":"[1.0.0] - 2026-01-20","text":""},{"location":"changelog/#added","title":"Added","text":""},{"location":"changelog/#core-models","title":"Core Models","text":"<ul> <li><code>GradientBoosting</code> - Standard gradient boosting for regression/classification</li> <li><code>MultiClassGradientBoosting</code> - Multi-class classification with softmax</li> <li><code>DART</code> - Dropout regularized trees</li> <li><code>OpenBoostGAM</code> - GPU-accelerated interpretable GAM</li> </ul>"},{"location":"changelog/#distributional-models-naturalboost","title":"Distributional Models (NaturalBoost)","text":"<ul> <li><code>NaturalBoostNormal</code> - Gaussian distribution</li> <li><code>NaturalBoostLogNormal</code> - Log-normal for positive values</li> <li><code>NaturalBoostGamma</code> - Gamma distribution</li> <li><code>NaturalBoostPoisson</code> - Count data</li> <li><code>NaturalBoostStudentT</code> - Heavy tails</li> <li><code>NaturalBoostTweedie</code> - Insurance claims (Kaggle favorite)</li> <li><code>NaturalBoostNegBin</code> - Sales forecasting (Kaggle favorite)</li> </ul>"},{"location":"changelog/#advanced-features","title":"Advanced Features","text":"<ul> <li><code>LinearLeafGBDT</code> - Linear models in tree leaves</li> <li>GPU acceleration via Numba CUDA</li> <li>Multi-GPU support via Ray</li> <li>GOSS sampling (LightGBM-style)</li> <li>Mini-batch training for out-of-core datasets</li> <li>Memory-mapped array support</li> </ul>"},{"location":"changelog/#sklearn-integration","title":"sklearn Integration","text":"<ul> <li><code>OpenBoostRegressor</code> - sklearn-compatible regressor</li> <li><code>OpenBoostClassifier</code> - sklearn-compatible classifier</li> <li><code>OpenBoostDistributionalRegressor</code> - Distributional regressor</li> <li><code>OpenBoostLinearLeafRegressor</code> - Linear leaf regressor</li> </ul>"},{"location":"changelog/#callbacks","title":"Callbacks","text":"<ul> <li><code>EarlyStopping</code> - Stop training when validation metric stops improving</li> <li><code>Logger</code> - Print training progress</li> <li><code>ModelCheckpoint</code> - Save best models during training</li> <li><code>LearningRateScheduler</code> - Dynamic learning rate</li> </ul>"},{"location":"changelog/#loss-functions","title":"Loss Functions","text":"<ul> <li>MSE, MAE, Huber, Quantile (regression)</li> <li>LogLoss, Softmax (classification)</li> <li>Poisson, Gamma, Tweedie (count/positive data)</li> <li>Custom loss function support</li> </ul>"},{"location":"changelog/#growth-strategies","title":"Growth Strategies","text":"<ul> <li>Level-wise (XGBoost-style)</li> <li>Leaf-wise (LightGBM-style)</li> <li>Symmetric/Oblivious (CatBoost-style)</li> </ul>"},{"location":"changelog/#utilities","title":"Utilities","text":"<ul> <li><code>compute_feature_importances()</code> - Gain-based importance</li> <li><code>suggest_params()</code> - Automatic parameter suggestions</li> <li><code>cross_val_predict()</code> - Out-of-fold predictions</li> <li><code>evaluate_coverage()</code> - Prediction interval validation</li> </ul>"},{"location":"changelog/#performance","title":"Performance","text":"<ul> <li>NaturalBoost: 1.3-11x faster than NGBoost</li> <li>OpenBoostGAM: 25-43x faster than InterpretML EBM on GPU</li> <li>Standard GBDT: Comparable to XGBoost (within 5% RMSE)</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Comprehensive README with examples</li> <li>Quickstart guide</li> <li>Uncertainty quantification tutorial</li> <li>Custom loss function tutorial</li> <li>XGBoost migration guide</li> <li>13 runnable examples</li> </ul>"},{"location":"changelog/#development-phases","title":"Development Phases","text":"<p>This release represents 22 phases of development:</p> <ul> <li>Phase 1-7: Core implementation</li> <li>Phase 8-9: Growth strategies and loss functions</li> <li>Phase 10-11: Feature importance and custom loss</li> <li>Phase 12-14: Callbacks, sklearn integration, regularization</li> <li>Phase 15-16: Distributional GBDT (NaturalBoost)</li> <li>Phase 17-18: Large-scale training, multi-GPU</li> <li>Phase 19-21: Integration testing, CUDA verification</li> <li>Phase 22: Pre-launch QA</li> </ul>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>Get up and running with OpenBoost in 5 minutes.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code># With uv (recommended)\nuv add openboost\n\n# With pip\npip install openboost\n\n# With GPU support\nuv add \"openboost[cuda]\"\npip install \"openboost[cuda]\"\n</code></pre>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"quickstart/#regression","title":"Regression","text":"<pre><code>import numpy as np\nimport openboost as ob\n\n# Generate sample data\nnp.random.seed(42)\nX = np.random.randn(1000, 10).astype(np.float32)\ny = (X[:, 0] * 2 + X[:, 1] + np.random.randn(1000) * 0.1).astype(np.float32)\n\n# Split data\nX_train, X_test = X[:800], X[800:]\ny_train, y_test = y[:800], y[800:]\n\n# Train model\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    max_depth=6,\n    learning_rate=0.1,\n    loss='mse',\n)\nmodel.fit(X_train, y_train)\n\n# Predict\npredictions = model.predict(X_test)\n\n# Evaluate\nrmse = np.sqrt(np.mean((predictions - y_test) ** 2))\nprint(f\"RMSE: {rmse:.4f}\")\n</code></pre>"},{"location":"quickstart/#binary-classification","title":"Binary Classification","text":"<pre><code>import openboost as ob\n\n# Train binary classifier\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    max_depth=6,\n    loss='logloss',\n)\nmodel.fit(X_train, y_train)  # y_train: 0 or 1\n\n# Get raw predictions (logits)\nlogits = model.predict(X_test)\n\n# Convert to probabilities\nprobabilities = 1 / (1 + np.exp(-logits))\n\n# Get class predictions\npredictions = (probabilities &gt; 0.5).astype(int)\n</code></pre>"},{"location":"quickstart/#multi-class-classification","title":"Multi-Class Classification","text":"<pre><code>import openboost as ob\n\n# Train multi-class classifier\nmodel = ob.MultiClassGradientBoosting(\n    n_classes=5,  # Number of classes\n    n_trees=100,\n    max_depth=6,\n)\nmodel.fit(X_train, y_train)  # y_train: 0, 1, 2, 3, or 4\n\n# Get class probabilities\nprobabilities = model.predict_proba(X_test)  # Shape: (n_samples, n_classes)\n\n# Get class predictions\npredictions = model.predict(X_test)  # Shape: (n_samples,)\n</code></pre>"},{"location":"quickstart/#sklearn-compatible-api","title":"sklearn-Compatible API","text":"<p>OpenBoost provides sklearn-compatible wrappers for easy integration:</p> <pre><code>from openboost import OpenBoostRegressor, OpenBoostClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\n# Regressor\nreg = OpenBoostRegressor(n_estimators=100, max_depth=6)\nreg.fit(X_train, y_train)\nprint(f\"R\u00b2 Score: {reg.score(X_test, y_test):.4f}\")\n\n# Classifier\nclf = OpenBoostClassifier(n_estimators=100, max_depth=6)\nclf.fit(X_train, y_train)\nprint(f\"Accuracy: {clf.score(X_test, y_test):.4f}\")\n\n# Cross-validation\nscores = cross_val_score(reg, X, y, cv=5)\nprint(f\"CV Score: {scores.mean():.4f} \u00b1 {scores.std():.4f}\")\n\n# Grid search\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [4, 6, 8],\n    'learning_rate': [0.05, 0.1, 0.2],\n}\ngrid = GridSearchCV(reg, param_grid, cv=3)\ngrid.fit(X_train, y_train)\nprint(f\"Best params: {grid.best_params_}\")\n</code></pre>"},{"location":"quickstart/#uncertainty-quantification","title":"Uncertainty Quantification","text":"<p>OpenBoost's NaturalBoost provides probabilistic predictions:</p> <pre><code>import openboost as ob\n\n# Train probabilistic model\nmodel = ob.NaturalBoostNormal(n_trees=100, max_depth=4)\nmodel.fit(X_train, y_train)\n\n# Point prediction (mean)\nmean = model.predict(X_test)\n\n# 90% prediction interval\nlower, upper = model.predict_interval(X_test, alpha=0.1)\n\n# Full distribution output\noutput = model.predict_distribution(X_test)\nprint(f\"Mean: {output.mean()[:5]}\")\nprint(f\"Std:  {output.std()[:5]}\")\n\n# Sample from predicted distribution\nsamples = model.sample(X_test, n_samples=100)  # Shape: (n_samples, n_test)\n</code></pre>"},{"location":"quickstart/#saving-and-loading-models","title":"Saving and Loading Models","text":"<pre><code>import openboost as ob\n\n# Train\nmodel = ob.GradientBoosting(n_trees=100)\nmodel.fit(X_train, y_train)\n\n# Save\nmodel.save('my_model.joblib')\n\n# Load\nloaded_model = ob.GradientBoosting.load('my_model.joblib')\n\n# Predictions match\nnp.testing.assert_allclose(\n    model.predict(X_test),\n    loaded_model.predict(X_test)\n)\n\n# Also works with pickle/joblib directly\nimport joblib\njoblib.dump(model, 'model.joblib')\nloaded = joblib.load('model.joblib')\n</code></pre>"},{"location":"quickstart/#callbacks-for-training-control","title":"Callbacks for Training Control","text":"<pre><code>import openboost as ob\nfrom openboost import EarlyStopping, Logger\n\n# Early stopping\nmodel = ob.GradientBoosting(n_trees=500, max_depth=6)\n\ncallbacks = [\n    EarlyStopping(patience=10, min_delta=0.001),\n    Logger(every=10),  # Print progress every 10 trees\n]\n\nmodel.fit(\n    X_train, y_train,\n    callbacks=callbacks,\n    eval_set=[(X_test, y_test)],\n)\n\nprint(f\"Stopped at {len(model.trees_)} trees\")\n</code></pre>"},{"location":"quickstart/#feature-importance","title":"Feature Importance","text":"<pre><code>import openboost as ob\n\nmodel = ob.GradientBoosting(n_trees=100)\nmodel.fit(X_train, y_train)\n\n# Compute importance (gain-based)\nimportance = ob.compute_feature_importances(model.trees_)\nprint(\"Feature importances:\", importance)\n\n# Get as dictionary with feature names\nfeature_names = ['age', 'income', 'score', ...]\nimportance_dict = ob.get_feature_importance_dict(model.trees_, feature_names)\n\n# Plot (requires matplotlib)\nob.plot_feature_importances(model.trees_, feature_names)\n</code></pre>"},{"location":"quickstart/#large-scale-training","title":"Large-Scale Training","text":"<p>For datasets that don't fit in memory or need faster training:</p> <pre><code>import openboost as ob\n\n# GOSS sampling (3x faster with minimal accuracy loss)\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    subsample_strategy='goss',\n    goss_top_rate=0.2,    # Keep top 20% high-gradient samples\n    goss_other_rate=0.1,  # Sample 10% of the rest\n)\nmodel.fit(X_train, y_train)\n\n# Memory-mapped arrays for out-of-core training\nX_mmap = ob.create_memmap_binned('data.npy', X_large)\nX_mmap = ob.load_memmap_binned('data.npy', n_features, n_samples)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Uncertainty Quantification Tutorial - Deep dive into NaturalBoost</li> <li>Custom Loss Functions - Define your own loss functions</li> <li>Migration from XGBoost - Switching from XGBoost to OpenBoost</li> </ul>"},{"location":"api/callbacks/","title":"Callbacks","text":"<p>Training callbacks for control and monitoring.</p>"},{"location":"api/callbacks/#available-callbacks","title":"Available Callbacks","text":""},{"location":"api/callbacks/#earlystopping","title":"EarlyStopping","text":""},{"location":"api/callbacks/#openboost.EarlyStopping","title":"EarlyStopping","text":"<pre><code>EarlyStopping(\n    patience=50,\n    min_delta=0.0,\n    restore_best=True,\n    verbose=False,\n)\n</code></pre> <p>               Bases: <code>Callback</code></p> <p>Stop training when validation metric stops improving.</p> <p>Works with ANY model that provides val_loss in TrainingState. Requires <code>eval_set</code> to be passed to <code>fit()</code>.</p> PARAMETER DESCRIPTION <code>patience</code> <p>Number of rounds without improvement before stopping.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> <code>min_delta</code> <p>Minimum change to qualify as an improvement.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>restore_best</code> <p>If True, restore model to best iteration after stopping.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>verbose</code> <p>If True, print message when stopping.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Attributes (after training):     best_score: Best validation score achieved.     best_round: Round at which best score was achieved.     stopped_round: Round at which training was stopped (or None).</p> Example <p>callback = EarlyStopping(patience=50, min_delta=1e-4) model.fit(X, y, callbacks=[callback], eval_set=[(X_val, y_val)]) print(f\"Best round: {model.best_iteration_}\")</p>"},{"location":"api/callbacks/#openboost.EarlyStopping.on_train_begin","title":"on_train_begin","text":"<pre><code>on_train_begin(state)\n</code></pre> <p>Reset state at start of training.</p>"},{"location":"api/callbacks/#openboost.EarlyStopping.on_round_end","title":"on_round_end","text":"<pre><code>on_round_end(state)\n</code></pre> <p>Check if we should stop training.</p>"},{"location":"api/callbacks/#openboost.EarlyStopping.on_train_end","title":"on_train_end","text":"<pre><code>on_train_end(state)\n</code></pre> <p>Restore best model if requested.</p>"},{"location":"api/callbacks/#logger","title":"Logger","text":""},{"location":"api/callbacks/#openboost.Logger","title":"Logger","text":"<pre><code>Logger(period=1, show_train=True, show_val=True)\n</code></pre> <p>               Bases: <code>Callback</code></p> <p>Log training progress to stdout.</p> PARAMETER DESCRIPTION <code>period</code> <p>Print every N rounds (default: 1).</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>show_train</code> <p>Include training loss in output.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_val</code> <p>Include validation loss in output.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Example <p>callback = Logger(period=10)  # Log every 10 rounds model.fit(X, y, callbacks=[callback], eval_set=[(X_val, y_val)]) [0]   train: 0.5234  valid: 0.5456 [10]  train: 0.2134  valid: 0.2345 [20]  train: 0.1234  valid: 0.1456 ...</p>"},{"location":"api/callbacks/#openboost.Logger.on_round_end","title":"on_round_end","text":"<pre><code>on_round_end(state)\n</code></pre> <p>Print progress if at logging period.</p>"},{"location":"api/callbacks/#modelcheckpoint","title":"ModelCheckpoint","text":""},{"location":"api/callbacks/#openboost.ModelCheckpoint","title":"ModelCheckpoint","text":"<pre><code>ModelCheckpoint(\n    filepath, save_best_only=True, verbose=False\n)\n</code></pre> <p>               Bases: <code>Callback</code></p> <p>Save model periodically or when validation score improves.</p> PARAMETER DESCRIPTION <code>filepath</code> <p>Path to save model (use .pkl extension).</p> <p> TYPE: <code>str</code> </p> <code>save_best_only</code> <p>If True, only save when validation improves.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>verbose</code> <p>If True, print message when saving.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Example <p>callback = ModelCheckpoint('best_model.pkl', save_best_only=True) model.fit(X, y, callbacks=[callback], eval_set=[(X_val, y_val)])</p>"},{"location":"api/callbacks/#openboost.ModelCheckpoint.on_round_end","title":"on_round_end","text":"<pre><code>on_round_end(state)\n</code></pre> <p>Save model if conditions are met.</p>"},{"location":"api/callbacks/#learningratescheduler","title":"LearningRateScheduler","text":""},{"location":"api/callbacks/#openboost.LearningRateScheduler","title":"LearningRateScheduler","text":"<pre><code>LearningRateScheduler(schedule)\n</code></pre> <p>               Bases: <code>Callback</code></p> <p>Adjust learning rate during training.</p> PARAMETER DESCRIPTION <code>schedule</code> <p>Function (round_idx) -&gt; learning_rate_multiplier</p> <p> </p> Example"},{"location":"api/callbacks/#openboost.LearningRateScheduler--decay-learning-rate-by-095-each-round","title":"Decay learning rate by 0.95 each round","text":"<p>scheduler = LearningRateScheduler(lambda r: 0.95 ** r) model.fit(X, y, callbacks=[scheduler])</p>"},{"location":"api/callbacks/#openboost.LearningRateScheduler--step-decay-halve-lr-at-round-50-and-100","title":"Step decay: halve LR at round 50 and 100","text":"<p>def step_decay(r): ...     if r &lt; 50: return 1.0 ...     elif r &lt; 100: return 0.5 ...     else: return 0.25 scheduler = LearningRateScheduler(step_decay)</p>"},{"location":"api/callbacks/#openboost.LearningRateScheduler.on_train_begin","title":"on_train_begin","text":"<pre><code>on_train_begin(state)\n</code></pre> <p>Store initial learning rate.</p>"},{"location":"api/callbacks/#openboost.LearningRateScheduler.on_round_begin","title":"on_round_begin","text":"<pre><code>on_round_begin(state)\n</code></pre> <p>Update learning rate for this round.</p>"},{"location":"api/callbacks/#historycallback","title":"HistoryCallback","text":""},{"location":"api/callbacks/#openboost.HistoryCallback","title":"HistoryCallback","text":"<pre><code>HistoryCallback()\n</code></pre> <p>               Bases: <code>Callback</code></p> <p>Record training history (losses per round).</p> <p>Attributes (after training):     history: Dict with 'train_loss' and 'val_loss' lists.</p> Example <p>history = HistoryCallback() model.fit(X, y, callbacks=[history], eval_set=[(X_val, y_val)]) plt.plot(history.history['train_loss'], label='train') plt.plot(history.history['val_loss'], label='valid') plt.legend()</p>"},{"location":"api/callbacks/#openboost.HistoryCallback.on_train_begin","title":"on_train_begin","text":"<pre><code>on_train_begin(state)\n</code></pre> <p>Reset history.</p>"},{"location":"api/callbacks/#openboost.HistoryCallback.on_round_end","title":"on_round_end","text":"<pre><code>on_round_end(state)\n</code></pre> <p>Record losses.</p>"},{"location":"api/callbacks/#base-classes","title":"Base Classes","text":""},{"location":"api/callbacks/#callback","title":"Callback","text":""},{"location":"api/callbacks/#openboost.Callback","title":"Callback","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for training callbacks.</p> <p>Subclass this to create custom callbacks for training hooks. All methods are optional - override only what you need.</p> <p>Example (custom callback):     &gt;&gt;&gt; class GradientTracker(Callback):     ...     def init(self):     ...         self.grad_norms = []     ...        ...     def on_round_end(self, state):     ...         if 'grad_norm' in state.extra:     ...             self.grad_norms.append(state.extra['grad_norm'])     ...         return True     &gt;&gt;&gt;      &gt;&gt;&gt; tracker = GradientTracker()     &gt;&gt;&gt; model.fit(X, y, callbacks=[tracker])     &gt;&gt;&gt; plt.plot(tracker.grad_norms)</p>"},{"location":"api/callbacks/#openboost.Callback.on_train_begin","title":"on_train_begin","text":"<pre><code>on_train_begin(state)\n</code></pre> <p>Called at the start of training.</p> PARAMETER DESCRIPTION <code>state</code> <p>Current training state.</p> <p> TYPE: <code>TrainingState</code> </p>"},{"location":"api/callbacks/#openboost.Callback.on_train_end","title":"on_train_end","text":"<pre><code>on_train_end(state)\n</code></pre> <p>Called at the end of training.</p> PARAMETER DESCRIPTION <code>state</code> <p>Current training state.</p> <p> TYPE: <code>TrainingState</code> </p>"},{"location":"api/callbacks/#openboost.Callback.on_round_begin","title":"on_round_begin","text":"<pre><code>on_round_begin(state)\n</code></pre> <p>Called at the start of each boosting round.</p> PARAMETER DESCRIPTION <code>state</code> <p>Current training state.</p> <p> TYPE: <code>TrainingState</code> </p>"},{"location":"api/callbacks/#openboost.Callback.on_round_end","title":"on_round_end","text":"<pre><code>on_round_end(state)\n</code></pre> <p>Called at the end of each boosting round.</p> PARAMETER DESCRIPTION <code>state</code> <p>Current training state.</p> <p> TYPE: <code>TrainingState</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True to continue training, False to stop early.</p>"},{"location":"api/callbacks/#trainingstate","title":"TrainingState","text":""},{"location":"api/callbacks/#openboost.TrainingState","title":"TrainingState  <code>dataclass</code>","text":"<pre><code>TrainingState(\n    model,\n    round_idx=0,\n    n_rounds=0,\n    train_loss=None,\n    val_loss=None,\n    extra=dict(),\n)\n</code></pre> <p>Shared state passed to callbacks during training.</p> <p>This object is passed to all callbacks at each training event, allowing them to inspect and modify training behavior.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>The model being trained (modified in place).</p> <p> TYPE: <code>Any</code> </p> <code>round_idx</code> <p>Current boosting round (0-indexed).</p> <p> TYPE: <code>int</code> </p> <code>n_rounds</code> <p>Total number of rounds requested.</p> <p> TYPE: <code>int</code> </p> <code>train_loss</code> <p>Training loss for current round (if computed).</p> <p> TYPE: <code>float | None</code> </p> <code>val_loss</code> <p>Validation loss for current round (if eval_set provided).</p> <p> TYPE: <code>float | None</code> </p> <code>extra</code> <p>Dict for custom data (research callbacks can use this).</p> <p> TYPE: <code>dict</code> </p>"},{"location":"api/callbacks/#callbackmanager","title":"CallbackManager","text":""},{"location":"api/callbacks/#openboost.CallbackManager","title":"CallbackManager","text":"<pre><code>CallbackManager(callbacks=None)\n</code></pre> <p>Orchestrates multiple callbacks.</p> <p>Used internally by training loops to manage callback execution.</p> PARAMETER DESCRIPTION <code>callbacks</code> <p>List of Callback instances.</p> <p> TYPE: <code>list[Callback] | None</code> DEFAULT: <code>None</code> </p>"},{"location":"api/callbacks/#openboost.CallbackManager.on_train_begin","title":"on_train_begin","text":"<pre><code>on_train_begin(state)\n</code></pre> <p>Call on_train_begin for all callbacks.</p>"},{"location":"api/callbacks/#openboost.CallbackManager.on_round_begin","title":"on_round_begin","text":"<pre><code>on_round_begin(state)\n</code></pre> <p>Call on_round_begin for all callbacks.</p>"},{"location":"api/callbacks/#openboost.CallbackManager.on_round_end","title":"on_round_end","text":"<pre><code>on_round_end(state)\n</code></pre> <p>Call on_round_end for all callbacks.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if training should continue, False if any callback wants to stop.</p>"},{"location":"api/callbacks/#openboost.CallbackManager.on_train_end","title":"on_train_end","text":"<pre><code>on_train_end(state)\n</code></pre> <p>Call on_train_end for all callbacks.</p>"},{"location":"api/distributions/","title":"Distributions","text":"<p>Probability distributions for NaturalBoost.</p>"},{"location":"api/distributions/#built-in-distributions","title":"Built-in Distributions","text":""},{"location":"api/distributions/#normal","title":"Normal","text":""},{"location":"api/distributions/#openboost.Normal","title":"Normal","text":"<p>               Bases: <code>Distribution</code></p> <p>Normal (Gaussian) distribution.</p> PARAMETER DESCRIPTION <code>loc</code> <p>Mean, unbounded</p> <p> TYPE: <code>\u03bc</code> </p> <code>scale</code> <p>Standard deviation, must be positive</p> <p> TYPE: <code>\u03c3</code> </p> Link functions <p>loc: identity (unbounded) scale: exp (ensures \u03c3 &gt; 0)</p> <p>PDF: p(y) = (1/\u221a(2\u03c0\u03c3\u00b2)) exp(-(y-\u03bc)\u00b2/(2\u03c3\u00b2)) NLL: 0.5 * log(2\u03c0\u03c3\u00b2) + (y-\u03bc)\u00b2/(2\u03c3\u00b2)</p>"},{"location":"api/distributions/#openboost.Normal.init_params","title":"init_params","text":"<pre><code>init_params(y)\n</code></pre> <p>Initialize with sample mean and std.</p>"},{"location":"api/distributions/#openboost.Normal.nll_gradient","title":"nll_gradient","text":"<pre><code>nll_gradient(y, params)\n</code></pre> <p>Compute gradients of NLL w.r.t. raw parameters.</p> <p>NLL = 0.5 * log(2\u03c0\u03c3\u00b2) + (y - \u03bc)\u00b2 / (2\u03c3\u00b2)</p> <p>For loc (identity link):     d(NLL)/d\u03bc = -(y - \u03bc) / \u03c3\u00b2     d\u00b2(NLL)/d\u03bc\u00b2 = 1 / \u03c3\u00b2</p> <p>For scale with exp link (\u03c3 = exp(s)):     d(NLL)/ds = 1 - (y - \u03bc)\u00b2 / \u03c3\u00b2     d\u00b2(NLL)/ds\u00b2 \u2248 2 (expected hessian at optimum)</p>"},{"location":"api/distributions/#openboost.Normal.fisher_information","title":"fisher_information","text":"<pre><code>fisher_information(params)\n</code></pre> <p>Fisher information matrix for Normal distribution.</p> <p>For Normal with exp link on scale: F = [[1/\u03c3\u00b2, 0   ],      [0,    2   ]]</p> <p>The off-diagonal is 0 because mean and variance are orthogonal parameters in the normal family.</p>"},{"location":"api/distributions/#openboost.Normal.quantile","title":"quantile","text":"<pre><code>quantile(params, q)\n</code></pre> <p>Quantile of Normal distribution.</p>"},{"location":"api/distributions/#openboost.Normal.sample","title":"sample","text":"<pre><code>sample(params, n_samples=1, seed=None)\n</code></pre> <p>Sample from Normal distribution.</p>"},{"location":"api/distributions/#openboost.Normal.nll","title":"nll","text":"<pre><code>nll(y, params)\n</code></pre> <p>Negative log-likelihood.</p>"},{"location":"api/distributions/#lognormal","title":"LogNormal","text":""},{"location":"api/distributions/#openboost.LogNormal","title":"LogNormal","text":"<p>               Bases: <code>Distribution</code></p> <p>Log-Normal distribution for positive continuous data.</p> <p>If X ~ LogNormal(\u03bc, \u03c3), then log(X) ~ Normal(\u03bc, \u03c3).</p> PARAMETER DESCRIPTION <code>loc</code> <p>Mean of underlying normal</p> <p> TYPE: <code>\u03bc</code> </p> <code>scale</code> <p>Std of underlying normal (must be positive)</p> <p> TYPE: <code>\u03c3</code> </p> Link functions <p>loc: identity scale: exp</p> <p>Mean: exp(\u03bc + \u03c3\u00b2/2) Variance: (exp(\u03c3\u00b2) - 1) * exp(2\u03bc + \u03c3\u00b2)</p>"},{"location":"api/distributions/#openboost.LogNormal.init_params","title":"init_params","text":"<pre><code>init_params(y)\n</code></pre> <p>Initialize from positive target values.</p>"},{"location":"api/distributions/#openboost.LogNormal.nll_gradient","title":"nll_gradient","text":"<pre><code>nll_gradient(y, params)\n</code></pre> <p>Gradients for LogNormal.</p> <p>NLL = log(y) + 0.5*log(2\u03c0\u03c3\u00b2) + (log(y) - \u03bc)\u00b2/(2\u03c3\u00b2)</p> <p>Same gradients as Normal but with log(y) as target.</p>"},{"location":"api/distributions/#openboost.LogNormal.fisher_information","title":"fisher_information","text":"<pre><code>fisher_information(params)\n</code></pre> <p>Same as Normal (parameters are for underlying normal).</p>"},{"location":"api/distributions/#gamma","title":"Gamma","text":""},{"location":"api/distributions/#openboost.Gamma","title":"Gamma","text":"<p>               Bases: <code>Distribution</code></p> <p>Gamma distribution for positive continuous data.</p> <p>Parameterization: shape (\u03b1) and rate (\u03b2) - Mean = \u03b1/\u03b2 - Variance = \u03b1/\u03b2\u00b2</p> PARAMETER DESCRIPTION <code>concentration</code> <p>Shape parameter, must be positive</p> <p> TYPE: <code>\u03b1</code> </p> <code>rate</code> <p>Rate parameter, must be positive</p> <p> TYPE: <code>\u03b2</code> </p> <p>Link functions: exp for both (ensure positivity)</p> <p>Alternative: Can also be parameterized by mean and dispersion.</p>"},{"location":"api/distributions/#openboost.Gamma.init_params","title":"init_params","text":"<pre><code>init_params(y)\n</code></pre> <p>Initialize using method of moments.</p> <p>mean = \u03b1/\u03b2, var = \u03b1/\u03b2\u00b2 =&gt; \u03b2 = mean/var, \u03b1 = mean * \u03b2 = mean\u00b2/var</p>"},{"location":"api/distributions/#openboost.Gamma.nll_gradient","title":"nll_gradient","text":"<pre><code>nll_gradient(y, params)\n</code></pre> <p>Gradients for Gamma distribution.</p> <p>NLL = -\u03b1log(\u03b2) + log(\u0393(\u03b1)) - (\u03b1-1)log(y) + \u03b2*y</p> <p>d(NLL)/d\u03b1 = -log(\u03b2) + \u03c8(\u03b1) - log(y) d(NLL)/d\u03b2 = -\u03b1/\u03b2 + y</p> <p>With exp links (\u03b1 = exp(a), \u03b2 = exp(b)): d(NLL)/da = \u03b1 * (-log(\u03b2) + \u03c8(\u03b1) - log(y)) d(NLL)/db = \u03b2 * (-\u03b1/\u03b2 + y) = -\u03b1 + \u03b2*y</p>"},{"location":"api/distributions/#openboost.Gamma.fisher_information","title":"fisher_information","text":"<pre><code>fisher_information(params)\n</code></pre> <p>Fisher information for Gamma (with exp links).</p>"},{"location":"api/distributions/#poisson","title":"Poisson","text":""},{"location":"api/distributions/#openboost.Poisson","title":"Poisson","text":"<p>               Bases: <code>Distribution</code></p> <p>Poisson distribution for count data.</p> <p>Single parameter: rate (\u03bb) - Mean = \u03bb - Variance = \u03bb</p> <p>Link function: exp (ensures \u03bb &gt; 0)</p>"},{"location":"api/distributions/#openboost.Poisson.nll_gradient","title":"nll_gradient","text":"<pre><code>nll_gradient(y, params)\n</code></pre> <p>Gradients for Poisson.</p> <p>NLL = \u03bb - y*log(\u03bb) + log(y!) d(NLL)/d\u03bb = 1 - y/\u03bb</p> <p>With exp link (\u03bb = exp(l)): d(NLL)/dl = \u03bb - y d\u00b2(NLL)/dl\u00b2 = \u03bb</p>"},{"location":"api/distributions/#openboost.Poisson.fisher_information","title":"fisher_information","text":"<pre><code>fisher_information(params)\n</code></pre> <p>Fisher information for Poisson: F = \u03bb.</p>"},{"location":"api/distributions/#studentt","title":"StudentT","text":""},{"location":"api/distributions/#openboost.StudentT","title":"StudentT","text":"<p>               Bases: <code>Distribution</code></p> <p>Student-t distribution for heavy-tailed data.</p> PARAMETER DESCRIPTION <code>loc</code> <p>Location parameter</p> <p> TYPE: <code>\u03bc</code> </p> <code>scale</code> <p>Scale parameter (positive)</p> <p> TYPE: <code>\u03c3</code> </p> <code>df</code> <p>Degrees of freedom (positive, typically &gt; 2)</p> <p> TYPE: <code>\u03bd</code> </p> <p>For \u03bd \u2192 \u221e, approaches Normal distribution. Lower \u03bd = heavier tails.</p> Link functions <p>loc: identity scale: exp df: softplus (ensures &gt; 0, typically &gt; 2)</p>"},{"location":"api/distributions/#openboost.StudentT.nll_gradient","title":"nll_gradient","text":"<pre><code>nll_gradient(y, params)\n</code></pre> <p>Gradients for Student-t (simplified, using expected hessians).</p>"},{"location":"api/distributions/#openboost.StudentT.fisher_information","title":"fisher_information","text":"<pre><code>fisher_information(params)\n</code></pre> <p>Fisher information for Student-t (diagonal approximation).</p>"},{"location":"api/distributions/#tweedie","title":"Tweedie","text":""},{"location":"api/distributions/#openboost.Tweedie","title":"Tweedie","text":"<pre><code>Tweedie(power=1.5)\n</code></pre> <p>               Bases: <code>Distribution</code></p> <p>Tweedie distribution for zero-inflated positive continuous data.</p> <p>Key use case: Insurance claims, revenue forecasting with zeros.</p> <p>Popular in Kaggle competitions: - Porto Seguro Safe Driver Prediction - Allstate Claims Severity - Any competition with zero-inflated positive targets</p> <p>The Tweedie distribution is a compound Poisson-Gamma: - \u03c1 = 1: Poisson (count data) - 1 &lt; \u03c1 &lt; 2: Compound Poisson-Gamma (zeros + positive continuous) - \u03c1 = 2: Gamma (positive continuous)</p> PARAMETER DESCRIPTION <code>mu</code> <p>Mean parameter (positive)</p> <p> TYPE: <code>\u03bc</code> </p> <code>phi</code> <p>Dispersion parameter (positive)</p> <p> TYPE: <code>\u03c6</code> </p> <p>Why better than XGBoost? - XGBoost Tweedie only outputs point estimates - NGBoost Tweedie outputs full distribution \u2192 prediction intervals,   uncertainty quantification, probabilistic forecasts</p> Link functions <p>mu: log (ensures \u03bc &gt; 0) phi: log (ensures \u03c6 &gt; 0)</p> <p>Initialize Tweedie with power parameter.</p> PARAMETER DESCRIPTION <code>power</code> <p>Variance power (1 &lt; power &lt; 2 for compound Poisson-Gamma)    1.5 is the default used in most Kaggle competitions.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.5</code> </p>"},{"location":"api/distributions/#openboost.Tweedie.init_params","title":"init_params","text":"<pre><code>init_params(y)\n</code></pre> <p>Initialize from target values.</p> <p>For Tweedie, \u03bc = E[Y], and \u03c6 is estimated from variance.</p>"},{"location":"api/distributions/#openboost.Tweedie.nll_gradient","title":"nll_gradient","text":"<pre><code>nll_gradient(y, params)\n</code></pre> <p>Gradients for Tweedie distribution.</p> <p>Using the deviance formulation (standard in GLMs).</p> <p>For Tweedie with power \u03c1: d(NLL)/d\u03bc = (\u03bc^(1-\u03c1) - y*\u03bc^(-\u03c1)) / \u03c6</p>"},{"location":"api/distributions/#openboost.Tweedie.fisher_information","title":"fisher_information","text":"<pre><code>fisher_information(params)\n</code></pre> <p>Fisher information for Tweedie.</p>"},{"location":"api/distributions/#openboost.Tweedie.quantile","title":"quantile","text":"<pre><code>quantile(params, q)\n</code></pre> <p>Approximate quantile using Normal approximation.</p>"},{"location":"api/distributions/#openboost.Tweedie.sample","title":"sample","text":"<pre><code>sample(params, n_samples=1, seed=None)\n</code></pre> <p>Sample from Tweedie using compound Poisson-Gamma.</p>"},{"location":"api/distributions/#openboost.Tweedie.nll","title":"nll","text":"<pre><code>nll(y, params)\n</code></pre> <p>Negative log-likelihood (deviance-based).</p>"},{"location":"api/distributions/#negativebinomial","title":"NegativeBinomial","text":""},{"location":"api/distributions/#openboost.NegativeBinomial","title":"NegativeBinomial","text":"<p>               Bases: <code>Distribution</code></p> <p>Negative Binomial distribution for overdispersed count data.</p> <p>Key use case: Sales forecasting, demand prediction, click counts.</p> <p>Popular in Kaggle competitions: - Rossmann Store Sales - Bike Sharing Demand - Grupo Bimbo Inventory Demand - Any competition with count data where variance &gt; mean</p> <p>Compared to Poisson: - Poisson: Var(Y) = Mean(Y) - NegBin: Var(Y) = Mean(Y) + Mean(Y)\u00b2/r  (overdispersion)</p> PARAMETER DESCRIPTION <code>mu</code> <p>Mean parameter (positive)</p> <p> TYPE: <code>\u03bc</code> </p> <code>r</code> <p>Dispersion parameter (positive, smaller = more overdispersion)</p> <p> </p> <p>Why better than XGBoost? - XGBoost can't output count distributions at all - NGBoost NegBin outputs full distribution \u2192 prediction intervals,   probability of exceeding thresholds, demand planning</p> Link functions <p>mu: log (ensures \u03bc &gt; 0) r: log (ensures r &gt; 0)</p>"},{"location":"api/distributions/#openboost.NegativeBinomial.init_params","title":"init_params","text":"<pre><code>init_params(y)\n</code></pre> <p>Initialize using method of moments.</p> <p>Mean = \u03bc Var = \u03bc + \u03bc\u00b2/r =&gt; r = \u03bc\u00b2 / (Var - \u03bc)</p>"},{"location":"api/distributions/#openboost.NegativeBinomial.nll_gradient","title":"nll_gradient","text":"<pre><code>nll_gradient(y, params)\n</code></pre> <p>Gradients for Negative Binomial.</p> <p>NLL = -log \u0393(y+r) + log \u0393(r) + log \u0393(y+1)        - rlog(r/(r+\u03bc)) - ylog(\u03bc/(r+\u03bc))</p>"},{"location":"api/distributions/#openboost.NegativeBinomial.fisher_information","title":"fisher_information","text":"<pre><code>fisher_information(params)\n</code></pre> <p>Fisher information for Negative Binomial.</p>"},{"location":"api/distributions/#openboost.NegativeBinomial.prob_exceed","title":"prob_exceed","text":"<pre><code>prob_exceed(params, threshold)\n</code></pre> <p>Probability that Y &gt; threshold.</p> <p>Very useful for demand planning: \"What's the probability we need  more than 100 units?\"</p>"},{"location":"api/distributions/#custom-distributions","title":"Custom Distributions","text":""},{"location":"api/distributions/#create_custom_distribution","title":"create_custom_distribution","text":""},{"location":"api/distributions/#openboost.create_custom_distribution","title":"create_custom_distribution","text":"<pre><code>create_custom_distribution(\n    param_names,\n    link_functions,\n    nll_fn,\n    mean_fn=None,\n    variance_fn=None,\n)\n</code></pre> <p>Convenience function to create a custom distribution.</p> <p>Example: Model y ~ Normal(A * exp(-B*x_feature), sigma)</p> <pre><code>&gt;&gt;&gt; dist = create_custom_distribution(\n...     param_names=['A', 'B', 'sigma'],\n...     link_functions={'A': 'exp', 'B': 'softplus', 'sigma': 'exp'},\n...     nll_fn=lambda y, p: 0.5*np.log(2*np.pi*p['sigma']**2) + (y-p['A'])**2/(2*p['sigma']**2),\n...     mean_fn=lambda p: p['A'],\n...     variance_fn=lambda p: p['sigma']**2,\n... )\n</code></pre>"},{"location":"api/distributions/#customdistribution","title":"CustomDistribution","text":""},{"location":"api/distributions/#openboost.CustomDistribution","title":"CustomDistribution","text":"<pre><code>CustomDistribution(\n    param_names,\n    link_functions,\n    nll_fn,\n    mean_fn=None,\n    variance_fn=None,\n    init_fn=None,\n    use_jax=True,\n    eps=1e-05,\n)\n</code></pre> <p>               Bases: <code>Distribution</code></p> <p>User-defined distribution with automatic gradient computation.</p> <p>Define any parametric distribution by specifying: 1. Parameter names and link functions 2. Negative log-likelihood function</p> <p>Gradients are computed automatically via: - JAX (if available) - fastest - Numerical differentiation (fallback)</p> <p>Example: Custom \"ratio\" distribution y ~ Normal(A*(1-B)/C, \u03c3)</p> <pre><code>&gt;&gt;&gt; def my_nll(y, params):\n...     A, B, C, sigma = params['A'], params['B'], params['C'], params['sigma']\n...     mu = A * (1 - B) / C\n...     return 0.5 * np.log(2 * np.pi * sigma**2) + (y - mu)**2 / (2 * sigma**2)\n&gt;&gt;&gt; \n&gt;&gt;&gt; dist = CustomDistribution(\n...     param_names=['A', 'B', 'C', 'sigma'],\n...     link_functions={\n...         'A': 'identity',      # A \u2208 (-\u221e, \u221e)\n...         'B': 'sigmoid',       # B \u2208 (0, 1)\n...         'C': 'softplus',      # C &gt; 0\n...         'sigma': 'exp',       # \u03c3 &gt; 0\n...     },\n...     nll_fn=my_nll,\n...     mean_fn=lambda params: params['A'] * (1 - params['B']) / params['C'],\n... )\n&gt;&gt;&gt; \n&gt;&gt;&gt; model = NGBoost(distribution=dist, n_trees=100)\n&gt;&gt;&gt; model.fit(X, y)\n</code></pre> <p>For Kaggle competitions with custom evaluation metrics, you can define the NLL to match the competition metric!</p> <p>Initialize custom distribution.</p> PARAMETER DESCRIPTION <code>param_names</code> <p>List of parameter names (e.g., ['A', 'B', 'sigma'])</p> <p> TYPE: <code>list[str]</code> </p> <code>link_functions</code> <p>Dict mapping param name to link type: - 'identity': no transformation, param \u2208 (-\u221e, \u221e) - 'exp': exponential, param &gt; 0 - 'softplus': log(1 + exp(x)), param &gt; 0 (smoother than exp) - 'sigmoid': 1/(1+exp(-x)), param \u2208 (0, 1) - 'square': x\u00b2, param \u2265 0</p> <p> TYPE: <code>dict[str, str]</code> </p> <code>nll_fn</code> <p>Function (y, params_dict) -&gt; array of NLL per sample</p> <p> TYPE: <code>callable</code> </p> <code>mean_fn</code> <p>Optional function (params_dict) -&gt; mean prediction</p> <p> TYPE: <code>callable | None</code> DEFAULT: <code>None</code> </p> <code>variance_fn</code> <p>Optional function (params_dict) -&gt; variance</p> <p> TYPE: <code>callable | None</code> DEFAULT: <code>None</code> </p> <code>init_fn</code> <p>Optional function (y) -&gt; dict of initial raw param values</p> <p> TYPE: <code>callable | None</code> DEFAULT: <code>None</code> </p> <code>use_jax</code> <p>Try to use JAX for autodiff (falls back to numerical if unavailable)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>eps</code> <p>Epsilon for numerical gradients</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-05</code> </p>"},{"location":"api/distributions/#openboost.CustomDistribution.nll_gradient","title":"nll_gradient","text":"<pre><code>nll_gradient(y, params)\n</code></pre> <p>Compute gradients (auto-selects JAX or numerical).</p>"},{"location":"api/distributions/#openboost.CustomDistribution.fisher_information","title":"fisher_information","text":"<pre><code>fisher_information(params)\n</code></pre> <p>Approximate Fisher information (diagonal).</p>"},{"location":"api/distributions/#openboost.CustomDistribution.quantile","title":"quantile","text":"<pre><code>quantile(params, q)\n</code></pre> <p>Approximate quantile using Normal assumption.</p>"},{"location":"api/distributions/#openboost.CustomDistribution.sample","title":"sample","text":"<pre><code>sample(params, n_samples=1, seed=None)\n</code></pre> <p>Sample using Normal approximation.</p>"},{"location":"api/distributions/#utilities","title":"Utilities","text":""},{"location":"api/distributions/#get_distribution","title":"get_distribution","text":""},{"location":"api/distributions/#openboost.get_distribution","title":"get_distribution","text":"<pre><code>get_distribution(name)\n</code></pre> <p>Get distribution by name or return instance.</p> PARAMETER DESCRIPTION <code>name</code> <p>Distribution name or Distribution instance</p> <p> TYPE: <code>str | Distribution</code> </p> RETURNS DESCRIPTION <code>Distribution</code> <p>Distribution instance</p> Example <p>dist = get_distribution('normal') dist = get_distribution('gamma')</p>"},{"location":"api/distributions/#list_distributions","title":"list_distributions","text":""},{"location":"api/distributions/#openboost.list_distributions","title":"list_distributions","text":"<pre><code>list_distributions()\n</code></pre> <p>List available distribution names.</p>"},{"location":"api/distributions/#base-classes","title":"Base Classes","text":""},{"location":"api/distributions/#distribution","title":"Distribution","text":""},{"location":"api/distributions/#openboost.Distribution","title":"Distribution","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for probability distributions.</p> <p>Subclasses must implement: - n_params: Number of distributional parameters - param_names: Names of parameters - link: Transform raw -&gt; constrained parameter space - link_inv: Transform constrained -&gt; raw - nll_gradient: Gradient and hessian of NLL w.r.t. raw parameters - fisher_information: Fisher information matrix (for NGBoost)</p>"},{"location":"api/distributions/#openboost.Distribution.n_params","title":"n_params  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>n_params\n</code></pre> <p>Number of distributional parameters.</p>"},{"location":"api/distributions/#openboost.Distribution.param_names","title":"param_names  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>param_names\n</code></pre> <p>Names of parameters, e.g., ['loc', 'scale'].</p>"},{"location":"api/distributions/#openboost.Distribution.link","title":"link  <code>abstractmethod</code>","text":"<pre><code>link(param_name, raw)\n</code></pre> <p>Apply link function: raw -&gt; constrained parameter space.</p> <p>E.g., for scale: exp(raw) to ensure positivity.</p> PARAMETER DESCRIPTION <code>param_name</code> <p>Name of the parameter</p> <p> TYPE: <code>str</code> </p> <code>raw</code> <p>Raw (unbounded) values</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>Constrained parameter values</p>"},{"location":"api/distributions/#openboost.Distribution.link_inv","title":"link_inv  <code>abstractmethod</code>","text":"<pre><code>link_inv(param_name, param)\n</code></pre> <p>Inverse link: constrained -&gt; raw (for initialization).</p> PARAMETER DESCRIPTION <code>param_name</code> <p>Name of the parameter</p> <p> TYPE: <code>str</code> </p> <code>param</code> <p>Constrained parameter values</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>Raw (unbounded) values</p>"},{"location":"api/distributions/#openboost.Distribution.nll_gradient","title":"nll_gradient  <code>abstractmethod</code>","text":"<pre><code>nll_gradient(y, params)\n</code></pre> <p>Compute gradient and hessian of NLL w.r.t. each RAW parameter.</p> <p>The gradient is d(NLL)/d(raw), accounting for the link function.</p> PARAMETER DESCRIPTION <code>y</code> <p>Observed target values</p> <p> TYPE: <code>NDArray</code> </p> <code>params</code> <p>Dictionary of constrained parameter values</p> <p> TYPE: <code>dict[str, NDArray]</code> </p> RETURNS DESCRIPTION <code>dict[str, GradHess]</code> <p>Dictionary mapping param_name -&gt; (gradient, hessian)</p>"},{"location":"api/distributions/#openboost.Distribution.fisher_information","title":"fisher_information  <code>abstractmethod</code>","text":"<pre><code>fisher_information(params)\n</code></pre> <p>Fisher information matrix at given parameters.</p> <p>Shape: (n_samples, n_params, n_params) Used for natural gradient computation in NGBoost.</p> PARAMETER DESCRIPTION <code>params</code> <p>Dictionary of constrained parameter values</p> <p> TYPE: <code>dict[str, NDArray]</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>Fisher information matrix</p>"},{"location":"api/distributions/#openboost.Distribution.natural_gradient","title":"natural_gradient","text":"<pre><code>natural_gradient(y, params)\n</code></pre> <p>Compute natural gradient: F^{-1} @ ordinary_gradient.</p> <p>Natural gradient accounts for the geometry of the parameter space, leading to faster convergence. This is the key insight of NGBoost.</p> PARAMETER DESCRIPTION <code>y</code> <p>Observed target values</p> <p> TYPE: <code>NDArray</code> </p> <code>params</code> <p>Dictionary of constrained parameter values</p> <p> TYPE: <code>dict[str, NDArray]</code> </p> RETURNS DESCRIPTION <code>dict[str, GradHess]</code> <p>Dictionary mapping param_name -&gt; (natural_gradient, hessian)</p>"},{"location":"api/distributions/#openboost.Distribution.init_params","title":"init_params","text":"<pre><code>init_params(y)\n</code></pre> <p>Initialize parameters from target values.</p> <p>Returns raw (pre-link) initial values for each parameter.</p> PARAMETER DESCRIPTION <code>y</code> <p>Target values for initialization</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>dict[str, float]</code> <p>Dictionary mapping param_name -&gt; initial raw value</p>"},{"location":"api/distributions/#openboost.Distribution.mean","title":"mean  <code>abstractmethod</code>","text":"<pre><code>mean(params)\n</code></pre> <p>Expected value E[Y|params].</p>"},{"location":"api/distributions/#openboost.Distribution.variance","title":"variance  <code>abstractmethod</code>","text":"<pre><code>variance(params)\n</code></pre> <p>Variance Var[Y|params].</p>"},{"location":"api/distributions/#openboost.Distribution.quantile","title":"quantile","text":"<pre><code>quantile(params, q)\n</code></pre> <p>q-th quantile of the distribution.</p>"},{"location":"api/distributions/#openboost.Distribution.sample","title":"sample","text":"<pre><code>sample(params, n_samples=1, seed=None)\n</code></pre> <p>Sample from the distribution.</p>"},{"location":"api/distributions/#openboost.Distribution.nll","title":"nll","text":"<pre><code>nll(y, params)\n</code></pre> <p>Negative log-likelihood (for evaluation).</p>"},{"location":"api/distributions/#distributionoutput","title":"DistributionOutput","text":""},{"location":"api/distributions/#openboost.DistributionOutput","title":"DistributionOutput  <code>dataclass</code>","text":"<pre><code>DistributionOutput(params, distribution)\n</code></pre> <p>Container for distribution parameter predictions.</p> ATTRIBUTE DESCRIPTION <code>params</code> <p>Dictionary mapping parameter names to predicted values</p> <p> TYPE: <code>dict[str, NDArray]</code> </p> <code>distribution</code> <p>The Distribution instance used</p> <p> TYPE: <code>'Distribution'</code> </p>"},{"location":"api/distributions/#openboost.DistributionOutput.mean","title":"mean","text":"<pre><code>mean()\n</code></pre> <p>Expected value E[Y|X].</p>"},{"location":"api/distributions/#openboost.DistributionOutput.variance","title":"variance","text":"<pre><code>variance()\n</code></pre> <p>Variance Var[Y|X].</p>"},{"location":"api/distributions/#openboost.DistributionOutput.std","title":"std","text":"<pre><code>std()\n</code></pre> <p>Standard deviation.</p>"},{"location":"api/distributions/#openboost.DistributionOutput.quantile","title":"quantile","text":"<pre><code>quantile(q)\n</code></pre> <p>q-th quantile (0 &lt; q &lt; 1).</p>"},{"location":"api/distributions/#openboost.DistributionOutput.interval","title":"interval","text":"<pre><code>interval(alpha=0.1)\n</code></pre> <p>(1-alpha) prediction interval.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>Significance level (0.1 = 90% interval)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> RETURNS DESCRIPTION <code>Tuple[NDArray, NDArray]</code> <p>(lower, upper) bounds</p>"},{"location":"api/distributions/#openboost.DistributionOutput.sample","title":"sample","text":"<pre><code>sample(n_samples=1, seed=None)\n</code></pre> <p>Draw samples from the predicted distribution.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>Number of samples per observation</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>Random seed for reproducibility</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>samples</code> <p>Shape (n_observations, n_samples)</p> <p> TYPE: <code>NDArray</code> </p>"},{"location":"api/distributions/#openboost.DistributionOutput.nll","title":"nll","text":"<pre><code>nll(y)\n</code></pre> <p>Negative log-likelihood for observed values.</p> PARAMETER DESCRIPTION <code>y</code> <p>Observed values</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>nll</code> <p>Per-sample negative log-likelihood</p> <p> TYPE: <code>NDArray</code> </p>"},{"location":"api/loss/","title":"Loss Functions","text":"<p>Built-in loss functions and utilities.</p>"},{"location":"api/loss/#regression-losses","title":"Regression Losses","text":""},{"location":"api/loss/#mse_gradient","title":"mse_gradient","text":""},{"location":"api/loss/#openboost.mse_gradient","title":"mse_gradient","text":"<pre><code>mse_gradient(pred, y)\n</code></pre> <p>Compute MSE gradient and hessian.</p> <p>Loss: L = (pred - y)^2 Gradient: dL/dpred = 2 * (pred - y) Hessian: d\u00b2L/dpred\u00b2 = 2</p>"},{"location":"api/loss/#mae_gradient","title":"mae_gradient","text":""},{"location":"api/loss/#openboost.mae_gradient","title":"mae_gradient","text":"<pre><code>mae_gradient(pred, y)\n</code></pre> <p>Compute MAE (L1) gradient and hessian.</p> <p>Loss: L = |pred - y| Gradient: sign(pred - y) Hessian: 0 (use small constant for GBDT stability)</p> <p>Note: MAE is not twice-differentiable at pred=y, so we use a small constant hessian. This is the standard approach in XGBoost/LightGBM.</p>"},{"location":"api/loss/#huber_gradient","title":"huber_gradient","text":""},{"location":"api/loss/#openboost.huber_gradient","title":"huber_gradient","text":"<pre><code>huber_gradient(pred, y, delta=1.0)\n</code></pre> <p>Compute Huber loss gradient and hessian.</p> L = 0.5 * (pred - y)^2           if |pred - y| &lt;= delta <p>delta * |pred - y| - 0.5 * delta^2  otherwise</p>"},{"location":"api/loss/#quantile_gradient","title":"quantile_gradient","text":""},{"location":"api/loss/#openboost.quantile_gradient","title":"quantile_gradient","text":"<pre><code>quantile_gradient(pred, y, alpha=0.5)\n</code></pre> <p>Compute Quantile (Pinball) loss gradient and hessian.</p> <p>Loss: L = alpha * max(y - pred, 0) + (1 - alpha) * max(pred - y, 0)</p> <p>This is the standard quantile regression loss: - alpha=0.5: Median regression (equivalent to MAE) - alpha=0.9: 90th percentile - alpha=0.1: 10th percentile</p> Gradient <p>alpha - 1  if pred &gt; y  (under-prediction) alpha      if pred &lt; y  (over-prediction)</p> <p>Hessian: Use constant (not twice-differentiable)</p> PARAMETER DESCRIPTION <code>pred</code> <p>Predictions</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Targets</p> <p> TYPE: <code>NDArray</code> </p> <code>alpha</code> <p>Quantile level (0 &lt; alpha &lt; 1)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p>"},{"location":"api/loss/#classification-losses","title":"Classification Losses","text":""},{"location":"api/loss/#logloss_gradient","title":"logloss_gradient","text":""},{"location":"api/loss/#openboost.logloss_gradient","title":"logloss_gradient","text":"<pre><code>logloss_gradient(pred, y)\n</code></pre> <p>Compute LogLoss gradient and hessian.</p> <p>Loss: L = -ylog(p) - (1-y)log(1-p), where p = sigmoid(pred) Gradient: dL/dpred = p - y Hessian: d\u00b2L/dpred\u00b2 = p * (1 - p)</p>"},{"location":"api/loss/#softmax_gradient","title":"softmax_gradient","text":""},{"location":"api/loss/#openboost.softmax_gradient","title":"softmax_gradient","text":"<pre><code>softmax_gradient(pred, y, n_classes)\n</code></pre> <p>Compute Softmax cross-entropy gradient and hessian for multi-class.</p> <p>This returns gradients for ALL classes at once. For GBDT, you typically train K trees per round (one per class).</p> PARAMETER DESCRIPTION <code>pred</code> <p>Predictions, shape (n_samples, n_classes) - raw logits</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Labels, shape (n_samples,) - integer class labels (0 to n_classes-1)</p> <p> TYPE: <code>NDArray</code> </p> <code>n_classes</code> <p>Number of classes</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>grad</code> <p>Gradients, shape (n_samples, n_classes)</p> <p> TYPE: <code>NDArray</code> </p> <code>hess</code> <p>Hessians, shape (n_samples, n_classes)</p> <p> TYPE: <code>NDArray</code> </p> <p>Note: For binary classification, use logloss instead (more efficient).</p>"},{"location":"api/loss/#countpositive-data-losses","title":"Count/Positive Data Losses","text":""},{"location":"api/loss/#poisson_gradient","title":"poisson_gradient","text":""},{"location":"api/loss/#openboost.poisson_gradient","title":"poisson_gradient","text":"<pre><code>poisson_gradient(pred, y)\n</code></pre> <p>Compute Poisson deviance gradient and hessian.</p> <p>For count data (clicks, purchases, etc.). Predictions are in log-space.</p> <p>Loss: L = exp(pred) - y * pred  (negative log-likelihood) Gradient: dL/dpred = exp(pred) - y Hessian: d\u00b2L/dpred\u00b2 = exp(pred)</p> <p>Note: y must be non-negative integers (counts).</p>"},{"location":"api/loss/#gamma_gradient","title":"gamma_gradient","text":""},{"location":"api/loss/#openboost.gamma_gradient","title":"gamma_gradient","text":"<pre><code>gamma_gradient(pred, y)\n</code></pre> <p>Compute Gamma deviance gradient and hessian.</p> <p>For positive continuous data (insurance claims, etc.). Predictions are in log-space.</p> <p>Loss: L = y * exp(-pred) + pred  (negative log-likelihood, ignoring constants) Gradient: dL/dpred = 1 - y * exp(-pred) Hessian: d\u00b2L/dpred\u00b2 = y * exp(-pred)</p> <p>Note: y must be strictly positive.</p>"},{"location":"api/loss/#tweedie_gradient","title":"tweedie_gradient","text":""},{"location":"api/loss/#openboost.tweedie_gradient","title":"tweedie_gradient","text":"<pre><code>tweedie_gradient(pred, y, rho=1.5)\n</code></pre> <p>Compute Tweedie deviance gradient and hessian.</p> <p>Tweedie distribution interpolates between Poisson (rho=1) and Gamma (rho=2). Commonly used for insurance claims with many zeros.</p> <p>For rho in (1, 2), predictions are in log-space: Loss: L = -y * exp(pred * (1-rho)) / (1-rho) + exp(pred * (2-rho)) / (2-rho)</p> PARAMETER DESCRIPTION <code>pred</code> <p>Predictions (in log-space)</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Targets (non-negative, can have zeros)</p> <p> TYPE: <code>NDArray</code> </p> <code>rho</code> <p>Variance power (1 &lt; rho &lt; 2 for compound Poisson-Gamma)</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.5</code> </p> <p>Note: rho=1.5 is a common default for insurance data.</p>"},{"location":"api/loss/#utilities","title":"Utilities","text":""},{"location":"api/loss/#get_loss_function","title":"get_loss_function","text":""},{"location":"api/loss/#openboost.get_loss_function","title":"get_loss_function","text":"<pre><code>get_loss_function(loss, **kwargs)\n</code></pre> <p>Get a loss function by name or return custom callable.</p> PARAMETER DESCRIPTION <code>loss</code> <p>Loss function name or callable. Available: - 'mse': Mean Squared Error (regression) - 'mae': Mean Absolute Error (L1 regression) - 'huber': Huber loss (robust regression) - 'logloss': Binary cross-entropy (classification) - 'quantile': Quantile regression (percentile prediction) - 'poisson': Poisson deviance (count data) - 'gamma': Gamma deviance (positive continuous) - 'tweedie': Tweedie deviance (compound Poisson-Gamma)</p> <p> TYPE: <code>str | LossFunction</code> </p> <code>**kwargs</code> <p>Additional parameters for specific losses: - quantile_alpha: Quantile level for 'quantile' loss (default 0.5) - tweedie_rho: Variance power for 'tweedie' loss (default 1.5)</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LossFunction</code> <p>Loss function callable.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; loss_fn = get_loss_function('mse')\n&gt;&gt;&gt; loss_fn = get_loss_function('quantile', quantile_alpha=0.9)\n&gt;&gt;&gt; loss_fn = get_loss_function('tweedie', tweedie_rho=1.5)\n</code></pre>"},{"location":"api/models/","title":"Models","text":"<p>All OpenBoost model classes.</p>"},{"location":"api/models/#standard-gbdt","title":"Standard GBDT","text":""},{"location":"api/models/#gradientboosting","title":"GradientBoosting","text":""},{"location":"api/models/#openboost.GradientBoosting","title":"GradientBoosting  <code>dataclass</code>","text":"<pre><code>GradientBoosting(\n    n_trees=100,\n    max_depth=6,\n    learning_rate=0.1,\n    loss=\"mse\",\n    min_child_weight=1.0,\n    reg_lambda=1.0,\n    reg_alpha=0.0,\n    gamma=0.0,\n    subsample=1.0,\n    colsample_bytree=1.0,\n    n_bins=256,\n    quantile_alpha=0.5,\n    tweedie_rho=1.5,\n    distributed=False,\n    n_workers=None,\n    subsample_strategy=\"none\",\n    goss_top_rate=0.2,\n    goss_other_rate=0.1,\n    batch_size=None,\n    n_gpus=None,\n    devices=None,\n)\n</code></pre> <p>               Bases: <code>PersistenceMixin</code></p> <p>Gradient Boosting ensemble model.</p> <p>A gradient boosting model that supports both built-in loss functions and custom loss functions. When using built-in losses with GPU, training is fully batched for maximum performance.</p> PARAMETER DESCRIPTION <code>n_trees</code> <p>Number of trees to train.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>max_depth</code> <p>Maximum depth of each tree.</p> <p> TYPE: <code>int</code> DEFAULT: <code>6</code> </p> <code>learning_rate</code> <p>Shrinkage factor applied to each tree.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>loss</code> <p>Loss function. Can be: - 'mse': Mean Squared Error (regression) - 'logloss': Binary cross-entropy (classification) - 'huber': Huber loss (robust regression) - 'mae': Mean Absolute Error (L1 regression) - 'quantile': Quantile regression (use with quantile_alpha) - Callable: Custom function(pred, y) -&gt; (grad, hess)</p> <p> TYPE: <code>str | LossFunction | Callable[..., tuple]</code> DEFAULT: <code>'mse'</code> </p> <code>min_child_weight</code> <p>Minimum sum of hessian in a leaf.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>reg_lambda</code> <p>L2 regularization on leaf values.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_bins</code> <p>Number of bins for histogram building.</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> <code>quantile_alpha</code> <p>Quantile level for 'quantile' loss (0 &lt; alpha &lt; 1). - 0.5: Median regression (default) - 0.9: 90th percentile - 0.1: 10th percentile</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>tweedie_rho</code> <p>Variance power for 'tweedie' loss (1 &lt; rho &lt; 2). - 1.5: Default (compound Poisson-Gamma)</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.5</code> </p> <code>subsample_strategy</code> <p>Sampling strategy for large-scale training (Phase 17): - 'none': No sampling (default) - 'random': Random subsampling - 'goss': Gradient-based One-Side Sampling (LightGBM-style)</p> <p> TYPE: <code>Literal['none', 'random', 'goss']</code> DEFAULT: <code>'none'</code> </p> <code>goss_top_rate</code> <p>Fraction of top-gradient samples to keep (for GOSS).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>goss_other_rate</code> <p>Fraction of remaining samples to sample (for GOSS).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>batch_size</code> <p>Mini-batch size for large datasets. If None, process all at once.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <p>Basic regression:</p> <pre><code>import openboost as ob\n\nmodel = ob.GradientBoosting(n_trees=100, loss='mse')\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n</code></pre> <p>Quantile regression (90th percentile):</p> <pre><code>model = ob.GradientBoosting(loss='quantile', quantile_alpha=0.9)\nmodel.fit(X_train, y_train)\n</code></pre> <p>GOSS for faster training:</p> <pre><code>model = ob.GradientBoosting(\n    n_trees=100,\n    subsample_strategy='goss',\n    goss_top_rate=0.2,\n    goss_other_rate=0.1,\n)\n</code></pre> <p>Multi-GPU training:</p> <pre><code>model = ob.GradientBoosting(n_trees=100, n_gpus=4)\nmodel.fit(X, y)  # Data parallel across 4 GPUs\n</code></pre>"},{"location":"api/models/#openboost.GradientBoosting.fit","title":"fit","text":"<pre><code>fit(\n    X, y, callbacks=None, eval_set=None, sample_weight=None\n)\n</code></pre> <p>Fit the gradient boosting model.</p> PARAMETER DESCRIPTION <code>X</code> <p>Training features, shape (n_samples, n_features).</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Training targets, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>callbacks</code> <p>List of Callback instances for training hooks.        Use EarlyStopping for early stopping, Logger for progress.</p> <p> TYPE: <code>list[Callback] | None</code> DEFAULT: <code>None</code> </p> <code>eval_set</code> <p>List of (X, y) tuples for validation (used with callbacks).</p> <p> TYPE: <code>list[tuple[NDArray, NDArray]] | None</code> DEFAULT: <code>None</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,).</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>self</code> <p>The fitted model.</p> <p> TYPE: <code>GradientBoosting</code> </p> Example <pre><code>from openboost import GradientBoosting, EarlyStopping, Logger\n\nmodel = GradientBoosting(n_trees=1000)\nmodel.fit(\n    X, y,\n    callbacks=[EarlyStopping(patience=50), Logger(period=10)],\n    eval_set=[(X_val, y_val)]\n)\n</code></pre>"},{"location":"api/models/#openboost.GradientBoosting.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Generate predictions for X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Features to predict on, shape (n_samples, n_features). Can be raw numpy array or pre-binned BinnedArray.</p> <p> TYPE: <code>NDArray | BinnedArray</code> </p> RETURNS DESCRIPTION <code>predictions</code> <p>Shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If model is not fitted or X has wrong shape.</p>"},{"location":"api/models/#multiclassgradientboosting","title":"MultiClassGradientBoosting","text":""},{"location":"api/models/#openboost.MultiClassGradientBoosting","title":"MultiClassGradientBoosting  <code>dataclass</code>","text":"<pre><code>MultiClassGradientBoosting(\n    n_classes,\n    n_trees=100,\n    max_depth=6,\n    learning_rate=0.1,\n    min_child_weight=1.0,\n    reg_lambda=1.0,\n    reg_alpha=0.0,\n    gamma=0.0,\n    subsample=1.0,\n    colsample_bytree=1.0,\n    n_bins=256,\n    subsample_strategy=\"none\",\n    goss_top_rate=0.2,\n    goss_other_rate=0.1,\n)\n</code></pre> <p>               Bases: <code>PersistenceMixin</code></p> <p>Multi-class Gradient Boosting classifier.</p> <p>Uses softmax loss and trains K trees per round (one per class), following the XGBoost/LightGBM approach.</p> PARAMETER DESCRIPTION <code>n_classes</code> <p>Number of classes.</p> <p> TYPE: <code>int</code> </p> <code>n_trees</code> <p>Number of boosting rounds (total trees = n_trees * n_classes).</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>max_depth</code> <p>Maximum depth of each tree.</p> <p> TYPE: <code>int</code> DEFAULT: <code>6</code> </p> <code>learning_rate</code> <p>Shrinkage factor applied to each tree.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>min_child_weight</code> <p>Minimum sum of hessian in a leaf.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>reg_lambda</code> <p>L2 regularization on leaf values.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_bins</code> <p>Number of bins for histogram building.</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> <code>subsample_strategy</code> <p>Sampling strategy (Phase 17): 'none', 'random', 'goss'.</p> <p> TYPE: <code>Literal['none', 'random', 'goss']</code> DEFAULT: <code>'none'</code> </p> <code>goss_top_rate</code> <p>Fraction of top-gradient samples to keep (for GOSS).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>goss_other_rate</code> <p>Fraction of remaining samples to sample (for GOSS).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> Example <pre><code>import openboost as ob\n\nmodel = ob.MultiClassGradientBoosting(n_classes=10, n_trees=100)\nmodel.fit(X_train, y_train)  # y_train: 0 to 9\npredictions = model.predict(X_test)  # Returns class labels\nproba = model.predict_proba(X_test)  # Returns probabilities\n</code></pre> <p>With GOSS sampling:</p> <pre><code>model = ob.MultiClassGradientBoosting(\n    n_classes=10, n_trees=100,\n    subsample_strategy='goss',\n    goss_top_rate=0.2,\n    goss_other_rate=0.1\n)\n</code></pre>"},{"location":"api/models/#openboost.MultiClassGradientBoosting.fit","title":"fit","text":"<pre><code>fit(X, y)\n</code></pre> <p>Fit the multi-class gradient boosting model.</p> PARAMETER DESCRIPTION <code>X</code> <p>Training features, shape (n_samples, n_features).</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Training labels, shape (n_samples,). Integer class labels 0 to n_classes-1.</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>self</code> <p>The fitted model.</p> <p> TYPE: <code>'MultiClassGradientBoosting'</code> </p>"},{"location":"api/models/#openboost.MultiClassGradientBoosting.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict class labels.</p> PARAMETER DESCRIPTION <code>X</code> <p>Features to predict on.</p> <p> TYPE: <code>NDArray | BinnedArray</code> </p> RETURNS DESCRIPTION <code>labels</code> <p>Shape (n_samples,). Integer class labels.</p> <p> TYPE: <code>NDArray</code> </p>"},{"location":"api/models/#openboost.MultiClassGradientBoosting.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre> <p>Predict class probabilities.</p> PARAMETER DESCRIPTION <code>X</code> <p>Features to predict on.</p> <p> TYPE: <code>NDArray | BinnedArray</code> </p> RETURNS DESCRIPTION <code>probabilities</code> <p>Shape (n_samples, n_classes).</p> <p> TYPE: <code>NDArray</code> </p>"},{"location":"api/models/#dart","title":"DART","text":""},{"location":"api/models/#openboost.DART","title":"DART  <code>dataclass</code>","text":"<pre><code>DART(\n    n_trees=100,\n    max_depth=6,\n    learning_rate=0.1,\n    loss=\"mse\",\n    dropout_rate=0.1,\n    skip_drop=0.0,\n    normalize=True,\n    sample_type=\"uniform\",\n    min_child_weight=1.0,\n    reg_lambda=1.0,\n    n_bins=256,\n    seed=None,\n)\n</code></pre> <p>               Bases: <code>PersistenceMixin</code></p> <p>DART: Gradient Boosting with Dropout.</p> <p>Implements DART (Dropouts meet Multiple Additive Regression Trees), which randomly drops trees during training to prevent overfitting.</p> PARAMETER DESCRIPTION <code>n_trees</code> <p>Number of trees to train.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>max_depth</code> <p>Maximum depth of each tree.</p> <p> TYPE: <code>int</code> DEFAULT: <code>6</code> </p> <code>learning_rate</code> <p>Base learning rate (shrinkage factor).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>loss</code> <p>Loss function ('mse', 'logloss', 'huber', or callable).</p> <p> TYPE: <code>str | LossFunction</code> DEFAULT: <code>'mse'</code> </p> <code>dropout_rate</code> <p>Fraction of trees to drop each round (0 to 1).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>skip_drop</code> <p>Probability of skipping dropout for a round.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>normalize</code> <p>If True, normalize dropped tree contributions.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>sample_type</code> <p>How to sample dropped trees ('uniform' or 'weighted').</p> <p> TYPE: <code>str</code> DEFAULT: <code>'uniform'</code> </p> <code>min_child_weight</code> <p>Minimum sum of hessian in a leaf.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>reg_lambda</code> <p>L2 regularization on leaf values.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_bins</code> <p>Number of bins for histogram building.</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> <code>seed</code> <p>Random seed for reproducibility.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Example <pre><code>import openboost as ob\n\n# DART with 10% dropout\nmodel = ob.DART(n_trees=100, dropout_rate=0.1)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\n# DART with higher dropout for more regularization\nmodel = ob.DART(n_trees=200, dropout_rate=0.3, skip_drop=0.5)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"api/models/#openboost.DART.fit","title":"fit","text":"<pre><code>fit(X, y)\n</code></pre> <p>Fit the DART model.</p> PARAMETER DESCRIPTION <code>X</code> <p>Training features, shape (n_samples, n_features).</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Training targets, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>self</code> <p>The fitted model.</p> <p> TYPE: <code>'DART'</code> </p>"},{"location":"api/models/#openboost.DART.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Generate predictions for X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Features to predict on, shape (n_samples, n_features). Can be raw numpy array or pre-binned BinnedArray.</p> <p> TYPE: <code>NDArray | BinnedArray</code> </p> RETURNS DESCRIPTION <code>predictions</code> <p>Shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p>"},{"location":"api/models/#interpretable-models","title":"Interpretable Models","text":""},{"location":"api/models/#openboostgam","title":"OpenBoostGAM","text":""},{"location":"api/models/#openboost.OpenBoostGAM","title":"OpenBoostGAM  <code>dataclass</code>","text":"<pre><code>OpenBoostGAM(\n    n_rounds=1000,\n    learning_rate=0.01,\n    reg_lambda=1.0,\n    loss=\"mse\",\n    n_bins=256,\n)\n</code></pre> <p>               Bases: <code>PersistenceMixin</code></p> <p>GPU-accelerated Generalized Additive Model.</p> An interpretable model where <p>prediction = sum(shape_functioni for all features)</p> <p>Each shape function is a lookup table mapping binned feature values to contribution scores. Trained via parallel gradient boosting.</p> PARAMETER DESCRIPTION <code>n_rounds</code> <p>Number of boosting rounds.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>learning_rate</code> <p>Shrinkage factor (smaller = more stable, needs more rounds).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>reg_lambda</code> <p>L2 regularization on leaf values.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>loss</code> <p>Loss function ('mse', 'logloss', or callable).</p> <p> TYPE: <code>str | LossFunction</code> DEFAULT: <code>'mse'</code> </p> <code>n_bins</code> <p>Number of bins for histogram building.</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> Example <pre><code>import openboost as ob\n\ngam = ob.OpenBoostGAM(n_rounds=1000, learning_rate=0.01)\ngam.fit(X_train, y_train)\npredictions = gam.predict(X_test)\n\n# Interpret: plot shape function for feature 0\ngam.plot_shape_function(0, feature_name=\"age\")\n</code></pre>"},{"location":"api/models/#openboost.OpenBoostGAM.fit","title":"fit","text":"<pre><code>fit(X, y)\n</code></pre> <p>Fit the GAM model.</p> PARAMETER DESCRIPTION <code>X</code> <p>Training features, shape (n_samples, n_features).</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Training targets, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>self</code> <p>The fitted model.</p> <p> TYPE: <code>'OpenBoostGAM'</code> </p>"},{"location":"api/models/#openboost.OpenBoostGAM.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Generate predictions.</p> PARAMETER DESCRIPTION <code>X</code> <p>Features, shape (n_samples, n_features).</p> <p> TYPE: <code>NDArray | BinnedArray</code> </p> RETURNS DESCRIPTION <code>predictions</code> <p>Shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p>"},{"location":"api/models/#openboost.OpenBoostGAM.get_feature_importance","title":"get_feature_importance","text":"<pre><code>get_feature_importance()\n</code></pre> <p>Get feature importance based on shape function variance.</p> RETURNS DESCRIPTION <code>importance</code> <p>Shape (n_features,), higher = more important.</p> <p> TYPE: <code>NDArray</code> </p>"},{"location":"api/models/#openboost.OpenBoostGAM.plot_shape_function","title":"plot_shape_function","text":"<pre><code>plot_shape_function(feature_idx, feature_name=None)\n</code></pre> <p>Plot the shape function for a feature.</p> PARAMETER DESCRIPTION <code>feature_idx</code> <p>Index of the feature to plot.</p> <p> TYPE: <code>int</code> </p> <code>feature_name</code> <p>Optional name for the x-axis label.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p>"},{"location":"api/models/#linearleafgbdt","title":"LinearLeafGBDT","text":""},{"location":"api/models/#openboost.LinearLeafGBDT","title":"LinearLeafGBDT  <code>dataclass</code>","text":"<pre><code>LinearLeafGBDT(\n    n_trees=100,\n    max_depth=4,\n    learning_rate=0.1,\n    loss=\"mse\",\n    min_samples_leaf=20,\n    reg_lambda_tree=1.0,\n    reg_lambda_linear=0.1,\n    max_features_linear=\"sqrt\",\n    n_bins=256,\n)\n</code></pre> <p>               Bases: <code>PersistenceMixin</code></p> <p>Gradient Boosting with Linear Leaf Trees.</p> <p>Each tree has linear models in its leaves instead of constant values. This enables: - Better extrapolation beyond training data range - Smoother decision boundaries - Can use shallower trees (linear models add complexity)</p> <p>Recommended settings: - Use max_depth=3-4 (shallower than standard GBDT) - Use larger min_samples_leaf (need samples to fit linear model)</p> PARAMETER DESCRIPTION <code>n_trees</code> <p>Number of boosting rounds</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>max_depth</code> <p>Maximum tree depth (typically 3-4, shallower than standard)</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>learning_rate</code> <p>Shrinkage factor</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>loss</code> <p>Loss function ('mse', 'mae', 'huber', or callable)</p> <p> TYPE: <code>str | LossFunction</code> DEFAULT: <code>'mse'</code> </p> <code>min_samples_leaf</code> <p>Minimum samples to fit linear model in leaf</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>reg_lambda_tree</code> <p>L2 regularization for tree splits</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>reg_lambda_linear</code> <p>L2 regularization for linear models (ridge)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>max_features_linear</code> <p>Max features per leaf's linear model - None: Use all features - 'sqrt': Use sqrt(n_features) features - 'log2': Use log2(n_features) features - int: Use exactly this many features</p> <p> TYPE: <code>int | str | None</code> DEFAULT: <code>'sqrt'</code> </p> <code>n_bins</code> <p>Number of bins for histogram building</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> Example <pre><code>model = LinearLeafGBDT(n_trees=100, max_depth=4)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\n# Compare extrapolation with standard GBDT\nfrom openboost import GradientBoosting\nstandard = GradientBoosting(n_trees=100, max_depth=6)\nstandard.fit(X_train, y_train)\n# LinearLeafGBDT typically extrapolates better on linear trends\n</code></pre>"},{"location":"api/models/#openboost.LinearLeafGBDT.fit","title":"fit","text":"<pre><code>fit(X, y)\n</code></pre> <p>Fit the linear leaf GBDT model.</p> PARAMETER DESCRIPTION <code>X</code> <p>Training features, shape (n_samples, n_features)</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Training targets, shape (n_samples,)</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>self</code> <p>Fitted model</p> <p> TYPE: <code>'LinearLeafGBDT'</code> </p>"},{"location":"api/models/#openboost.LinearLeafGBDT.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Generate predictions.</p> PARAMETER DESCRIPTION <code>X</code> <p>Features, shape (n_samples, n_features)</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>predictions</code> <p>Shape (n_samples,)</p> <p> TYPE: <code>NDArray</code> </p>"},{"location":"api/models/#probabilistic-models-naturalboost","title":"Probabilistic Models (NaturalBoost)","text":""},{"location":"api/models/#naturalboost","title":"NaturalBoost","text":""},{"location":"api/models/#openboost.NaturalBoost","title":"NaturalBoost  <code>dataclass</code>","text":"<pre><code>NaturalBoost(\n    distribution=\"normal\",\n    n_trees=100,\n    max_depth=4,\n    learning_rate=0.1,\n    min_child_weight=1.0,\n    reg_lambda=1.0,\n    reg_alpha=0.0,\n    subsample=1.0,\n    colsample_bytree=1.0,\n    n_bins=256,\n)\n</code></pre> <p>               Bases: <code>DistributionalGBDT</code></p> <p>Natural Gradient Boosting for probabilistic prediction.</p> <p>OpenBoost's implementation of natural gradient boosting, inspired by NGBoost. Uses natural gradient instead of ordinary gradient, leading to faster convergence by accounting for the geometry of the parameter space.</p> <p>Natural gradient: F^{-1} @ ordinary_gradient where F is the Fisher information matrix.</p> <p>Key advantages over standard GBDT: - Full probability distributions, not just point estimates - Prediction intervals and uncertainty quantification - Faster convergence than ordinary gradient descent</p> <p>Key advantages over official NGBoost: - GPU acceleration via histogram-based trees - Faster on large datasets (&gt;10k samples) - Custom distributions with autodiff support</p> Reference <p>Duan et al. \"NGBoost: Natural Gradient Boosting for Probabilistic Prediction.\" ICML 2020.</p> PARAMETER DESCRIPTION <code>distribution</code> <p>Distribution name or instance</p> <p> TYPE: <code>Literal['normal', 'lognormal', 'gamma', 'poisson', 'studentt', 'tweedie', 'negbin'] | Distribution</code> DEFAULT: <code>'normal'</code> </p> <code>n_trees</code> <p>Number of boosting rounds (often needs fewer than ordinary)</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>max_depth</code> <p>Maximum depth of each tree (default 4, often smaller is better)</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>learning_rate</code> <p>Shrinkage factor</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>min_child_weight</code> <p>Minimum sum of hessian in a leaf</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>reg_lambda</code> <p>L2 regularization</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_bins</code> <p>Number of bins for histogram building</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> Example <pre><code>model = NaturalBoost(distribution='normal', n_trees=500)\nmodel.fit(X_train, y_train)\n\n# Get prediction intervals\nlower, upper = model.predict_interval(X_test, alpha=0.1)\n\n# Get full distribution\noutput = model.predict_distribution(X_test)\nsamples = output.sample(n_samples=1000)\n</code></pre>"},{"location":"api/models/#naturalboostnormal","title":"NaturalBoostNormal","text":""},{"location":"api/models/#openboost.NaturalBoostNormal","title":"NaturalBoostNormal","text":"<pre><code>NaturalBoostNormal(**kwargs)\n</code></pre> <p>NaturalBoost with Normal distribution.</p>"},{"location":"api/models/#naturalboostlognormal","title":"NaturalBoostLogNormal","text":""},{"location":"api/models/#openboost.NaturalBoostLogNormal","title":"NaturalBoostLogNormal","text":"<pre><code>NaturalBoostLogNormal(**kwargs)\n</code></pre> <p>NaturalBoost with LogNormal distribution (for positive data).</p>"},{"location":"api/models/#naturalboostgamma","title":"NaturalBoostGamma","text":""},{"location":"api/models/#openboost.NaturalBoostGamma","title":"NaturalBoostGamma","text":"<pre><code>NaturalBoostGamma(**kwargs)\n</code></pre> <p>NaturalBoost with Gamma distribution (for positive data).</p>"},{"location":"api/models/#naturalboostpoisson","title":"NaturalBoostPoisson","text":""},{"location":"api/models/#openboost.NaturalBoostPoisson","title":"NaturalBoostPoisson","text":"<pre><code>NaturalBoostPoisson(**kwargs)\n</code></pre> <p>NaturalBoost with Poisson distribution (for count data).</p>"},{"location":"api/models/#naturalbooststudentt","title":"NaturalBoostStudentT","text":""},{"location":"api/models/#openboost.NaturalBoostStudentT","title":"NaturalBoostStudentT","text":"<pre><code>NaturalBoostStudentT(**kwargs)\n</code></pre> <p>NaturalBoost with Student-t distribution (for heavy-tailed data).</p>"},{"location":"api/models/#naturalboosttweedie","title":"NaturalBoostTweedie","text":""},{"location":"api/models/#openboost.NaturalBoostTweedie","title":"NaturalBoostTweedie","text":"<pre><code>NaturalBoostTweedie(power=1.5, **kwargs)\n</code></pre> <p>NaturalBoost with Tweedie distribution (for insurance claims, zero-inflated data).</p> <p>Kaggle Use Cases: - Porto Seguro Safe Driver Prediction - Allstate Claims Severity - Any zero-inflated positive target</p> PARAMETER DESCRIPTION <code>power</code> <p>Tweedie power parameter (1 &lt; power &lt; 2).    1.5 is the default for insurance claims.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.5</code> </p> <code>**kwargs</code> <p>Other NaturalBoost parameters (n_trees, learning_rate, etc.)</p> <p> DEFAULT: <code>{}</code> </p> Example <pre><code>model = NaturalBoostTweedie(power=1.5, n_trees=500)\nmodel.fit(X_train, y_train)  # y has zeros and positive values\n\n# Get prediction intervals (XGBoost can't do this!)\nlower, upper = model.predict_interval(X_test, alpha=0.1)\n</code></pre>"},{"location":"api/models/#naturalboostnegbin","title":"NaturalBoostNegBin","text":""},{"location":"api/models/#openboost.NaturalBoostNegBin","title":"NaturalBoostNegBin","text":"<pre><code>NaturalBoostNegBin(**kwargs)\n</code></pre> <p>NaturalBoost with Negative Binomial distribution (for overdispersed count data).</p> <p>Kaggle Use Cases: - Rossmann Store Sales - Bike Sharing Demand - Grupo Bimbo Inventory Demand - Any count prediction where variance &gt; mean</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>NaturalBoost parameters (n_trees, learning_rate, etc.)</p> <p> DEFAULT: <code>{}</code> </p> Example <pre><code>model = NaturalBoostNegBin(n_trees=500)\nmodel.fit(X_train, y_train)  # y is count data\n\n# Probability of exceeding threshold (demand planning!)\noutput = model.predict_distribution(X_test)\nprob_high_demand = output.distribution.prob_exceed(output.params, 100)\n</code></pre>"},{"location":"api/models/#sklearn-wrappers","title":"sklearn Wrappers","text":""},{"location":"api/models/#openboostregressor","title":"OpenBoostRegressor","text":""},{"location":"api/models/#openboost.OpenBoostRegressor","title":"OpenBoostRegressor","text":"<pre><code>OpenBoostRegressor(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    loss=\"squared_error\",\n    min_child_weight=1.0,\n    reg_lambda=1.0,\n    reg_alpha=0.0,\n    gamma=0.0,\n    subsample=1.0,\n    colsample_bytree=1.0,\n    n_bins=256,\n    quantile_alpha=0.5,\n    subsample_strategy=\"none\",\n    goss_top_rate=0.2,\n    goss_other_rate=0.1,\n    batch_size=None,\n    early_stopping_rounds=None,\n    verbose=0,\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Gradient Boosting Regressor with sklearn-compatible interface.</p> <p>This is a thin wrapper around OpenBoost's GradientBoosting that provides full compatibility with sklearn's ecosystem (GridSearchCV, Pipeline, etc.).</p>"},{"location":"api/models/#openboost.OpenBoostRegressor--parameters","title":"Parameters","text":"<p>n_estimators : int, default=100     Number of boosting rounds (trees). max_depth : int, default=6     Maximum depth of each tree. learning_rate : float, default=0.1     Shrinkage factor applied to each tree's contribution. loss : {'squared_error', 'absolute_error', 'huber', 'quantile'}, default='squared_error'     Loss function to optimize. min_child_weight : float, default=1.0     Minimum sum of hessian in a leaf node. reg_lambda : float, default=1.0     L2 regularization on leaf values. reg_alpha : float, default=0.0     L1 regularization on leaf values. gamma : float, default=0.0     Minimum gain required to make a split. subsample : float, default=1.0     Fraction of samples to use for each tree. colsample_bytree : float, default=1.0     Fraction of features to use for each tree. n_bins : int, default=256     Number of bins for histogram building. quantile_alpha : float, default=0.5     Quantile level for 'quantile' loss. subsample_strategy : {'none', 'random', 'goss'}, default='none'     Sampling strategy for large-scale training (Phase 17).     - 'none': No sampling (default)     - 'random': Random subsampling     - 'goss': Gradient-based One-Side Sampling (LightGBM-style) goss_top_rate : float, default=0.2     Fraction of top-gradient samples to keep (for GOSS). goss_other_rate : float, default=0.1     Fraction of remaining samples to sample (for GOSS). batch_size : int, optional     Mini-batch size for large datasets. If None, process all at once. early_stopping_rounds : int, optional     Stop training if validation score doesn't improve for this many rounds.     Requires eval_set to be passed to fit(). verbose : int, default=0     Verbosity level (0=silent, N=log every N rounds). random_state : int, optional     Random seed for reproducibility.</p>"},{"location":"api/models/#openboost.OpenBoostRegressor--attributes","title":"Attributes","text":"<p>n_features_in_ : int     Number of features seen during fit. feature_names_in_ : ndarray of shape (n_features_in_,)     Names of features seen during fit (if X is a DataFrame). feature_importances_ : ndarray of shape (n_features_in_,)     Feature importances (based on split frequency). booster_ : GradientBoosting     The underlying fitted OpenBoost model. best_iteration_ : int     Iteration with best validation score (if early stopping used). best_score_ : float     Best validation score achieved (if early stopping used).</p>"},{"location":"api/models/#openboost.OpenBoostRegressor--examples","title":"Examples","text":"<p>from openboost import OpenBoostRegressor reg = OpenBoostRegressor(n_estimators=100, max_depth=6) reg.fit(X_train, y_train) reg.predict(X_test) reg.score(X_test, y_test)  # R\u00b2 score</p>"},{"location":"api/models/#openboost.OpenBoostRegressor--with-early-stopping","title":"With early stopping","text":"<p>reg = OpenBoostRegressor(n_estimators=1000, early_stopping_rounds=50) reg.fit(X_train, y_train, eval_set=[(X_val, y_val)]) print(f\"Best iteration: {reg.best_iteration_}\")</p>"},{"location":"api/models/#openboost.OpenBoostRegressor--gridsearchcv","title":"GridSearchCV","text":"<p>from sklearn.model_selection import GridSearchCV param_grid = {'n_estimators': [50, 100], 'max_depth': [3, 5]} search = GridSearchCV(OpenBoostRegressor(), param_grid, cv=5) search.fit(X, y)</p>"},{"location":"api/models/#openboost.OpenBoostRegressor.fit","title":"fit","text":"<pre><code>fit(X, y, sample_weight=None, eval_set=None)\n</code></pre> <p>Fit the gradient boosting regressor.</p>"},{"location":"api/models/#openboost.OpenBoostRegressor.fit--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Training features. y : array-like of shape (n_samples,)     Target values. sample_weight : array-like of shape (n_samples,), optional     Sample weights. eval_set : list of (X, y) tuples, optional     Validation sets for early stopping.</p>"},{"location":"api/models/#openboost.OpenBoostRegressor.fit--returns","title":"Returns","text":"<p>self : OpenBoostRegressor     Fitted estimator.</p>"},{"location":"api/models/#openboost.OpenBoostRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict target values.</p>"},{"location":"api/models/#openboost.OpenBoostRegressor.predict--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Features to predict on.</p>"},{"location":"api/models/#openboost.OpenBoostRegressor.predict--returns","title":"Returns","text":"<p>y_pred : ndarray of shape (n_samples,)     Predicted values.</p>"},{"location":"api/models/#openboostclassifier","title":"OpenBoostClassifier","text":""},{"location":"api/models/#openboost.OpenBoostClassifier","title":"OpenBoostClassifier","text":"<pre><code>OpenBoostClassifier(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    min_child_weight=1.0,\n    reg_lambda=1.0,\n    reg_alpha=0.0,\n    gamma=0.0,\n    subsample=1.0,\n    colsample_bytree=1.0,\n    n_bins=256,\n    subsample_strategy=\"none\",\n    goss_top_rate=0.2,\n    goss_other_rate=0.1,\n    batch_size=None,\n    early_stopping_rounds=None,\n    verbose=0,\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Gradient Boosting Classifier with sklearn-compatible interface.</p> <p>Automatically handles binary and multi-class classification. Uses logloss for binary, softmax for multi-class.</p>"},{"location":"api/models/#openboost.OpenBoostClassifier--parameters","title":"Parameters","text":"<p>n_estimators : int, default=100     Number of boosting rounds. max_depth : int, default=6     Maximum depth of each tree. learning_rate : float, default=0.1     Shrinkage factor. min_child_weight : float, default=1.0     Minimum sum of hessian in a leaf. reg_lambda : float, default=1.0     L2 regularization on leaf values. reg_alpha : float, default=0.0     L1 regularization on leaf values. gamma : float, default=0.0     Minimum gain required to make a split. subsample : float, default=1.0     Fraction of samples per tree. colsample_bytree : float, default=1.0     Fraction of features per tree. n_bins : int, default=256     Number of bins for histogram building. subsample_strategy : {'none', 'random', 'goss'}, default='none'     Sampling strategy for large-scale training (Phase 17). goss_top_rate : float, default=0.2     Fraction of top-gradient samples to keep (for GOSS). goss_other_rate : float, default=0.1     Fraction of remaining samples to sample (for GOSS). batch_size : int, optional     Mini-batch size for large datasets. early_stopping_rounds : int, optional     Stop if validation doesn't improve. verbose : int, default=0     Verbosity level. random_state : int, optional     Random seed.</p>"},{"location":"api/models/#openboost.OpenBoostClassifier--attributes","title":"Attributes","text":"<p>classes_ : ndarray     Unique class labels. n_classes_ : int     Number of classes. n_features_in_ : int     Number of features. feature_importances_ : ndarray     Feature importances. booster_ : GradientBoosting or MultiClassGradientBoosting     Underlying model.</p>"},{"location":"api/models/#openboost.OpenBoostClassifier--examples","title":"Examples","text":"<p>from openboost import OpenBoostClassifier clf = OpenBoostClassifier(n_estimators=100) clf.fit(X_train, y_train) clf.predict(X_test) clf.predict_proba(X_test) clf.classes_ array([0, 1])</p>"},{"location":"api/models/#openboost.OpenBoostClassifier--multi-class","title":"Multi-class","text":"<p>clf.fit(X_train, y_train)  # y_train has 3+ classes clf.predict_proba(X_test).shape (n_samples, n_classes)</p>"},{"location":"api/models/#openboost.OpenBoostClassifier.fit","title":"fit","text":"<pre><code>fit(X, y, sample_weight=None, eval_set=None)\n</code></pre> <p>Fit the gradient boosting classifier.</p>"},{"location":"api/models/#openboost.OpenBoostClassifier.fit--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Training features. y : array-like of shape (n_samples,)     Target class labels. sample_weight : array-like of shape (n_samples,), optional     Sample weights. eval_set : list of (X, y) tuples, optional     Validation sets for early stopping.</p>"},{"location":"api/models/#openboost.OpenBoostClassifier.fit--returns","title":"Returns","text":"<p>self : OpenBoostClassifier     Fitted estimator.</p>"},{"location":"api/models/#openboost.OpenBoostClassifier.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict class labels.</p>"},{"location":"api/models/#openboost.OpenBoostClassifier.predict--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Features to predict on.</p>"},{"location":"api/models/#openboost.OpenBoostClassifier.predict--returns","title":"Returns","text":"<p>y_pred : ndarray of shape (n_samples,)     Predicted class labels.</p>"},{"location":"api/models/#openboost.OpenBoostClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre> <p>Predict class probabilities.</p>"},{"location":"api/models/#openboost.OpenBoostClassifier.predict_proba--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Features to predict on.</p>"},{"location":"api/models/#openboost.OpenBoostClassifier.predict_proba--returns","title":"Returns","text":"<p>proba : ndarray of shape (n_samples, n_classes)     Class probabilities.</p>"},{"location":"api/models/#openboostdistributionalregressor","title":"OpenBoostDistributionalRegressor","text":""},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor","title":"OpenBoostDistributionalRegressor","text":"<pre><code>OpenBoostDistributionalRegressor(\n    distribution=\"normal\",\n    n_estimators=100,\n    max_depth=4,\n    learning_rate=0.1,\n    min_child_weight=1.0,\n    reg_lambda=1.0,\n    n_bins=256,\n    use_natural_gradient=True,\n    verbose=0,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Distributional regression with sklearn-compatible interface.</p> <p>Predicts full probability distributions instead of point estimates. Uses natural gradient boosting (NGBoost) by default for faster convergence.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor--parameters","title":"Parameters","text":"<p>distribution : str, default='normal'     Distribution family. Options: 'normal', 'lognormal', 'gamma',      'poisson', 'studentt'. n_estimators : int, default=100     Number of boosting rounds. max_depth : int, default=4     Maximum depth of each tree. Typically shallower than standard GBDT. learning_rate : float, default=0.1     Shrinkage factor. min_child_weight : float, default=1.0     Minimum sum of hessian in a leaf. reg_lambda : float, default=1.0     L2 regularization on leaf values. n_bins : int, default=256     Number of bins for histogram building. use_natural_gradient : bool, default=True     If True, use NGBoost (natural gradient). Recommended for faster     convergence and better uncertainty calibration. verbose : int, default=0     Verbosity level.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor--attributes","title":"Attributes","text":"<p>n_features_in_ : int     Number of features seen during fit. booster_ : NGBoost or DistributionalGBDT     The underlying fitted model.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor--examples","title":"Examples","text":"<p>from openboost import OpenBoostDistributionalRegressor model = OpenBoostDistributionalRegressor(distribution='normal') model.fit(X_train, y_train)</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor--point-prediction-mean","title":"Point prediction (mean)","text":"<p>y_pred = model.predict(X_test)</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor--prediction-intervals-90","title":"Prediction intervals (90%)","text":"<p>lower, upper = model.predict_interval(X_test, alpha=0.1)</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor--full-distribution-parameters","title":"Full distribution parameters","text":"<p>params = model.predict_distribution(X_test) mu, sigma = params['loc'], params['scale']</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor--sample-from-predicted-distribution","title":"Sample from predicted distribution","text":"<p>samples = model.sample(X_test, n_samples=100)</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.fit","title":"fit","text":"<pre><code>fit(X, y, **kwargs)\n</code></pre> <p>Fit the distributional regressor.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.fit--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Training features. y : array-like of shape (n_samples,)     Target values.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.fit--returns","title":"Returns","text":"<p>self : OpenBoostDistributionalRegressor     Fitted estimator.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict mean (expected value).</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.predict--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Features to predict on.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.predict--returns","title":"Returns","text":"<p>y_pred : ndarray of shape (n_samples,)     Predicted mean values.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.predict_interval","title":"predict_interval","text":"<pre><code>predict_interval(X, alpha=0.1)\n</code></pre> <p>Predict (1-alpha) prediction interval.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.predict_interval--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Features to predict on. alpha : float, default=0.1     Significance level. 0.1 gives a 90% prediction interval.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.predict_interval--returns","title":"Returns","text":"<p>lower : ndarray of shape (n_samples,)     Lower bounds of the interval. upper : ndarray of shape (n_samples,)     Upper bounds of the interval.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.predict_distribution","title":"predict_distribution","text":"<pre><code>predict_distribution(X)\n</code></pre> <p>Predict all distribution parameters.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.predict_distribution--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Features to predict on.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.predict_distribution--returns","title":"Returns","text":"<p>params : dict     Dictionary mapping parameter names to predicted values.     For Normal: {'loc': mean, 'scale': std}</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.predict_quantile","title":"predict_quantile","text":"<pre><code>predict_quantile(X, q)\n</code></pre> <p>Predict q-th quantile.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.predict_quantile--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Features to predict on. q : float     Quantile level (0 &lt; q &lt; 1).</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.predict_quantile--returns","title":"Returns","text":"<p>quantiles : ndarray of shape (n_samples,)     Predicted quantiles.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.sample","title":"sample","text":"<pre><code>sample(X, n_samples=1, seed=None)\n</code></pre> <p>Sample from predicted distribution.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.sample--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_obs, n_features)     Features to predict on. n_samples : int, default=1     Number of samples per observation. seed : int, optional     Random seed for reproducibility.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.sample--returns","title":"Returns","text":"<p>samples : ndarray of shape (n_obs, n_samples)     Samples from the predicted distribution.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.nll_score","title":"nll_score","text":"<pre><code>nll_score(X, y)\n</code></pre> <p>Compute negative log-likelihood (lower is better).</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.nll_score--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Features. y : array-like of shape (n_samples,)     True target values.</p>"},{"location":"api/models/#openboost.OpenBoostDistributionalRegressor.nll_score--returns","title":"Returns","text":"<p>nll : float     Mean negative log-likelihood.</p>"},{"location":"api/models/#openboostlinearleafregressor","title":"OpenBoostLinearLeafRegressor","text":""},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor","title":"OpenBoostLinearLeafRegressor","text":"<pre><code>OpenBoostLinearLeafRegressor(\n    n_estimators=100,\n    max_depth=4,\n    learning_rate=0.1,\n    loss=\"squared_error\",\n    min_samples_leaf=20,\n    reg_lambda=1.0,\n    reg_lambda_linear=0.1,\n    max_features_linear=\"sqrt\",\n    n_bins=256,\n    verbose=0,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Linear Leaf Gradient Boosting with sklearn-compatible interface.</p> <p>Uses trees with linear models in leaves instead of constant values. This provides better extrapolation beyond the training data range.</p>"},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor--parameters","title":"Parameters","text":"<p>n_estimators : int, default=100     Number of boosting rounds. max_depth : int, default=4     Maximum tree depth. Typically shallower than standard GBDT since     linear models in leaves add flexibility. learning_rate : float, default=0.1     Shrinkage factor. loss : str, default='squared_error'     Loss function: 'squared_error', 'absolute_error', 'huber'. min_samples_leaf : int, default=20     Minimum samples in a leaf to fit linear model. reg_lambda : float, default=1.0     L2 regularization for tree splits. reg_lambda_linear : float, default=0.1     L2 regularization for linear models in leaves (ridge). max_features_linear : int, str, or None, default='sqrt'     Max features for linear model in each leaf:     - None: Use all features     - 'sqrt': Use sqrt(n_features)     - 'log2': Use log2(n_features)     - int: Use exactly this many features n_bins : int, default=256     Number of bins for histogram building. verbose : int, default=0     Verbosity level.</p>"},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor--attributes","title":"Attributes","text":"<p>n_features_in_ : int     Number of features seen during fit. booster_ : LinearLeafGBDT     The underlying fitted model.</p>"},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor--examples","title":"Examples","text":"<p>from openboost import OpenBoostLinearLeafRegressor model = OpenBoostLinearLeafRegressor(n_estimators=100, max_depth=4) model.fit(X_train, y_train) y_pred = model.predict(X_test)</p>"},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor--compare-with-standard-gbdt-on-extrapolation-tasks","title":"Compare with standard GBDT on extrapolation tasks","text":""},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor--linearleafregressor-typically-performs-better-when-the","title":"LinearLeafRegressor typically performs better when the","text":""},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor--underlying-relationship-has-linear-components","title":"underlying relationship has linear components","text":""},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor.fit","title":"fit","text":"<pre><code>fit(X, y, **kwargs)\n</code></pre> <p>Fit the linear leaf regressor.</p>"},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor.fit--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Training features. y : array-like of shape (n_samples,)     Target values.</p>"},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor.fit--returns","title":"Returns","text":"<p>self : OpenBoostLinearLeafRegressor     Fitted estimator.</p>"},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict target values.</p>"},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor.predict--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Features to predict on.</p>"},{"location":"api/models/#openboost.OpenBoostLinearLeafRegressor.predict--returns","title":"Returns","text":"<p>y_pred : ndarray of shape (n_samples,)     Predicted values.</p>"},{"location":"api/openboost/","title":"openboost","text":"<p>Core module - main entry point for OpenBoost.</p>"},{"location":"api/openboost/#quick-reference","title":"Quick Reference","text":"<pre><code>import openboost as ob\n\n# Check version and backend\nprint(ob.__version__)\nprint(ob.get_backend())  # \"cuda\" or \"cpu\"\n\n# Data binning\nX_binned = ob.array(X, n_bins=256)\n\n# Models\nmodel = ob.GradientBoosting(n_trees=100)\nmodel = ob.NaturalBoostNormal(n_trees=100)\nmodel = ob.OpenBoostGAM(n_rounds=500)\n</code></pre>"},{"location":"api/openboost/#data-layer","title":"Data Layer","text":""},{"location":"api/openboost/#openboost.array","title":"array","text":"<pre><code>array(\n    X, n_bins=256, *, categorical_features=None, device=None\n)\n</code></pre> <p>Convert input data to binned format for tree building.</p> <p>This is the primary entry point for data. Binning is done once, then the binned data can be used for training many models.</p> <p>Missing values (NaN) are automatically detected and encoded as bin 255. The model learns the optimal direction for missing values at each split.</p> <p>Categorical features use native category encoding instead of quantile binning, enabling the model to learn optimal category groupings.</p> PARAMETER DESCRIPTION <code>X</code> <p>Input features, shape (n_samples, n_features) Accepts numpy arrays, PyTorch tensors, JAX arrays, CuPy arrays. NaN values are handled automatically.</p> <p> TYPE: <code>ArrayLike</code> </p> <code>n_bins</code> <p>Maximum number of bins for numeric features (max 255).</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> <code>categorical_features</code> <p>List of column indices that are categorical.                  These use category encoding instead of quantile binning.                  Max 254 unique categories per feature (255 reserved for NaN).</p> <p> TYPE: <code>Sequence[int] | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Target device (\"cuda\" or \"cpu\"). Auto-detected if None.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>BinnedArray</code> <p>BinnedArray with binned data in feature-major layout (n_features, n_samples).</p> <code>BinnedArray</code> <p>NaN values are encoded as MISSING_BIN (255).</p> Example <p>import openboost as ob import numpy as np</p>"},{"location":"api/openboost/#openboost.array--numeric-features-with-missing-values","title":"Numeric features with missing values","text":"<p>X = np.array([[1.0, np.nan], [2.0, 3.0], [np.nan, 4.0]]) X_binned = ob.array(X) print(X_binned.has_missing)  # [True, True]</p>"},{"location":"api/openboost/#openboost.array--mixed-numeric-and-categorical","title":"Mixed numeric and categorical","text":"<p>X = np.array([[25, 0, 50000], [30, 1, 60000], [35, 2, 70000]]) X_binned = ob.array(X, categorical_features=[1])  # Feature 1 is categorical print(X_binned.is_categorical)  # [False, True, False]</p>"},{"location":"api/openboost/#openboost.BinnedArray","title":"BinnedArray  <code>dataclass</code>","text":"<pre><code>BinnedArray(\n    data,\n    bin_edges,\n    n_features,\n    n_samples,\n    device,\n    has_missing=(lambda: array([], dtype=bool_))(),\n    is_categorical=(lambda: array([], dtype=bool_))(),\n    category_maps=list(),\n    n_categories=(lambda: array([], dtype=int32))(),\n)\n</code></pre> <p>Binned feature matrix ready for tree building.</p> ATTRIBUTE DESCRIPTION <code>data</code> <p>Binned data, shape (n_features, n_samples), dtype uint8   NaN values are encoded as bin 255 (MISSING_BIN)</p> <p> TYPE: <code>NDArray[uint8]</code> </p> <code>bin_edges</code> <p>List of bin edges per feature, for inverse transform</p> <p> TYPE: <code>list[NDArray[float64]]</code> </p> <code>n_features</code> <p>Number of features</p> <p> TYPE: <code>int</code> </p> <code>n_samples</code> <p>Number of samples</p> <p> TYPE: <code>int</code> </p> <code>device</code> <p>\"cuda\" or \"cpu\"</p> <p> TYPE: <code>str</code> </p> <code>has_missing</code> <p>Boolean array (n_features,) indicating which features have NaN</p> <p> TYPE: <code>NDArray[bool_]</code> </p> <code>is_categorical</code> <p>Boolean array (n_features,) indicating categorical features</p> <p> TYPE: <code>NDArray[bool_]</code> </p> <code>category_maps</code> <p>List of dicts mapping original values -&gt; bin indices (None for numeric)</p> <p> TYPE: <code>list[dict | None]</code> </p> <code>n_categories</code> <p>Number of categories per feature (0 for numeric)</p> <p> TYPE: <code>NDArray[int32]</code> </p>"},{"location":"api/openboost/#openboost.BinnedArray.any_missing","title":"any_missing  <code>property</code>","text":"<pre><code>any_missing\n</code></pre> <p>Check if any feature has missing values.</p>"},{"location":"api/openboost/#openboost.BinnedArray.any_categorical","title":"any_categorical  <code>property</code>","text":"<pre><code>any_categorical\n</code></pre> <p>Check if any feature is categorical.</p>"},{"location":"api/openboost/#openboost.BinnedArray.transform","title":"transform","text":"<pre><code>transform(X)\n</code></pre> <p>Transform new data using the bin edges from this BinnedArray.</p> <p>Use this method to transform test/validation data using the same binning learned from training data. This ensures tree splits work correctly across train and test sets.</p> PARAMETER DESCRIPTION <code>X</code> <p>New input features, shape (n_samples_new, n_features). Must have the same number of features as the training data.</p> <p> TYPE: <code>ArrayLike</code> </p> RETURNS DESCRIPTION <code>'BinnedArray'</code> <p>BinnedArray with new data binned using training bin edges.</p> Example <p>X_train_binned = ob.array(X_train) model.fit(X_train_binned, y_train) X_test_binned = X_train_binned.transform(X_test) predictions = model.predict(X_test_binned)</p>"},{"location":"api/openboost/#backend-control","title":"Backend Control","text":""},{"location":"api/openboost/#openboost.get_backend","title":"get_backend","text":"<pre><code>get_backend()\n</code></pre> <p>Get the current compute backend.</p> RETURNS DESCRIPTION <code>Literal['cuda', 'cpu']</code> <p>\"cuda\" if NVIDIA GPU is available, \"cpu\" otherwise.</p>"},{"location":"api/openboost/#openboost.set_backend","title":"set_backend","text":"<pre><code>set_backend(backend)\n</code></pre> <p>Force a specific backend.</p> PARAMETER DESCRIPTION <code>backend</code> <p>\"cuda\" or \"cpu\"</p> <p> TYPE: <code>Literal['cuda', 'cpu']</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If backend is not \"cuda\" or \"cpu\"</p> <code>RuntimeError</code> <p>If CUDA is requested but not available</p>"},{"location":"api/openboost/#openboost.is_cuda","title":"is_cuda","text":"<pre><code>is_cuda()\n</code></pre> <p>Check if using CUDA backend.</p>"},{"location":"api/openboost/#low-level-tree-building","title":"Low-Level Tree Building","text":""},{"location":"api/openboost/#openboost.fit_tree","title":"fit_tree","text":"<pre><code>fit_tree(\n    X,\n    grad,\n    hess,\n    *,\n    max_depth=6,\n    min_child_weight=1.0,\n    reg_lambda=1.0,\n    reg_alpha=0.0,\n    min_gain=0.0,\n    gamma=None,\n    growth=\"levelwise\",\n    max_leaves=None,\n    subsample=1.0,\n    colsample_bytree=1.0,\n)\n</code></pre> <p>Fit a single gradient boosting tree.</p> <p>This is the core function of OpenBoost. It builds a tree using the specified growth strategy and returns a TreeStructure that can be used for prediction.</p> <p>Phase 8: Uses composable growth strategies from _growth.py. Phase 11: Added reg_alpha, subsample, colsample_bytree. Phase 14: Handles missing values automatically via BinnedArray.has_missing.</p> PARAMETER DESCRIPTION <code>X</code> <p>Binned feature data (BinnedArray from ob.array(), or raw binned array) Missing values (NaN in original data) are encoded as bin 255.</p> <p> TYPE: <code>BinnedArray | NDArray</code> </p> <code>grad</code> <p>Gradient vector, shape (n_samples,), float32</p> <p> TYPE: <code>NDArray</code> </p> <code>hess</code> <p>Hessian vector, shape (n_samples,), float32</p> <p> TYPE: <code>NDArray</code> </p> <code>max_depth</code> <p>Maximum tree depth</p> <p> TYPE: <code>int</code> DEFAULT: <code>6</code> </p> <code>min_child_weight</code> <p>Minimum sum of hessian in a leaf</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>reg_lambda</code> <p>L2 regularization on leaf values</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>reg_alpha</code> <p>L1 regularization on leaf values (Phase 11)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>min_gain</code> <p>Minimum gain to make a split</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>gamma</code> <p>Alias for min_gain (XGBoost compatibility)</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>growth</code> <p>Growth strategy - \"levelwise\", \"leafwise\", \"symmetric\",      or a GrowthStrategy instance</p> <p> TYPE: <code>str | GrowthStrategy</code> DEFAULT: <code>'levelwise'</code> </p> <code>max_leaves</code> <p>Maximum leaves (for leafwise growth)</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>subsample</code> <p>Row sampling ratio (0.0-1.0), 1.0 = no sampling (Phase 11)</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>colsample_bytree</code> <p>Column sampling ratio (0.0-1.0), 1.0 = no sampling (Phase 11)</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>TreeStructure</code> <p>TreeStructure that can predict via tree.predict(X) or tree(X)</p> Example <p>import openboost as ob import numpy as np</p>"},{"location":"api/openboost/#openboost.fit_tree--missing-values-handled-automatically","title":"Missing values handled automatically","text":"<p>X_train = np.array([[1.0, np.nan], [2.0, 3.0], [np.nan, 4.0]]) X_binned = ob.array(X_train) pred = np.zeros(3, dtype=np.float32)</p> <p>for round in range(100): ...     grad = 2 * (pred - y)  # MSE gradient ...     hess = np.ones_like(grad) * 2 ...     tree = ob.fit_tree(X_binned, grad, hess) ...     pred = pred + 0.1 * tree.predict(X_binned)</p>"},{"location":"api/openboost/#openboost.fit_tree--use-leaf-wise-growth-lightgbm-style","title":"Use leaf-wise growth (LightGBM style)","text":"<p>tree = ob.fit_tree(X_binned, grad, hess, growth=\"leafwise\", max_leaves=32)</p>"},{"location":"api/openboost/#openboost.fit_tree--use-symmetric-growth-catboost-style","title":"Use symmetric growth (CatBoost style)","text":"<p>tree = ob.fit_tree(X_binned, grad, hess, growth=\"symmetric\")</p>"},{"location":"api/openboost/#openboost.fit_tree--stochastic-gradient-boosting-phase-11","title":"Stochastic gradient boosting (Phase 11)","text":"<p>tree = ob.fit_tree(X_binned, grad, hess, subsample=0.8, colsample_bytree=0.8)</p>"},{"location":"api/openboost/#openboost.predict_tree","title":"predict_tree","text":"<pre><code>predict_tree(tree, X)\n</code></pre> <p>Predict using a fitted tree.</p> PARAMETER DESCRIPTION <code>tree</code> <p>Fitted Tree object</p> <p> TYPE: <code>Tree</code> </p> <code>X</code> <p>BinnedArray or binned data (n_features, n_samples)</p> <p> TYPE: <code>BinnedArray | NDArray</code> </p> RETURNS DESCRIPTION <code>predictions</code> <p>Shape (n_samples,), float32</p> <p> TYPE: <code>NDArray</code> </p>"},{"location":"api/openboost/#openboost.predict_ensemble","title":"predict_ensemble","text":"<pre><code>predict_ensemble(\n    trees, X, learning_rate=1.0, init_score=0.0\n)\n</code></pre> <p>Predict using an ensemble of trees.</p> PARAMETER DESCRIPTION <code>trees</code> <p>List of fitted Tree objects</p> <p> TYPE: <code>list[Tree]</code> </p> <code>X</code> <p>BinnedArray or binned data</p> <p> TYPE: <code>BinnedArray | NDArray</code> </p> <code>learning_rate</code> <p>Learning rate to apply to each tree</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>init_score</code> <p>Initial prediction value</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> RETURNS DESCRIPTION <code>predictions</code> <p>Shape (n_samples,)</p> <p> TYPE: <code>NDArray</code> </p>"},{"location":"api/utils/","title":"Utilities","text":"<p>Helper functions for tuning, cross-validation, and evaluation.</p>"},{"location":"api/utils/#parameter-tuning","title":"Parameter Tuning","text":""},{"location":"api/utils/#suggest_params","title":"suggest_params","text":""},{"location":"api/utils/#openboost.suggest_params","title":"suggest_params","text":"<pre><code>suggest_params(\n    X, y, task=\"regression\", n_estimators_cap=500\n)\n</code></pre> <p>Suggest hyperparameters based on dataset characteristics.</p> <p>This provides reasonable starting points based on heuristics. For best results, use these as initial values and tune with cross-validation.</p> PARAMETER DESCRIPTION <code>X</code> <p>Feature matrix, shape (n_samples, n_features).</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Target values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>task</code> <p>Type of task - 'regression', 'classification', or 'distributional'.</p> <p> TYPE: <code>Literal['regression', 'classification', 'distributional']</code> DEFAULT: <code>'regression'</code> </p> <code>n_estimators_cap</code> <p>Maximum number of estimators to suggest.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dictionary of suggested hyperparameters suitable for passing to</p> <code>dict[str, Any]</code> <p>OpenBoostRegressor, OpenBoostClassifier, etc.</p> Example <p>params = suggest_params(X_train, y_train, task='regression') model = OpenBoostRegressor(**params) model.fit(X_train, y_train)</p> Notes <ul> <li>For small datasets (&lt; 1000 samples): Fewer trees, more regularization</li> <li>For large datasets (&gt; 100k samples): More trees, lower learning rate</li> <li>For high-dimensional data: More column sampling, shallower trees</li> <li>For noisy data: Consider distributional models for uncertainty</li> </ul>"},{"location":"api/utils/#get_param_grid","title":"get_param_grid","text":""},{"location":"api/utils/#openboost.get_param_grid","title":"get_param_grid","text":"<pre><code>get_param_grid(task='regression')\n</code></pre> <p>Get a suggested parameter grid for hyperparameter tuning.</p> PARAMETER DESCRIPTION <code>task</code> <p>Type of task - 'regression', 'classification', or 'distributional'.</p> <p> TYPE: <code>Literal['regression', 'classification', 'distributional']</code> DEFAULT: <code>'regression'</code> </p> RETURNS DESCRIPTION <code>dict[str, list]</code> <p>Dictionary of parameter names to lists of values, suitable for</p> <code>dict[str, list]</code> <p>sklearn's GridSearchCV or RandomizedSearchCV.</p> Example <p>from sklearn.model_selection import GridSearchCV from openboost import OpenBoostRegressor from openboost.utils import get_param_grid</p> <p>param_grid = get_param_grid('regression') search = GridSearchCV(OpenBoostRegressor(), param_grid, cv=3) search.fit(X, y) print(search.best_params_)</p>"},{"location":"api/utils/#cross-validation","title":"Cross-Validation","text":""},{"location":"api/utils/#cross_val_predict","title":"cross_val_predict","text":""},{"location":"api/utils/#openboost.cross_val_predict","title":"cross_val_predict","text":"<pre><code>cross_val_predict(model, X, y, cv=5, random_state=42)\n</code></pre> <p>Generate out-of-fold predictions using cross-validation.</p> <p>Each sample gets a prediction from a model that was not trained on it. Useful for stacking/blending in competitions and for honest evaluation.</p> PARAMETER DESCRIPTION <code>model</code> <p>An OpenBoost model instance (will be cloned for each fold).</p> <p> TYPE: <code>Any</code> </p> <code>X</code> <p>Feature matrix, shape (n_samples, n_features).</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Target values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>cv</code> <p>Number of cross-validation folds.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>random_state</code> <p>Random seed for reproducible fold splits.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>42</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>Out-of-fold predictions, shape (n_samples,) for regression or</p> <code>NDArray</code> <p>shape (n_samples, n_classes) for classification probabilities.</p> Example <p>from openboost import OpenBoostRegressor from openboost.utils import cross_val_predict</p> <p>model = OpenBoostRegressor(n_estimators=100) oof_pred = cross_val_predict(model, X, y, cv=5)</p>"},{"location":"api/utils/#openboost.cross_val_predict--use-oof-predictions-for-stacking","title":"Use OOF predictions for stacking","text":"<p>from sklearn.linear_model import Ridge meta_model = Ridge() meta_model.fit(oof_pred.reshape(-1, 1), y)</p>"},{"location":"api/utils/#cross_val_predict_proba","title":"cross_val_predict_proba","text":""},{"location":"api/utils/#openboost.cross_val_predict_proba","title":"cross_val_predict_proba","text":"<pre><code>cross_val_predict_proba(model, X, y, cv=5, random_state=42)\n</code></pre> <p>Generate out-of-fold probability predictions using cross-validation.</p> <p>Similar to cross_val_predict but returns class probabilities instead of class labels. Only works with classifiers.</p> PARAMETER DESCRIPTION <code>model</code> <p>An OpenBoost classifier instance (must have predict_proba).</p> <p> TYPE: <code>Any</code> </p> <code>X</code> <p>Feature matrix, shape (n_samples, n_features).</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Target labels, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>cv</code> <p>Number of cross-validation folds.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>random_state</code> <p>Random seed for reproducible fold splits.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>42</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>Out-of-fold probability predictions, shape (n_samples, n_classes).</p> Example <p>from openboost import OpenBoostClassifier from openboost.utils import cross_val_predict_proba</p> <p>model = OpenBoostClassifier(n_estimators=100) oof_proba = cross_val_predict_proba(model, X, y, cv=5)</p> RAISES DESCRIPTION <code>AttributeError</code> <p>If model doesn't have predict_proba method.</p>"},{"location":"api/utils/#openboost.cross_val_predict_proba--use-probabilities-for-stacking","title":"Use probabilities for stacking","text":"<p>meta_features = oof_proba[:, 1]  # P(class=1)</p>"},{"location":"api/utils/#cross_val_predict_interval","title":"cross_val_predict_interval","text":""},{"location":"api/utils/#openboost.cross_val_predict_interval","title":"cross_val_predict_interval","text":"<pre><code>cross_val_predict_interval(\n    model, X, y, alpha=0.1, cv=5, random_state=42\n)\n</code></pre> <p>Generate out-of-fold prediction intervals using cross-validation.</p> <p>For distributional models that support uncertainty quantification. Returns lower and upper bounds of the prediction interval.</p> PARAMETER DESCRIPTION <code>model</code> <p>An OpenBoost distributional model (must have predict_interval).</p> <p> TYPE: <code>Any</code> </p> <code>X</code> <p>Feature matrix, shape (n_samples, n_features).</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Target values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>alpha</code> <p>Significance level (0.1 = 90% interval).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>cv</code> <p>Number of cross-validation folds.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>random_state</code> <p>Random seed for reproducible fold splits.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>42</code> </p> RETURNS DESCRIPTION <code>tuple[NDArray, NDArray]</code> <p>Tuple of (lower_bounds, upper_bounds), each shape (n_samples,).</p> Example <p>from openboost import OpenBoostDistributionalRegressor from openboost.utils import cross_val_predict_interval</p> <p>model = OpenBoostDistributionalRegressor(distribution='normal') lower, upper = cross_val_predict_interval(model, X, y, alpha=0.1)</p> RAISES DESCRIPTION <code>AttributeError</code> <p>If model doesn't have predict_interval method.</p>"},{"location":"api/utils/#openboost.cross_val_predict_interval--check-coverage","title":"Check coverage","text":"<p>coverage = np.mean((y &gt;= lower) &amp; (y &lt;= upper)) print(f\"90% interval coverage: {coverage:.2%}\")</p>"},{"location":"api/utils/#evaluate_coverage","title":"evaluate_coverage","text":""},{"location":"api/utils/#openboost.evaluate_coverage","title":"evaluate_coverage","text":"<pre><code>evaluate_coverage(y_true, lower, upper, alpha=0.1)\n</code></pre> <p>Evaluate prediction interval coverage and width.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True target values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>lower</code> <p>Lower bounds of intervals, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>upper</code> <p>Upper bounds of intervals, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>alpha</code> <p>Expected significance level (for reporting).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> RETURNS DESCRIPTION <code>dict[str, float]</code> <p>Dictionary with:</p> <code>dict[str, float]</code> <ul> <li>coverage: Fraction of true values within intervals</li> </ul> <code>dict[str, float]</code> <ul> <li>expected_coverage: Expected coverage (1 - alpha)</li> </ul> <code>dict[str, float]</code> <ul> <li>mean_width: Average interval width</li> </ul> <code>dict[str, float]</code> <ul> <li>median_width: Median interval width</li> </ul> Example <p>lower, upper = model.predict_interval(X_test, alpha=0.1) metrics = evaluate_coverage(y_test, lower, upper, alpha=0.1) print(f\"Coverage: {metrics['coverage']:.2%}\") print(f\"Mean width: {metrics['mean_width']:.4f}\")</p>"},{"location":"api/utils/#feature-importance","title":"Feature Importance","text":""},{"location":"api/utils/#compute_feature_importances","title":"compute_feature_importances","text":""},{"location":"api/utils/#openboost.compute_feature_importances","title":"compute_feature_importances","text":"<pre><code>compute_feature_importances(\n    model, importance_type=\"frequency\", normalize=True\n)\n</code></pre> <p>Compute feature importances from any tree-based model.</p> <p>Works with: GradientBoosting, DART, OpenBoostGAM, MultiClassGradientBoosting, and any model with a <code>trees_</code> attribute.</p> PARAMETER DESCRIPTION <code>model</code> <p>A fitted model with <code>trees_</code> attribute.</p> <p> TYPE: <code>Any</code> </p> <code>importance_type</code> <p>Type of importance calculation: - 'frequency': Number of times feature is used for splits (default) - 'gain': Sum of gain from splits on each feature (if available) - 'cover': Sum of samples covered by splits (if available)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'frequency'</code> </p> <code>normalize</code> <p>If True, normalize importances to sum to 1.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>importances</code> <p>Array of shape (n_features,) with importance scores.</p> <p> TYPE: <code>NDArray</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If model has no trees or unknown importance_type.</p> Example <p>model = ob.GradientBoosting(n_trees=100).fit(X, y) importances = compute_feature_importances(model)</p>"},{"location":"api/utils/#openboost.compute_feature_importances--use-with-sklearn-style-attribute","title":"Use with sklearn-style attribute","text":"<p>model.feature_importances_ = compute_feature_importances(model)</p>"},{"location":"api/utils/#get_feature_importance_dict","title":"get_feature_importance_dict","text":""},{"location":"api/utils/#openboost.get_feature_importance_dict","title":"get_feature_importance_dict","text":"<pre><code>get_feature_importance_dict(\n    model,\n    feature_names=None,\n    importance_type=\"frequency\",\n    top_n=None,\n)\n</code></pre> <p>Get feature importances as a sorted dictionary.</p> <p>Convenience function that returns importances as a dict, optionally with feature names and limited to top N features.</p> PARAMETER DESCRIPTION <code>model</code> <p>Fitted tree-based model.</p> <p> TYPE: <code>Any</code> </p> <code>feature_names</code> <p>Optional list of feature names.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>importance_type</code> <p>Type of importance ('frequency', 'gain', 'cover').</p> <p> TYPE: <code>str</code> DEFAULT: <code>'frequency'</code> </p> <code>top_n</code> <p>If provided, return only top N features.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, float]</code> <p>Dict mapping feature name/index to importance, sorted by importance.</p> Example <p>importance_dict = get_feature_importance_dict( ...     model,  ...     feature_names=['age', 'income', 'score'], ...     top_n=2 ... )</p>"},{"location":"api/utils/#openboost.get_feature_importance_dict--income-045-age-032","title":"{'income': 0.45, 'age': 0.32}","text":""},{"location":"api/utils/#plot_feature_importances","title":"plot_feature_importances","text":""},{"location":"api/utils/#openboost.plot_feature_importances","title":"plot_feature_importances","text":"<pre><code>plot_feature_importances(\n    model,\n    feature_names=None,\n    importance_type=\"frequency\",\n    top_n=20,\n    ax=None,\n    **kwargs,\n)\n</code></pre> <p>Plot feature importances as a horizontal bar chart.</p> PARAMETER DESCRIPTION <code>model</code> <p>Fitted tree-based model.</p> <p> TYPE: <code>Any</code> </p> <code>feature_names</code> <p>Optional list of feature names.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>importance_type</code> <p>Type of importance ('frequency', 'gain', 'cover').</p> <p> TYPE: <code>str</code> DEFAULT: <code>'frequency'</code> </p> <code>top_n</code> <p>Number of top features to show.</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ax</code> <p>Matplotlib axes to plot on (creates new if None).</p> <p> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments passed to barh().</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <p>Matplotlib axes object.</p> Example <p>plot_feature_importances(model, top_n=10) plt.show()</p>"},{"location":"api/utils/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"api/utils/#regression","title":"Regression","text":""},{"location":"api/utils/#openboost.mse_score","title":"mse_score","text":"<pre><code>mse_score(y_true, y_pred, *, sample_weight=None)\n</code></pre> <p>Compute Mean Squared Error.</p> <p>Thin wrapper around sklearn.metrics.mean_squared_error with sample weight support.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>y_pred</code> <p>Predicted values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>MSE (lower is better). Perfect predictions = 0.</p> Example <p>import openboost as ob y_true = np.array([1.0, 2.0, 3.0]) y_pred = np.array([1.1, 2.0, 2.8]) ob.mse_score(y_true, y_pred) 0.016666...</p>"},{"location":"api/utils/#openboost.mae_score","title":"mae_score","text":"<pre><code>mae_score(y_true, y_pred, *, sample_weight=None)\n</code></pre> <p>Compute Mean Absolute Error.</p> <p>Thin wrapper around sklearn.metrics.mean_absolute_error with sample weight support.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>y_pred</code> <p>Predicted values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>MAE (lower is better). Perfect predictions = 0.</p> Example <p>import openboost as ob y_true = np.array([1.0, 2.0, 3.0]) y_pred = np.array([1.1, 2.0, 2.8]) ob.mae_score(y_true, y_pred) 0.1</p>"},{"location":"api/utils/#openboost.rmse_score","title":"rmse_score","text":"<pre><code>rmse_score(y_true, y_pred, *, sample_weight=None)\n</code></pre> <p>Compute Root Mean Squared Error.</p> <p>Thin wrapper around sklearn.metrics.mean_squared_error with squared=False.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>y_pred</code> <p>Predicted values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>RMSE (lower is better). Perfect predictions = 0.</p> Example <p>import openboost as ob y_true = np.array([1.0, 2.0, 3.0]) y_pred = np.array([1.1, 2.0, 2.8]) ob.rmse_score(y_true, y_pred) 0.1291...</p>"},{"location":"api/utils/#openboost.r2_score","title":"r2_score","text":"<pre><code>r2_score(y_true, y_pred, *, sample_weight=None)\n</code></pre> <p>Compute R\u00b2 (coefficient of determination).</p> <p>Thin wrapper around sklearn.metrics.r2_score with sample weight support.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>y_pred</code> <p>Predicted values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>R\u00b2 score. Perfect predictions = 1.0, baseline (mean) = 0.0.</p> Example <p>import openboost as ob y_true = np.array([1.0, 2.0, 3.0, 4.0]) y_pred = np.array([1.1, 1.9, 3.1, 3.9]) ob.r2_score(y_true, y_pred) 0.98</p>"},{"location":"api/utils/#classification","title":"Classification","text":""},{"location":"api/utils/#openboost.accuracy_score","title":"accuracy_score","text":"<pre><code>accuracy_score(y_true, y_pred, *, sample_weight=None)\n</code></pre> <p>Compute classification accuracy.</p> <p>Thin wrapper around sklearn.metrics.accuracy_score with sample weight support.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True labels, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>y_pred</code> <p>Predicted labels, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Accuracy score between 0 and 1.</p> Example <p>import openboost as ob y_true = np.array([0, 1, 1, 0]) y_pred = np.array([0, 1, 0, 0]) ob.accuracy_score(y_true, y_pred) 0.75</p>"},{"location":"api/utils/#openboost.roc_auc_score","title":"roc_auc_score","text":"<pre><code>roc_auc_score(y_true, y_score, *, sample_weight=None)\n</code></pre> <p>Compute Area Under the ROC Curve (AUC).</p> <p>Thin wrapper around sklearn.metrics.roc_auc_score with sample weight support.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True binary labels, shape (n_samples,). Values should be 0 or 1.</p> <p> TYPE: <code>NDArray</code> </p> <code>y_score</code> <p>Predicted scores/probabilities, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>AUC score between 0 and 1. Random classifier = 0.5, perfect = 1.0.</p> Example <p>import openboost as ob y_true = np.array([0, 0, 1, 1]) y_score = np.array([0.1, 0.4, 0.35, 0.8]) ob.roc_auc_score(y_true, y_score) 0.75</p>"},{"location":"api/utils/#openboost.log_loss_score","title":"log_loss_score","text":"<pre><code>log_loss_score(y_true, y_pred, *, sample_weight=None)\n</code></pre> <p>Compute log loss (cross-entropy loss) for binary classification.</p> <p>Thin wrapper around sklearn.metrics.log_loss with sample weight support.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True binary labels, shape (n_samples,). Values should be 0 or 1.</p> <p> TYPE: <code>NDArray</code> </p> <code>y_pred</code> <p>Predicted probabilities for positive class, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Log loss (lower is better). Perfect predictions = 0.</p> Example <p>import openboost as ob y_true = np.array([0, 0, 1, 1]) y_pred = np.array([0.1, 0.2, 0.7, 0.9]) ob.log_loss_score(y_true, y_pred) 0.1738...</p>"},{"location":"api/utils/#openboost.f1_score","title":"f1_score","text":"<pre><code>f1_score(\n    y_true, y_pred, *, average=\"binary\", sample_weight=None\n)\n</code></pre> <p>Compute F1 score (harmonic mean of precision and recall).</p> <p>Thin wrapper around sklearn.metrics.f1_score with sample weight support.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True labels, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>y_pred</code> <p>Predicted labels, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>average</code> <p>Averaging method for multi-class: - 'binary': Only for binary classification. - 'micro': Global TP, FP, FN counts. - 'macro': Unweighted mean of per-class F1. - 'weighted': Weighted mean by support.</p> <p> TYPE: <code>Literal['binary', 'micro', 'macro', 'weighted']</code> DEFAULT: <code>'binary'</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>F1 score between 0 and 1.</p> Example <p>import openboost as ob y_true = np.array([0, 1, 1, 0, 1]) y_pred = np.array([0, 1, 0, 0, 1]) ob.f1_score(y_true, y_pred) 0.8</p>"},{"location":"api/utils/#openboost.precision_score","title":"precision_score","text":"<pre><code>precision_score(\n    y_true, y_pred, *, average=\"binary\", sample_weight=None\n)\n</code></pre> <p>Compute precision (positive predictive value).</p> <p>Thin wrapper around sklearn.metrics.precision_score with sample weight support.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True labels, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>y_pred</code> <p>Predicted labels, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>average</code> <p>Averaging method for multi-class: - 'binary': Only for binary classification. - 'micro': Global TP, FP counts. - 'macro': Unweighted mean of per-class precision. - 'weighted': Weighted mean by support.</p> <p> TYPE: <code>Literal['binary', 'micro', 'macro', 'weighted']</code> DEFAULT: <code>'binary'</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Precision score between 0 and 1.</p> Example <p>import openboost as ob y_true = np.array([0, 1, 1, 0, 1]) y_pred = np.array([0, 1, 0, 1, 1]) ob.precision_score(y_true, y_pred) 0.666...</p>"},{"location":"api/utils/#openboost.recall_score","title":"recall_score","text":"<pre><code>recall_score(\n    y_true, y_pred, *, average=\"binary\", sample_weight=None\n)\n</code></pre> <p>Compute recall (sensitivity, true positive rate).</p> <p>Thin wrapper around sklearn.metrics.recall_score with sample weight support.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True labels, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>y_pred</code> <p>Predicted labels, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>average</code> <p>Averaging method for multi-class: - 'binary': Only for binary classification. - 'micro': Global TP, FN counts. - 'macro': Unweighted mean of per-class recall. - 'weighted': Weighted mean by support.</p> <p> TYPE: <code>Literal['binary', 'micro', 'macro', 'weighted']</code> DEFAULT: <code>'binary'</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Recall score between 0 and 1.</p> Example <p>import openboost as ob y_true = np.array([0, 1, 1, 0, 1]) y_pred = np.array([0, 1, 0, 0, 1]) ob.recall_score(y_true, y_pred) 0.666...</p>"},{"location":"api/utils/#probabilistic-metrics","title":"Probabilistic Metrics","text":""},{"location":"api/utils/#openboost.crps_gaussian","title":"crps_gaussian","text":"<pre><code>crps_gaussian(y_true, mean, std, *, sample_weight=None)\n</code></pre> <p>Compute Continuous Ranked Probability Score for Gaussian predictions.</p> <p>CRPS is a strictly proper scoring rule that measures the quality of probabilistic predictions. Lower is better. For Gaussian distributions, there's a closed-form solution.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>mean</code> <p>Predicted mean, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>std</code> <p>Predicted standard deviation, shape (n_samples,). Must be &gt; 0.</p> <p> TYPE: <code>NDArray</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Mean CRPS (lower is better). Perfect calibration minimizes CRPS.</p> Example <p>import openboost as ob y_true = np.array([1.0, 2.0, 3.0]) mean = np.array([1.1, 2.0, 2.8]) std = np.array([0.5, 0.5, 0.5]) ob.crps_gaussian(y_true, mean, std) 0.123...</p> Notes <p>CRPS formula for Gaussian: CRPS(N(\u03bc,\u03c3\u00b2), y) = \u03c3 * [z*\u03a6(z) + \u03c6(z) - 1/\u221a\u03c0] where z = (y - \u03bc) / \u03c3, \u03a6 is CDF, \u03c6 is PDF of standard normal.</p> <p>For NaturalBoost models, use:</p> <p>output = model.predict_distribution(X) mean, std = output.params[:, 0], np.sqrt(output.params[:, 1]) crps = ob.crps_gaussian(y_true, mean, std)</p>"},{"location":"api/utils/#openboost.crps_empirical","title":"crps_empirical","text":"<pre><code>crps_empirical(y_true, samples, *, sample_weight=None)\n</code></pre> <p>Compute CRPS using empirical distribution from Monte Carlo samples.</p> <p>For non-Gaussian distributions, CRPS can be estimated from samples. This is useful for NaturalBoost models with non-Normal distributions.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>samples</code> <p>Monte Carlo samples, shape (n_samples, n_mc_samples). Each row contains samples from the predictive distribution.</p> <p> TYPE: <code>NDArray</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Mean CRPS estimated from samples (lower is better).</p> Example <p>model = ob.NaturalBoostGamma(n_trees=100) model.fit(X_train, y_train) samples = model.sample(X_test, n_samples=1000)  # (n_test, 1000) crps = ob.crps_empirical(y_test, samples)</p> Notes <p>Uses the formula: CRPS = E|X - y| - 0.5 * E|X - X'| where X, X' are independent samples from the predictive distribution.</p>"},{"location":"api/utils/#openboost.brier_score","title":"brier_score","text":"<pre><code>brier_score(y_true, y_prob, *, sample_weight=None)\n</code></pre> <p>Compute Brier score for probabilistic binary classification.</p> <p>Brier score measures the mean squared error of probability predictions. It's a strictly proper scoring rule for binary outcomes.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True binary labels, shape (n_samples,). Values should be 0 or 1.</p> <p> TYPE: <code>NDArray</code> </p> <code>y_prob</code> <p>Predicted probabilities for positive class, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Brier score (lower is better). Perfect predictions = 0, random = 0.25.</p> Example <p>import openboost as ob y_true = np.array([0, 0, 1, 1]) y_prob = np.array([0.1, 0.2, 0.8, 0.9]) ob.brier_score(y_true, y_prob) 0.025</p> Notes <p>Brier score = mean((y_prob - y_true)\u00b2)</p> <p>Decomposition: Brier = Reliability - Resolution + Uncertainty - Reliability: calibration error (how well probabilities match frequencies) - Resolution: how different predictions are from the base rate - Uncertainty: entropy of the outcome distribution</p>"},{"location":"api/utils/#openboost.pinball_loss","title":"pinball_loss","text":"<pre><code>pinball_loss(\n    y_true, y_pred, quantile=0.5, *, sample_weight=None\n)\n</code></pre> <p>Compute pinball loss (quantile loss) for quantile regression.</p> <p>Pinball loss is the proper scoring rule for quantile estimation. At quantile=0.5, it equals MAE.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>y_pred</code> <p>Predicted quantile values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>quantile</code> <p>The quantile being predicted, in (0, 1). Default 0.5 (median).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Pinball loss (lower is better).</p> Example <p>import openboost as ob y_true = np.array([1.0, 2.0, 3.0]) y_pred_median = np.array([1.1, 2.0, 2.8]) ob.pinball_loss(y_true, y_pred_median, quantile=0.5) 0.1</p> Notes <p>Pinball loss: L(y, q) = (y - q) * \u03c4 if y &gt;= q else (q - y) * (1 - \u03c4) where \u03c4 is the quantile.</p> <p>For prediction intervals from NaturalBoost:</p> <p>lower, upper = model.predict_interval(X, alpha=0.1)  # 90% interval loss_lower = ob.pinball_loss(y, lower, quantile=0.05) loss_upper = ob.pinball_loss(y, upper, quantile=0.95)</p>"},{"location":"api/utils/#openboost.pinball_loss--lower-quantile-eg-10th-percentile","title":"Lower quantile (e.g., 10th percentile)","text":"<p>y_pred_q10 = np.array([0.5, 1.5, 2.0]) ob.pinball_loss(y_true, y_pred_q10, quantile=0.1)</p>"},{"location":"api/utils/#openboost.interval_score","title":"interval_score","text":"<pre><code>interval_score(\n    y_true, lower, upper, alpha=0.1, *, sample_weight=None\n)\n</code></pre> <p>Compute interval score for prediction intervals.</p> <p>Interval score is a strictly proper scoring rule for prediction intervals. It rewards narrow intervals while penalizing miscoverage.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>lower</code> <p>Lower bound of prediction interval, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>upper</code> <p>Upper bound of prediction interval, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>alpha</code> <p>Nominal miscoverage rate (0.1 for 90% interval). Default 0.1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Interval score (lower is better).</p> Example <p>import openboost as ob y_true = np.array([1.0, 2.0, 3.0]) lower = np.array([0.5, 1.5, 2.5]) upper = np.array([1.5, 2.5, 3.5]) ob.interval_score(y_true, lower, upper, alpha=0.1) 1.0</p> Notes <p>Interval Score = (upper - lower) + (2/\u03b1) * (lower - y) * I(y &lt; lower)                                  + (2/\u03b1) * (y - upper) * I(y &gt; upper)</p> <p>The score combines: 1. Interval width (prefer narrow intervals) 2. Penalty for observations below lower bound 3. Penalty for observations above upper bound</p> <p>Use with NaturalBoost:</p> <p>lower, upper = model.predict_interval(X_test, alpha=0.1) score = ob.interval_score(y_test, lower, upper, alpha=0.1)</p>"},{"location":"api/utils/#openboost.expected_calibration_error","title":"expected_calibration_error","text":"<pre><code>expected_calibration_error(\n    y_true, y_prob, n_bins=10, *, strategy=\"uniform\"\n)\n</code></pre> <p>Compute Expected Calibration Error (ECE) for probability predictions.</p> <p>ECE measures the miscalibration of predicted probabilities. A well-calibrated model has ECE close to 0.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True binary labels, shape (n_samples,). Values should be 0 or 1.</p> <p> TYPE: <code>NDArray</code> </p> <code>y_prob</code> <p>Predicted probabilities for positive class, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>n_bins</code> <p>Number of bins to use. Default 10.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>strategy</code> <p>Binning strategy: - 'uniform': Bins of equal width in [0, 1]. - 'quantile': Bins with equal number of samples.</p> <p> TYPE: <code>Literal['uniform', 'quantile']</code> DEFAULT: <code>'uniform'</code> </p> RETURNS DESCRIPTION <code>float</code> <p>ECE (lower is better). Perfect calibration = 0.</p> Example <p>import openboost as ob y_true = np.array([0, 0, 1, 1, 1]) y_prob = np.array([0.1, 0.3, 0.6, 0.8, 0.9]) ob.expected_calibration_error(y_true, y_prob) 0.06</p> Notes <p>ECE = \u03a3 (|bin_size| / n) * |accuracy_in_bin - mean_confidence_in_bin|</p> <p>For reliability diagrams, use calibration_curve to get bin data.</p>"},{"location":"api/utils/#openboost.calibration_curve","title":"calibration_curve","text":"<pre><code>calibration_curve(\n    y_true, y_prob, n_bins=10, *, strategy=\"uniform\"\n)\n</code></pre> <p>Compute calibration curve data for reliability diagrams.</p> <p>Returns the fraction of positives and mean predicted probability for each bin. This data can be used to create reliability diagrams.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True binary labels, shape (n_samples,). Values should be 0 or 1.</p> <p> TYPE: <code>NDArray</code> </p> <code>y_prob</code> <p>Predicted probabilities for positive class, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>n_bins</code> <p>Number of bins to use. Default 10.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>strategy</code> <p>Binning strategy: - 'uniform': Bins of equal width in [0, 1]. - 'quantile': Bins with equal number of samples.</p> <p> TYPE: <code>Literal['uniform', 'quantile']</code> DEFAULT: <code>'uniform'</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>Tuple of (fraction_of_positives, mean_predicted_value, bin_counts):</p> <code>NDArray</code> <ul> <li>fraction_of_positives: Actual fraction of positives in each bin.</li> </ul> <code>NDArray</code> <ul> <li>mean_predicted_value: Mean predicted probability in each bin.</li> </ul> <code>tuple[NDArray, NDArray, NDArray]</code> <ul> <li>bin_counts: Number of samples in each bin.</li> </ul> Example <p>import openboost as ob import matplotlib.pyplot as plt</p> <p>y_true = np.array([0, 0, 1, 1, 1, 0, 1, 0, 1, 1]) y_prob = np.array([0.1, 0.2, 0.3, 0.5, 0.6, 0.4, 0.7, 0.3, 0.8, 0.9]) frac_pos, mean_pred, counts = ob.calibration_curve(y_true, y_prob, n_bins=5)</p>"},{"location":"api/utils/#openboost.calibration_curve--plot-reliability-diagram","title":"Plot reliability diagram","text":"<p>plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated') plt.plot(mean_pred, frac_pos, 's-', label='Model') plt.xlabel('Mean predicted probability') plt.ylabel('Fraction of positives') plt.legend()</p>"},{"location":"api/utils/#openboost.negative_log_likelihood","title":"negative_log_likelihood","text":"<pre><code>negative_log_likelihood(\n    y_true, mean, std, *, sample_weight=None\n)\n</code></pre> <p>Compute negative log-likelihood for Gaussian predictions.</p> <p>NLL is a proper scoring rule for probabilistic predictions. Lower is better.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True values, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>mean</code> <p>Predicted mean, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>std</code> <p>Predicted standard deviation, shape (n_samples,). Must be &gt; 0.</p> <p> TYPE: <code>NDArray</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,). If None, uniform weights.</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Mean negative log-likelihood (lower is better).</p> Example <p>import openboost as ob y_true = np.array([1.0, 2.0, 3.0]) mean = np.array([1.1, 2.0, 2.8]) std = np.array([0.5, 0.5, 0.5]) ob.negative_log_likelihood(y_true, mean, std) 0.92...</p> Notes <p>NLL = 0.5 * log(2\u03c0) + log(\u03c3) + (y - \u03bc)\u00b2 / (2\u03c3\u00b2)</p> <p>For NaturalBoost Normal models:</p> <p>output = model.predict_distribution(X) mean, var = output.params[:, 0], output.params[:, 1] nll = ob.negative_log_likelihood(y, mean, np.sqrt(var))</p>"},{"location":"api/utils/#sampling","title":"Sampling","text":""},{"location":"api/utils/#goss_sample","title":"goss_sample","text":""},{"location":"api/utils/#openboost.goss_sample","title":"goss_sample","text":"<pre><code>goss_sample(\n    grad, hess=None, top_rate=0.2, other_rate=0.1, seed=None\n)\n</code></pre> <p>Gradient-based One-Side Sampling (GOSS).</p> <p>GOSS keeps all samples with large gradient magnitudes (important for learning) and randomly samples from the rest. Small-gradient samples are upweighted to maintain unbiased gradient estimates.</p> <p>This gives ~3x speedup with minimal accuracy loss compared to random subsampling.</p> Algorithm <ol> <li>Sort samples by |gradient|</li> <li>Keep top <code>top_rate</code> samples by gradient magnitude</li> <li>Randomly sample <code>other_rate</code> from the rest</li> <li>Upweight small-gradient samples by (1 - top_rate) / other_rate</li> </ol> PARAMETER DESCRIPTION <code>grad</code> <p>Gradient array, shape (n_samples,) or (n_samples, n_params).   For multi-parameter distributions, uses sum of absolute gradients.</p> <p> TYPE: <code>NDArray</code> </p> <code>hess</code> <p>Hessian array (unused, for API compatibility).</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> <code>top_rate</code> <p>Fraction of high-gradient samples to keep (default 0.2).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>other_rate</code> <p>Fraction of low-gradient samples to sample (default 0.1).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>seed</code> <p>Random seed for reproducibility.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SamplingResult</code> <p>SamplingResult with selected indices and weights.</p> Example <p>grad = compute_gradients(pred, y) result = goss_sample(grad, top_rate=0.2, other_rate=0.1)</p> References <ul> <li>LightGBM paper: https://papers.nips.cc/paper/6907-lightgbm</li> </ul>"},{"location":"api/utils/#openboost.goss_sample--use-selected-samples-for-histogram-building","title":"Use selected samples for histogram building","text":"<p>hist = build_histogram(X[result.indices],  ...                        grad[result.indices] * result.weights, ...                        hess[result.indices] * result.weights)</p>"},{"location":"api/utils/#minibatchiterator","title":"MiniBatchIterator","text":""},{"location":"api/utils/#openboost.MiniBatchIterator","title":"MiniBatchIterator","text":"<pre><code>MiniBatchIterator(\n    n_samples, batch_size, shuffle=False, seed=None\n)\n</code></pre> <p>Iterator for mini-batch training.</p> <p>Yields chunks of sample indices for processing datasets larger than memory.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>Total number of samples.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>Number of samples per batch.</p> <p> TYPE: <code>int</code> </p> <code>shuffle</code> <p>Whether to shuffle indices before iteration.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>Random seed for shuffling.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Example <p>iterator = MiniBatchIterator(n_samples=10_000_000, batch_size=100_000) for batch_indices in iterator: ...     # Load batch data ...     X_batch = load_batch(X_mmap, batch_indices) ...     grad_batch = grad[batch_indices] ...     hess_batch = hess[batch_indices] ...    ...     # Build and accumulate histogram ...     batch_hist = build_histogram(X_batch, grad_batch, hess_batch) ...     total_hist += batch_hist</p>"},{"location":"api/utils/#openboost.MiniBatchIterator.n_batches","title":"n_batches  <code>property</code>","text":"<pre><code>n_batches\n</code></pre> <p>Number of batches per epoch.</p>"},{"location":"api/utils/#openboost.MiniBatchIterator.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Iterate over batch indices.</p>"},{"location":"api/utils/#openboost.MiniBatchIterator.__next__","title":"__next__","text":"<pre><code>__next__()\n</code></pre> <p>Get next batch of indices.</p>"},{"location":"api/utils/#openboost.MiniBatchIterator.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Number of batches.</p>"},{"location":"api/utils/#create_memmap_binned","title":"create_memmap_binned","text":""},{"location":"api/utils/#openboost.create_memmap_binned","title":"create_memmap_binned","text":"<pre><code>create_memmap_binned(path, X, n_bins=256)\n</code></pre> <p>Create memory-mapped binned array for large datasets.</p> <p>Bins the data and saves to disk as a memory-mapped file, enabling training on datasets larger than RAM.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to save the memory-mapped file.</p> <p> TYPE: <code>str</code> </p> <code>X</code> <p>Input features, shape (n_samples, n_features).</p> <p> TYPE: <code>NDArray</code> </p> <code>n_bins</code> <p>Number of bins for quantile binning.</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> RETURNS DESCRIPTION <code>memmap</code> <p>Memory-mapped binned array, shape (n_features, n_samples).</p> Example"},{"location":"api/utils/#openboost.create_memmap_binned--create-once","title":"Create once","text":"<p>X_mmap = create_memmap_binned('data.npy', X_train)</p>"},{"location":"api/utils/#openboost.create_memmap_binned--load-for-training-no-copy-uses-disk","title":"Load for training (no copy, uses disk)","text":"<p>X_mmap = np.memmap('data.npy', mode='r', dtype=np.uint8, ...                     shape=(n_features, n_samples))</p>"},{"location":"api/utils/#load_memmap_binned","title":"load_memmap_binned","text":""},{"location":"api/utils/#openboost.load_memmap_binned","title":"load_memmap_binned","text":"<pre><code>load_memmap_binned(path, n_features, n_samples)\n</code></pre> <p>Load memory-mapped binned array.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to the memory-mapped file.</p> <p> TYPE: <code>str</code> </p> <code>n_features</code> <p>Number of features.</p> <p> TYPE: <code>int</code> </p> <code>n_samples</code> <p>Number of samples.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>memmap</code> <p>Memory-mapped binned array, shape (n_features, n_samples).</p>"},{"location":"getting-started/gpu-setup/","title":"GPU Setup","text":"<p>OpenBoost automatically detects and uses CUDA GPUs when available.</p>"},{"location":"getting-started/gpu-setup/#verify-gpu-detection","title":"Verify GPU Detection","text":"<pre><code>import openboost as ob\n\nprint(f\"Backend: {ob.get_backend()}\")  # \"cuda\" or \"cpu\"\nprint(f\"Using GPU: {ob.is_cuda()}\")    # True if GPU active\n</code></pre>"},{"location":"getting-started/gpu-setup/#manual-backend-selection","title":"Manual Backend Selection","text":"<pre><code>import openboost as ob\n\n# Force CPU (useful for debugging or comparison)\nob.set_backend(\"cpu\")\n\n# Force GPU\nob.set_backend(\"cuda\")\n\n# Or use environment variable\n# export OPENBOOST_BACKEND=cuda\n</code></pre>"},{"location":"getting-started/gpu-setup/#gpu-performance","title":"GPU Performance","text":"<p>GPU acceleration provides significant speedups for larger datasets:</p> Dataset Size Typical Speedup &lt;5K samples ~1x (CPU overhead dominates) 5K-10K 2-7x 25K+ 2-3x 100K+ 5-10x <p>Best practices for GPU</p> <ul> <li>Ensure data is <code>float32</code> (not <code>float64</code>)</li> <li>Use larger datasets (GPU overhead not worth it for &lt;5K samples)</li> <li>GPU shows best speedup at 10K+ samples</li> </ul>"},{"location":"getting-started/gpu-setup/#multi-gpu-training","title":"Multi-GPU Training","text":"<pre><code>import openboost as ob\n\n# Use multiple GPUs with Ray (requires ray[default])\nmodel = ob.GradientBoosting(n_trees=100, n_gpus=4)\nmodel.fit(X, y)\n\n# Or specify exact GPU devices\nmodel = ob.GradientBoosting(n_trees=100, devices=[0, 2])\nmodel.fit(X, y)\n</code></pre>"},{"location":"getting-started/gpu-setup/#requirements","title":"Requirements","text":"<ul> <li>NVIDIA GPU with CUDA Compute Capability 3.5+</li> <li>CUDA Toolkit 11.0+ or 12.0+</li> <li>Numba 0.60+ (<code>pip install numba</code>)</li> </ul>"},{"location":"getting-started/gpu-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/gpu-setup/#training-seems-slow-on-gpu","title":"Training seems slow on GPU","text":"<ul> <li>Ensure data is <code>float32</code> (not <code>float64</code>)</li> <li>Use larger datasets (GPU overhead not worth it for &lt;5K samples)</li> <li>GPU shows best speedup at 10K+ samples</li> </ul>"},{"location":"getting-started/gpu-setup/#model-trained-on-gpu-loading-on-cpu-machine","title":"Model trained on GPU, loading on CPU machine","text":"<pre><code># Models are saved in a backend-agnostic format\nmodel.save(\"model.joblib\")\n\n# Load on any machine (CPU or GPU)\nloaded = ob.GradientBoosting.load(\"model.joblib\")\n</code></pre>"},{"location":"getting-started/gpu-setup/#cuda-not-detected","title":"CUDA not detected","text":"<ol> <li>Check CUDA installation: <code>nvcc --version</code></li> <li>Check Numba can see GPU: <code>python -c \"from numba import cuda; print(cuda.gpus)\"</code></li> <li>Ensure compatible CUDA version with Numba</li> </ol>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#quick-install","title":"Quick Install","text":"pipuvconda <pre><code>pip install openboost\n</code></pre> <pre><code>uv add openboost\n</code></pre> <pre><code># Coming soon\nconda install -c conda-forge openboost\n</code></pre>"},{"location":"getting-started/installation/#with-gpu-support","title":"With GPU Support","text":"<p>For CUDA GPU acceleration:</p> pipuv <pre><code>pip install \"openboost[cuda]\"\n</code></pre> <pre><code>uv add \"openboost[cuda]\"\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"Extra What it includes Install <code>cuda</code> CuPy for GPU acceleration <code>pip install \"openboost[cuda]\"</code> <code>sklearn</code> scikit-learn integration <code>pip install \"openboost[sklearn]\"</code> <code>distributed</code> Ray for multi-GPU training <code>pip install \"openboost[distributed]\"</code> <code>all</code> Everything <code>pip install \"openboost[all]\"</code>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>NumPy 1.24+</li> <li>Numba 0.60+</li> </ul>"},{"location":"getting-started/installation/#for-gpu-support","title":"For GPU Support","text":"<ul> <li>NVIDIA GPU with CUDA Compute Capability 3.5+</li> <li>CUDA Toolkit 11.0+ or 12.0+</li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import openboost as ob\n\nprint(f\"OpenBoost version: {ob.__version__}\")\nprint(f\"Backend: {ob.get_backend()}\")  # \"cuda\" or \"cpu\"\nprint(f\"GPU available: {ob.is_cuda()}\")\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<pre><code>git clone https://github.com/jxucoder/openboost.git\ncd openboost\nuv sync --extra dev\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Get up and running with OpenBoost in 5 minutes.</p>"},{"location":"getting-started/quickstart/#basic-regression","title":"Basic Regression","text":"<pre><code>import numpy as np\nimport openboost as ob\n\n# Generate sample data\nnp.random.seed(42)\nX = np.random.randn(1000, 10).astype(np.float32)\ny = (X[:, 0] * 2 + X[:, 1] + np.random.randn(1000) * 0.1).astype(np.float32)\n\n# Split data\nX_train, X_test = X[:800], X[800:]\ny_train, y_test = y[:800], y[800:]\n\n# Train model\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    max_depth=6,\n    learning_rate=0.1,\n    loss='mse',\n)\nmodel.fit(X_train, y_train)\n\n# Predict\npredictions = model.predict(X_test)\n\n# Evaluate\nrmse = np.sqrt(np.mean((predictions - y_test) ** 2))\nprint(f\"RMSE: {rmse:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#binary-classification","title":"Binary Classification","text":"<pre><code>import openboost as ob\n\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    max_depth=6,\n    loss='logloss',\n)\nmodel.fit(X_train, y_train)  # y_train: 0 or 1\n\n# Get probabilities\nlogits = model.predict(X_test)\nprobabilities = 1 / (1 + np.exp(-logits))\n\n# Get class predictions\npredictions = (probabilities &gt; 0.5).astype(int)\n</code></pre>"},{"location":"getting-started/quickstart/#multi-class-classification","title":"Multi-Class Classification","text":"<pre><code>import openboost as ob\n\nmodel = ob.MultiClassGradientBoosting(\n    n_classes=5,\n    n_trees=100,\n    max_depth=6,\n)\nmodel.fit(X_train, y_train)  # y_train: 0, 1, 2, 3, or 4\n\n# Get probabilities\nprobabilities = model.predict_proba(X_test)  # Shape: (n_samples, n_classes)\n\n# Get class predictions\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"getting-started/quickstart/#uncertainty-quantification","title":"Uncertainty Quantification","text":"<pre><code>import openboost as ob\n\n# Train probabilistic model\nmodel = ob.NaturalBoostNormal(n_trees=100, max_depth=4)\nmodel.fit(X_train, y_train)\n\n# Point prediction (mean)\nmean = model.predict(X_test)\n\n# 90% prediction interval\nlower, upper = model.predict_interval(X_test, alpha=0.1)\n\n# Sample from predicted distribution\nsamples = model.sample(X_test, n_samples=100)\n</code></pre>"},{"location":"getting-started/quickstart/#sklearn-compatible-api","title":"sklearn-Compatible API","text":"<pre><code>from openboost import OpenBoostRegressor, OpenBoostClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\n# Regressor\nreg = OpenBoostRegressor(n_estimators=100, max_depth=6)\nreg.fit(X_train, y_train)\nprint(f\"R\u00b2 Score: {reg.score(X_test, y_test):.4f}\")\n\n# Cross-validation\nscores = cross_val_score(reg, X, y, cv=5)\nprint(f\"CV Score: {scores.mean():.4f} \u00b1 {scores.std():.4f}\")\n\n# Grid search\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [4, 6, 8],\n    'learning_rate': [0.05, 0.1, 0.2],\n}\ngrid = GridSearchCV(reg, param_grid, cv=3)\ngrid.fit(X_train, y_train)\nprint(f\"Best params: {grid.best_params_}\")\n</code></pre>"},{"location":"getting-started/quickstart/#callbacks","title":"Callbacks","text":"<pre><code>import openboost as ob\nfrom openboost import EarlyStopping, Logger\n\nmodel = ob.GradientBoosting(n_trees=500, max_depth=6)\n\ncallbacks = [\n    EarlyStopping(patience=10, min_delta=0.001),\n    Logger(every=10),\n]\n\nmodel.fit(\n    X_train, y_train,\n    callbacks=callbacks,\n    eval_set=[(X_test, y_test)],\n)\n\nprint(f\"Stopped at {len(model.trees_)} trees\")\n</code></pre>"},{"location":"getting-started/quickstart/#saving-and-loading","title":"Saving and Loading","text":"<pre><code>import openboost as ob\n\n# Train\nmodel = ob.GradientBoosting(n_trees=100)\nmodel.fit(X_train, y_train)\n\n# Save\nmodel.save('my_model.joblib')\n\n# Load\nloaded_model = ob.GradientBoosting.load('my_model.joblib')\npredictions = loaded_model.predict(X_test)\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>GPU Setup - Configure GPU acceleration</li> <li>Uncertainty Quantification - Deep dive into NaturalBoost</li> <li>Custom Loss Functions - Define your own objectives</li> </ul>"},{"location":"migration/from-xgboost/","title":"Migrating from XGBoost to OpenBoost","text":"<p>This guide helps you transition from XGBoost to OpenBoost with minimal changes.</p>"},{"location":"migration/from-xgboost/#parameter-mapping","title":"Parameter Mapping","text":""},{"location":"migration/from-xgboost/#xgboost-openboost","title":"XGBoost \u2192 OpenBoost","text":"XGBoost Parameter OpenBoost Parameter Notes <code>n_estimators</code> <code>n_trees</code> / <code>n_estimators</code> Same meaning <code>max_depth</code> <code>max_depth</code> Same meaning <code>learning_rate</code> / <code>eta</code> <code>learning_rate</code> Same meaning <code>min_child_weight</code> <code>min_child_weight</code> Same meaning <code>reg_lambda</code> / <code>lambda</code> <code>reg_lambda</code> L2 regularization <code>reg_alpha</code> / <code>alpha</code> <code>reg_alpha</code> L1 regularization <code>subsample</code> <code>subsample</code> Row sampling <code>colsample_bytree</code> <code>colsample_bytree</code> Column sampling <code>gamma</code> / <code>min_split_loss</code> <code>gamma</code> Min gain to split <code>objective</code> <code>loss</code> See loss mapping below"},{"location":"migration/from-xgboost/#loss-function-mapping","title":"Loss Function Mapping","text":"XGBoost Objective OpenBoost Loss <code>reg:squarederror</code> <code>'mse'</code> <code>reg:absoluteerror</code> <code>'mae'</code> <code>reg:pseudohubererror</code> <code>'huber'</code> <code>binary:logistic</code> <code>'logloss'</code> <code>multi:softmax</code> Use <code>MultiClassGradientBoosting</code> <code>multi:softprob</code> Use <code>MultiClassGradientBoosting</code> <code>count:poisson</code> <code>'poisson'</code> <code>reg:gamma</code> <code>'gamma'</code> <code>reg:tweedie</code> <code>'tweedie'</code>"},{"location":"migration/from-xgboost/#code-examples","title":"Code Examples","text":""},{"location":"migration/from-xgboost/#basic-regression","title":"Basic Regression","text":"<pre><code># XGBoost\nimport xgboost as xgb\nmodel = xgb.XGBRegressor(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    reg_lambda=1.0,\n)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\n# OpenBoost equivalent\nimport openboost as ob\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    max_depth=6,\n    learning_rate=0.1,\n    reg_lambda=1.0,\n    loss='mse',\n)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n</code></pre>"},{"location":"migration/from-xgboost/#binary-classification","title":"Binary Classification","text":"<pre><code># XGBoost\nmodel = xgb.XGBClassifier(\n    n_estimators=100,\n    max_depth=6,\n    objective='binary:logistic',\n)\nmodel.fit(X_train, y_train)\npred_proba = model.predict_proba(X_test)[:, 1]\n\n# OpenBoost equivalent\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    max_depth=6,\n    loss='logloss',\n)\nmodel.fit(X_train, y_train)\nlogits = model.predict(X_test)\npred_proba = 1 / (1 + np.exp(-logits))  # Sigmoid\n\n# Or use sklearn wrapper\nfrom openboost import OpenBoostClassifier\nmodel = OpenBoostClassifier(n_estimators=100, max_depth=6)\nmodel.fit(X_train, y_train)\npred_proba = model.predict_proba(X_test)[:, 1]\n</code></pre>"},{"location":"migration/from-xgboost/#multi-class-classification","title":"Multi-Class Classification","text":"<pre><code># XGBoost\nmodel = xgb.XGBClassifier(\n    n_estimators=100,\n    max_depth=6,\n    objective='multi:softprob',\n    num_class=5,\n)\nmodel.fit(X_train, y_train)\npred_proba = model.predict_proba(X_test)\npred = model.predict(X_test)\n\n# OpenBoost equivalent\nmodel = ob.MultiClassGradientBoosting(\n    n_classes=5,\n    n_trees=100,\n    max_depth=6,\n)\nmodel.fit(X_train, y_train)\npred_proba = model.predict_proba(X_test)\npred = model.predict(X_test)\n\n# Or use sklearn wrapper\nfrom openboost import OpenBoostClassifier\nmodel = OpenBoostClassifier(n_estimators=100, max_depth=6)\nmodel.fit(X_train, y_train)  # Auto-detects multi-class\n</code></pre>"},{"location":"migration/from-xgboost/#sklearn-compatible-api","title":"sklearn-Compatible API","text":"<p>OpenBoost provides drop-in replacements for XGBoost's sklearn API:</p> <pre><code># XGBoost sklearn\nfrom xgboost import XGBRegressor, XGBClassifier\n\n# OpenBoost sklearn (same interface!)\nfrom openboost import OpenBoostRegressor, OpenBoostClassifier\n\n# Works with sklearn pipelines\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', OpenBoostRegressor(n_estimators=100)),\n])\n\n# Works with cross-validation\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(OpenBoostRegressor(), X, y, cv=5)\n\n# Works with grid search\nfrom sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(\n    OpenBoostRegressor(),\n    {'n_estimators': [50, 100], 'max_depth': [4, 6]},\n    cv=3,\n)\ngrid.fit(X, y)\n</code></pre>"},{"location":"migration/from-xgboost/#early-stopping","title":"Early Stopping","text":"<pre><code># XGBoost\nmodel = xgb.XGBRegressor(\n    n_estimators=1000,\n    early_stopping_rounds=10,\n)\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    verbose=False,\n)\n\n# OpenBoost equivalent\nfrom openboost import EarlyStopping, Logger\n\nmodel = ob.GradientBoosting(n_trees=1000)\nmodel.fit(\n    X_train, y_train,\n    callbacks=[\n        EarlyStopping(patience=10),\n        Logger(every=10),\n    ],\n    eval_set=[(X_val, y_val)],\n)\n</code></pre>"},{"location":"migration/from-xgboost/#feature-importance","title":"Feature Importance","text":"<pre><code># XGBoost\nmodel.fit(X_train, y_train)\nimportance = model.feature_importances_\n\n# OpenBoost\nmodel.fit(X_train, y_train)\nimportance = ob.compute_feature_importances(model.trees_)\n\n# Or with sklearn wrapper\nfrom openboost import OpenBoostRegressor\nmodel = OpenBoostRegressor()\nmodel.fit(X_train, y_train)\nimportance = model.feature_importances_  # Same as XGBoost!\n</code></pre>"},{"location":"migration/from-xgboost/#saving-and-loading","title":"Saving and Loading","text":"<pre><code># XGBoost\nmodel.save_model('model.json')\nloaded = xgb.XGBRegressor()\nloaded.load_model('model.json')\n\n# OpenBoost\nmodel.save('model.joblib')\nloaded = ob.GradientBoosting.load('model.joblib')\n\n# Or with joblib directly (same as XGBoost pickle)\nimport joblib\njoblib.dump(model, 'model.joblib')\nloaded = joblib.load('model.joblib')\n</code></pre>"},{"location":"migration/from-xgboost/#feature-comparison","title":"Feature Comparison","text":"Feature XGBoost OpenBoost GPU Support \u2705 \u2705 Custom Loss \u26a0\ufe0f (requires Python wrapper) \u2705 (native Python) Uncertainty \u274c \u2705 (NaturalBoost) Interpretable GAM \u274c \u2705 (OpenBoostGAM) Linear Leaves \u274c \u2705 (LinearLeafGBDT) DART \u2705 \u2705 Growth Strategies Level-wise Level-wise, Leaf-wise, Symmetric GOSS Sampling \u274c (use LightGBM) \u2705 Pure Python \u274c (C++) \u2705"},{"location":"migration/from-xgboost/#what-openboost-does-better","title":"What OpenBoost Does Better","text":""},{"location":"migration/from-xgboost/#1-uncertainty-quantification","title":"1. Uncertainty Quantification","text":"<pre><code># XGBoost: Just point predictions\npred = xgb_model.predict(X_test)  # Single number\n\n# OpenBoost: Full distributions\nmodel = ob.NaturalBoostNormal(n_trees=100)\nmodel.fit(X_train, y_train)\nmean = model.predict(X_test)\nlower, upper = model.predict_interval(X_test)  # 90% interval\nsamples = model.sample(X_test, n_samples=1000)  # Monte Carlo\n</code></pre>"},{"location":"migration/from-xgboost/#2-custom-loss-functions","title":"2. Custom Loss Functions","text":"<pre><code># XGBoost: Requires Python callback wrapper, tricky to get right\n# OpenBoost: Native Python, just return (grad, hess)\ndef my_loss(pred, y):\n    grad = pred - y\n    hess = np.ones_like(pred)\n    return grad.astype(np.float32), hess.astype(np.float32)\n\nmodel = ob.GradientBoosting(loss=my_loss)\n</code></pre>"},{"location":"migration/from-xgboost/#3-interpretable-models","title":"3. Interpretable Models","text":"<pre><code># XGBoost: SHAP values (post-hoc, expensive)\n# OpenBoost: Inherently interpretable GAM\ngam = ob.OpenBoostGAM(n_rounds=500)\ngam.fit(X_train, y_train)\ngam.plot_shape_function(0, feature_name=\"age\")\n</code></pre>"},{"location":"migration/from-xgboost/#4-code-readability","title":"4. Code Readability","text":"<pre><code># XGBoost: 200K+ lines of C++\n# OpenBoost: ~6K lines of Python you can actually read and modify\n</code></pre>"},{"location":"migration/from-xgboost/#what-xgboost-does-better","title":"What XGBoost Does Better","text":""},{"location":"migration/from-xgboost/#1-raw-speed-on-cpu","title":"1. Raw Speed on CPU","text":"<p>XGBoost is highly optimized C++ and will be faster on CPU for very large datasets.</p>"},{"location":"migration/from-xgboost/#2-distributed-training-spark-dask","title":"2. Distributed Training (Spark, Dask)","text":"<p>XGBoost has mature distributed training support.</p>"},{"location":"migration/from-xgboost/#3-community-and-ecosystem","title":"3. Community and Ecosystem","text":"<p>XGBoost has more examples, tutorials, and community support.</p>"},{"location":"migration/from-xgboost/#migration-checklist","title":"Migration Checklist","text":"<ul> <li>[ ] Replace <code>xgb.XGBRegressor</code> with <code>ob.GradientBoosting</code> or <code>OpenBoostRegressor</code></li> <li>[ ] Replace <code>xgb.XGBClassifier</code> with <code>ob.GradientBoosting(loss='logloss')</code> or <code>OpenBoostClassifier</code></li> <li>[ ] Replace <code>n_estimators</code> with <code>n_trees</code> (or use sklearn wrapper)</li> <li>[ ] Replace <code>objective</code> with <code>loss</code></li> <li>[ ] Update early stopping syntax</li> <li>[ ] Update feature importance code</li> <li>[ ] Update save/load code</li> </ul>"},{"location":"migration/from-xgboost/#gradual-migration","title":"Gradual Migration","text":"<p>You can use both libraries during migration:</p> <pre><code>import xgboost as xgb\nimport openboost as ob\n\n# Keep XGBoost for existing models\nxgb_model = xgb.XGBRegressor()\nxgb_model.fit(X_train, y_train)\n\n# Try OpenBoost for new features\nob_model = ob.NaturalBoostNormal()  # Uncertainty!\nob_model.fit(X_train, y_train)\n\n# Compare predictions\nxgb_pred = xgb_model.predict(X_test)\nob_pred = ob_model.predict(X_test)\nprint(f\"Correlation: {np.corrcoef(xgb_pred, ob_pred)[0,1]:.4f}\")\n</code></pre>"},{"location":"migration/from-xgboost/#getting-help","title":"Getting Help","text":"<ul> <li>Quickstart Guide - Get started with OpenBoost</li> <li>Uncertainty Tutorial - Learn NaturalBoost</li> <li>Custom Loss Tutorial - Define your own objectives</li> </ul>"},{"location":"releases/ANNOUNCEMENTS/","title":"OpenBoost 1.0.0rc1 Announcements","text":"<p>Copy-paste ready announcements for different platforms.</p>"},{"location":"releases/ANNOUNCEMENTS/#twitterx-thread","title":"Twitter/X (Thread)","text":""},{"location":"releases/ANNOUNCEMENTS/#tweet-1-main","title":"Tweet 1 (Main)","text":"<pre><code>Releasing OpenBoost 1.0.0rc1 \ud83d\ude80\n\nGPU-accelerated GBDT variants in pure Python.\n\nNGBoost, InterpretML EBM? Slow, CPU-only.\nOpenBoost brings them to GPU \u2192 2-40x faster.\n\nPlus: fully readable, customizable. Built for the AI coding era.\n\npip install openboost\n\n\ud83e\uddf5 Thread on why this matters...\n</code></pre>"},{"location":"releases/ANNOUNCEMENTS/#tweet-2","title":"Tweet 2","text":"<pre><code>The problem with XGBoost/LightGBM:\n\nWant to add a custom split criterion? \u2192 Write C++, recompile\nWant to modify histogram building? \u2192 Understand CUDA kernels\nWant to debug training? \u2192 Good luck with C++ stack traces\n\nWith OpenBoost: it's all Python. Modify, reload, done.\n</code></pre>"},{"location":"releases/ANNOUNCEMENTS/#tweet-3","title":"Tweet 3","text":"<pre><code>What's included:\n\n\u2705 NaturalBoost (NGBoost \u2192 GPU, 2x faster)\n\u2705 OpenBoostGAM (InterpretML EBM \u2192 GPU, 40x faster)\n\u2705 DART, LinearLeaf, multi-GPU\n\u2705 Standard GBDT (use XGBoost if you need speed)\n\u2705 Full sklearn compatibility\n</code></pre>"},{"location":"releases/ANNOUNCEMENTS/#tweet-4","title":"Tweet 4","text":"<pre><code>Why \"built for the AI coding era\"?\n\nWhen you ask Claude/ChatGPT to modify XGBoost's tree building...it can't help.\n\nAsk it to modify OpenBoost? It can read the code, suggest changes, and help you implement new algorithms.\n\nAI can't help with C++ monoliths. It can with Python.\n</code></pre>"},{"location":"releases/ANNOUNCEMENTS/#tweet-5","title":"Tweet 5","text":"<pre><code>Coming soon: native \"train-many\" optimization.\n\nIndustry reality:\n- Hyperparameter tuning = 100s of models\n- Cross-validation = k models  \n- Per-segment models = 1000s of models\n\nXGBoost optimizes for 1 model fast.\nOpenBoost will optimize for many models efficiently.\n</code></pre>"},{"location":"releases/ANNOUNCEMENTS/#tweet-6","title":"Tweet 6","text":"<pre><code>Try it:\n\npip install openboost\n\nimport openboost as ob\nmodel = ob.GradientBoosting(n_trees=100)\nmodel.fit(X, y)\n\nDocs: jxucoder.github.io/openboost\nGitHub: github.com/jxucoder/openboost\n\nFeedback welcome! This is rc1 - looking for battle testing before 1.0.\n</code></pre>"},{"location":"releases/ANNOUNCEMENTS/#linkedin","title":"LinkedIn","text":"<pre><code>\ud83d\ude80 Announcing OpenBoost 1.0.0rc1\n\nI've been working on something different: GPU-accelerated gradient boosting written entirely in Python.\n\n**The Problem**\nXGBoost and LightGBM are incredible libraries, but they're 200K+ lines of C++. Want to customize the split criterion? Add a new distribution? Modify how histograms are built? You need C++ expertise and hours of build time.\n\n**The Solution**\nOpenBoost GPU-accelerates GBDT variants that were previously CPU-only and slow:\n\n**What's Included**\n\u2022 NaturalBoost - probabilistic predictions (NGBoost \u2192 GPU, 1.3-2x faster)\n\u2022 OpenBoostGAM - interpretable models (InterpretML EBM \u2192 GPU, 10-40x faster)\n\u2022 Standard GBDT, DART, LinearLeaf (for custom algorithms; use XGBoost for production speed)\n\u2022 Full sklearn compatibility\n\n**Why Now?**\nIn the era of AI-assisted development, code readability matters more than ever. An AI assistant can help you modify Python code. It can't help with C++ monoliths.\n\n**Coming Next: Train-Many Optimization**\nIndustry workloads often require training many models (hyperparameter tuning, per-store models, cross-validation). XGBoost optimizes for one model fast. OpenBoost is building native support for training many models efficiently.\n\nTry it: pip install openboost\nDocs: https://jxucoder.github.io/openboost\n\nThis is rc1 - I'm looking for feedback and battle testing before the final 1.0 release. Would love to hear from anyone using gradient boosting in production.\n\n#MachineLearning #Python #DataScience #OpenSource\n</code></pre>"},{"location":"releases/ANNOUNCEMENTS/#reddit-rmachinelearning","title":"Reddit r/MachineLearning","text":""},{"location":"releases/ANNOUNCEMENTS/#title","title":"Title","text":"<pre><code>[P] OpenBoost: GPU-accelerated gradient boosting in pure Python (20K lines vs XGBoost's 200K C++)\n</code></pre>"},{"location":"releases/ANNOUNCEMENTS/#body","title":"Body","text":"<p><pre><code>I've released OpenBoost 1.0.0rc1, a gradient boosting library written entirely in Python with GPU acceleration via Numba.\n\n**Why another GBDT library?**\n\nXGBoost and LightGBM are engineering marvels, but they're essentially black boxes to most users. 200K+ lines of C++, complex build systems, CUDA kernels you can't modify without serious expertise.\n\nOpenBoost takes a different approach: ~20K lines of Python that GPU-accelerates GBDT variants that were previously slow and CPU-only.\n\n**What's included:**\n- NaturalBoost: NGBoost \u2192 GPU (1.3-2x faster)\n- OpenBoostGAM: InterpretML EBM \u2192 GPU (10-40x faster)\n- DART, linear-leaf models, multi-GPU via Ray\n- Standard GBDT (use XGBoost for production speed; OpenBoost for readability/customization)\n- Full sklearn compatibility\n\n**The \"agentic era\" angle:**\nWhen you're working with AI coding assistants (Cursor, Copilot, etc.), they can actually help you modify OpenBoost's algorithms. They can read the tree building code, understand it, and help you customize it. They can't do that with C++ monoliths.\n\n**Planned: train-many optimization**\nIndustry workloads often need to train many models (hyperparameter tuning, CV, per-store/per-product models). XGBoost optimizes for training one model fast. OpenBoost's Python architecture enables native optimization for training many models efficiently - batching, GPU parallelization across models.\n\n**Installation:**\n</code></pre> pip install openboost pip install openboost[cuda]  # GPU support <pre><code>**Links:**\n- Docs: https://jxucoder.github.io/openboost\n- GitHub: https://github.com/jxucoder/openboost\n- Examples: https://github.com/jxucoder/openboost/tree/main/examples\n\nThis is rc1 - looking for feedback before 1.0. Happy to answer questions about the implementation or benchmarks.\n</code></pre></p>"},{"location":"releases/ANNOUNCEMENTS/#hacker-news","title":"Hacker News","text":""},{"location":"releases/ANNOUNCEMENTS/#title_1","title":"Title","text":"<pre><code>Show HN: OpenBoost \u2013 GPU gradient boosting in pure Python (20K lines vs XGBoost's 200K C++)\n</code></pre>"},{"location":"releases/ANNOUNCEMENTS/#body_1","title":"Body","text":"<pre><code>I built OpenBoost because I wanted to understand and modify gradient boosting algorithms without diving into C++.\n\nXGBoost and LightGBM are incredible, but they're inaccessible to most practitioners. Want to add a custom loss function? Add a new distribution for probabilistic predictions? Modify the split criterion? You need C++ expertise.\n\nOpenBoost is ~20K lines of Python using Numba for GPU acceleration. It brings GPU to GBDT variants that were previously CPU-only and slow.\n\nKey insight: NGBoost and InterpretML EBM are great algorithms trapped in slow Python implementations. OpenBoost GPU-accelerates them:\n- NaturalBoost: 1.3-2x faster than NGBoost\n- OpenBoostGAM: 10-40x faster than InterpretML EBM\n\nFor standard GBDT, use XGBoost/LightGBM. For variants and custom algorithms, OpenBoost fills the gap.\n\nFeatures:\n- Probabilistic predictions with GPU acceleration\n- Interpretable GAMs with GPU acceleration\n- DART, linear-leaf models, multi-GPU via Ray\n- Full sklearn compatibility\n\nThe \"agentic era\" motivation: I use AI coding assistants daily. They can help me modify Python code. They can't help with 200K lines of C++. OpenBoost is designed to be readable and modifiable by both humans and AI.\n\n**Coming soon: train-many optimization.** Industry reality is often training hundreds or thousands of models (hyperparameter search, CV, per-segment models). XGBoost optimizes for training one model fast. OpenBoost's architecture enables batching and GPU parallelization across models - optimize for training many models at once.\n\nDocs: https://jxucoder.github.io/openboost\nGitHub: https://github.com/jxucoder/openboost\n\nThis is rc1. Feedback welcome.\n</code></pre>"},{"location":"releases/ANNOUNCEMENTS/#copy-these-files-after-release","title":"Copy these files after release","text":"<p>After you create the GitHub release, you can copy: 1. The LinkedIn post as-is 2. The Twitter thread (post as separate tweets) 3. Reddit post to r/MachineLearning (flair: [P] for Project) 4. HN post to Show HN</p> <p>Timing suggestion: Post to HN/Reddit during US morning hours (9-11am ET) for maximum visibility.</p>"},{"location":"releases/RELEASE_NOTES_1.0.0rc1/","title":"OpenBoost 1.0.0rc1 Release Notes","text":"<p>The GPU-native, all-Python platform for tree-based machine learning.</p>"},{"location":"releases/RELEASE_NOTES_1.0.0rc1/#why-openboost","title":"Why OpenBoost?","text":"<p>XGBoost and LightGBM are 200K+ lines of C++. OpenBoost is ~20K lines of Python.</p> <p>For standard GBDT, use XGBoost. For GBDT variants (probabilistic predictions, interpretable GAMs, custom algorithms), OpenBoost brings GPU acceleration to methods that were previously CPU-only and slow.</p> XGBoost / LightGBM OpenBoost Code 200K+ lines of C++ ~20K lines of Python GPU Added later Native from day one Customize Modify C++, recompile Modify Python, reload"},{"location":"releases/RELEASE_NOTES_1.0.0rc1/#whats-new-in-100rc1","title":"What's New in 1.0.0rc1","text":""},{"location":"releases/RELEASE_NOTES_1.0.0rc1/#core-models","title":"Core Models","text":"<ul> <li><code>GradientBoosting</code> - Standard gradient boosting for regression/classification</li> <li><code>MultiClassGradientBoosting</code> - Multi-class classification with softmax</li> <li><code>DART</code> - Dropout regularized trees</li> <li><code>OpenBoostGAM</code> - GPU-accelerated interpretable GAM (10-40x faster than InterpretML EBM)</li> </ul>"},{"location":"releases/RELEASE_NOTES_1.0.0rc1/#probabilistic-predictions-naturalboost","title":"Probabilistic Predictions (NaturalBoost)","text":"<p>Predict full probability distributions, not just point estimates: - Normal, LogNormal, Gamma, Poisson, StudentT - Tweedie - Insurance claims (Kaggle favorite) - NegativeBinomial - Sales forecasting (Kaggle favorite)</p> <pre><code>model = ob.NaturalBoostNormal(n_trees=100)\nmodel.fit(X_train, y_train)\nlower, upper = model.predict_interval(X_test, alpha=0.1)  # 90% prediction interval\n</code></pre>"},{"location":"releases/RELEASE_NOTES_1.0.0rc1/#advanced-features","title":"Advanced Features","text":"<ul> <li>Linear models in tree leaves (<code>LinearLeafGBDT</code>)</li> <li>GPU acceleration via Numba CUDA</li> <li>Multi-GPU support via Ray</li> <li>GOSS sampling (LightGBM-style)</li> <li>Full sklearn compatibility (<code>OpenBoostRegressor</code>, <code>OpenBoostClassifier</code>)</li> </ul>"},{"location":"releases/RELEASE_NOTES_1.0.0rc1/#performance","title":"Performance","text":"<p>OpenBoost GPU-accelerates GBDT variants that were previously slow: - NaturalBoost: 1.3-2x faster than NGBoost - OpenBoostGAM: 10-40x faster than InterpretML EBM</p> <p>For standard GBDT, XGBoost/LightGBM are faster. OpenBoost's value is in the variants.</p>"},{"location":"releases/RELEASE_NOTES_1.0.0rc1/#installation","title":"Installation","text":"<pre><code>pip install openboost\n\n# With GPU support\npip install openboost[cuda]\n</code></pre>"},{"location":"releases/RELEASE_NOTES_1.0.0rc1/#quick-start","title":"Quick Start","text":"<pre><code>import openboost as ob\n\nmodel = ob.GradientBoosting(n_trees=100, max_depth=6)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"releases/RELEASE_NOTES_1.0.0rc1/#known-limitations-rc1","title":"Known Limitations (rc1)","text":"<ul> <li><code>sample_weight</code> not yet fully supported on GPU (works on CPU)</li> <li><code>MultiClassGradientBoosting</code> does not support callbacks yet</li> <li>Multi-GPU requires Ray and raw numpy arrays</li> </ul>"},{"location":"releases/RELEASE_NOTES_1.0.0rc1/#links","title":"Links","text":"<ul> <li>Documentation: https://jxucoder.github.io/openboost</li> <li>GitHub: https://github.com/jxucoder/openboost</li> <li>Examples: https://github.com/jxucoder/openboost/tree/main/examples</li> </ul>"},{"location":"releases/RELEASE_NOTES_1.0.0rc1/#whats-next-for-100","title":"What's Next for 1.0.0","text":"<ul> <li>GPU sample_weight support</li> <li>Callbacks for multi-class models</li> <li>Additional distributions</li> </ul>"},{"location":"releases/RELEASE_NOTES_1.0.0rc1/#roadmap-train-many-optimization","title":"Roadmap: Train-Many Optimization","text":"<p>Industry workloads often require training many models at once: - Hyperparameter tuning (100s of configs) - Cross-validation (k models) - Per-segment models (one per store/product/customer)</p> <p>XGBoost/LightGBM optimize for training one model fast. OpenBoost's Python architecture plans to enable native optimization for training many models efficiently - batching, GPU parallelization across models, and amortized overhead.</p> <p>This is a planned advantage for future releases.</p> <p>Built for the agentic era: every algorithm readable, modifiable, debuggable in Python.</p>"},{"location":"tutorials/custom-loss/","title":"Custom Loss Functions","text":"<p>OpenBoost lets you define any loss function in Python. No C++, no recompilation.</p>"},{"location":"tutorials/custom-loss/#how-gradient-boosting-works","title":"How Gradient Boosting Works","text":"<p>Gradient boosting minimizes a loss function by iteratively fitting trees to negative gradients. For each iteration:</p> <ol> <li>Compute gradient: <code>grad = \u2202L/\u2202pred</code></li> <li>Compute Hessian: <code>hess = \u2202\u00b2L/\u2202pred\u00b2</code></li> <li>Fit tree to <code>(grad, hess)</code> weighted samples</li> <li>Update predictions: <code>pred += learning_rate * tree(X)</code></li> </ol> <p>To use a custom loss, you just need to provide the gradient and Hessian.</p>"},{"location":"tutorials/custom-loss/#basic-custom-loss","title":"Basic Custom Loss","text":"<p>A custom loss function takes predictions and targets, returns gradients and Hessians:</p> <pre><code>import numpy as np\nimport openboost as ob\n\ndef my_custom_loss(pred, y):\n    \"\"\"Custom loss function.\n\n    Args:\n        pred: Current predictions, shape (n_samples,)\n        y: True targets, shape (n_samples,)\n\n    Returns:\n        grad: Gradient of loss w.r.t. predictions, shape (n_samples,)\n        hess: Hessian (second derivative), shape (n_samples,)\n    \"\"\"\n    # Example: Mean Squared Error\n    # L = 0.5 * (pred - y)\u00b2\n    # grad = pred - y\n    # hess = 1\n\n    grad = (pred - y).astype(np.float32)\n    hess = np.ones_like(pred, dtype=np.float32)\n\n    return grad, hess\n\n# Use with GradientBoosting\nmodel = ob.GradientBoosting(n_trees=100, loss=my_custom_loss)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"tutorials/custom-loss/#example-asymmetric-loss","title":"Example: Asymmetric Loss","text":"<p>Penalize under-predictions more than over-predictions:</p> <pre><code>def asymmetric_loss(pred, y, alpha=0.7):\n    \"\"\"Asymmetric loss: heavier penalty for under-prediction.\n\n    L = alpha * |error| if error &gt; 0 (under-prediction)\n        (1-alpha) * |error| if error &lt; 0 (over-prediction)\n\n    Args:\n        alpha: Weight for under-prediction penalty (0.5 = symmetric)\n    \"\"\"\n    error = y - pred\n\n    # Gradient\n    grad = np.where(error &gt; 0, -alpha, 1 - alpha).astype(np.float32)\n\n    # Hessian (constant for linear loss)\n    hess = np.ones_like(pred, dtype=np.float32)\n\n    return grad, hess\n\n# Conservative model (prefers over-prediction)\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    loss=lambda p, y: asymmetric_loss(p, y, alpha=0.8),\n)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"tutorials/custom-loss/#example-quantile-regression","title":"Example: Quantile Regression","text":"<p>Predict any quantile (not just the mean):</p> <pre><code>def quantile_loss(pred, y, tau=0.5):\n    \"\"\"Quantile loss for any percentile.\n\n    tau=0.5: Median (robust to outliers)\n    tau=0.9: 90th percentile\n    tau=0.1: 10th percentile\n    \"\"\"\n    error = y - pred\n\n    grad = np.where(error &gt; 0, -tau, 1 - tau).astype(np.float32)\n    hess = np.ones_like(pred, dtype=np.float32)\n\n    return grad, hess\n\n# Predict 90th percentile\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    loss=lambda p, y: quantile_loss(p, y, tau=0.9),\n)\nmodel.fit(X_train, y_train)\n</code></pre> <p>Note: OpenBoost also has built-in quantile loss via <code>loss='quantile'</code> and <code>quantile_alpha=0.9</code>.</p>"},{"location":"tutorials/custom-loss/#example-huber-loss","title":"Example: Huber Loss","text":"<p>Robust to outliers (L2 near zero, L1 far from zero):</p> <pre><code>def huber_loss(pred, y, delta=1.0):\n    \"\"\"Huber loss: smooth transition between L2 and L1.\n\n    L = 0.5 * error\u00b2 if |error| &lt; delta\n        delta * (|error| - 0.5*delta) if |error| &gt;= delta\n    \"\"\"\n    error = pred - y\n    abs_error = np.abs(error)\n\n    # Gradient\n    grad = np.where(\n        abs_error &lt; delta,\n        error,  # L2 region: gradient = error\n        delta * np.sign(error)  # L1 region: gradient = \u00b1delta\n    ).astype(np.float32)\n\n    # Hessian\n    hess = np.where(\n        abs_error &lt; delta,\n        1.0,  # L2 region: constant Hessian\n        1e-6,  # L1 region: small constant for stability\n    ).astype(np.float32)\n\n    return grad, hess\n\nmodel = ob.GradientBoosting(n_trees=100, loss=huber_loss)\n</code></pre>"},{"location":"tutorials/custom-loss/#example-focal-loss-classification","title":"Example: Focal Loss (Classification)","text":"<p>For imbalanced classification, down-weight easy examples:</p> <pre><code>def focal_loss(pred, y, gamma=2.0):\n    \"\"\"Focal loss for imbalanced classification.\n\n    Focuses learning on hard misclassified examples.\n    gamma=0: Standard log loss\n    gamma=2: Strong focus on hard examples\n    \"\"\"\n    # Sigmoid to get probabilities\n    p = 1 / (1 + np.exp(-pred))\n    p = np.clip(p, 1e-7, 1 - 1e-7)\n\n    # Focal weight: (1-p_t)^gamma\n    p_t = np.where(y == 1, p, 1 - p)\n    focal_weight = (1 - p_t) ** gamma\n\n    # Gradient (includes focal weight)\n    grad = focal_weight * (p - y)\n\n    # Hessian (approximation)\n    hess = np.maximum(\n        focal_weight * p * (1 - p),\n        1e-6\n    ).astype(np.float32)\n\n    return grad.astype(np.float32), hess\n\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    loss=lambda p, y: focal_loss(p, y, gamma=2.0),\n)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"tutorials/custom-loss/#example-log-cosh-loss","title":"Example: Log-Cosh Loss","text":"<p>Smooth approximation to MAE:</p> <pre><code>def log_cosh_loss(pred, y):\n    \"\"\"Log-cosh loss: smooth approximation to L1.\n\n    L = log(cosh(error))\n    Behaves like L2 for small errors, L1 for large errors.\n    \"\"\"\n    error = pred - y\n\n    # Gradient: tanh(error)\n    grad = np.tanh(error).astype(np.float32)\n\n    # Hessian: sech\u00b2(error) = 1 - tanh\u00b2(error)\n    hess = (1 - np.tanh(error) ** 2).astype(np.float32)\n    hess = np.maximum(hess, 1e-6)  # Numerical stability\n\n    return grad, hess\n\nmodel = ob.GradientBoosting(n_trees=100, loss=log_cosh_loss)\n</code></pre>"},{"location":"tutorials/custom-loss/#low-level-api-full-control","title":"Low-Level API: Full Control","text":"<p>For complete control over the training loop:</p> <pre><code>import openboost as ob\nimport numpy as np\n\n# Bin data once\nX_binned = ob.array(X_train)\n\n# Initialize predictions\npred = np.zeros(len(y_train), dtype=np.float32)\ntrees = []\nlearning_rate = 0.1\n\nfor i in range(100):\n    # YOUR loss function\n    error = pred - y_train\n    grad = error  # MSE gradient\n    hess = np.ones_like(grad)\n\n    # Fit tree to gradients\n    tree = ob.fit_tree(\n        X_binned, grad, hess,\n        max_depth=6,\n        min_child_weight=1.0,\n        reg_lambda=1.0,\n    )\n\n    # Update predictions\n    pred = pred + learning_rate * tree(X_binned)\n    trees.append(tree)\n\n    # YOUR early stopping logic\n    if i % 10 == 0:\n        loss = 0.5 * np.mean(error ** 2)\n        print(f\"Round {i}: Loss = {loss:.4f}\")\n</code></pre>"},{"location":"tutorials/custom-loss/#using-with-pytorchjax","title":"Using with PyTorch/JAX","text":"<p>Get gradients from your deep learning framework:</p> <pre><code>import torch\nimport openboost as ob\n\n# Your custom PyTorch loss\ndef my_torch_loss(pred, y):\n    return torch.mean((pred - y) ** 2)\n\n# Training loop\nX_binned = ob.array(X_train)\npred = torch.zeros(len(y_train), requires_grad=True)\ny = torch.from_numpy(y_train)\n\nfor i in range(100):\n    # Compute gradients with PyTorch\n    loss = my_torch_loss(pred, y)\n    grad = torch.autograd.grad(loss, pred, create_graph=True)[0]\n\n    # For Hessian, use autograd again or approximate\n    hess = torch.ones_like(grad)  # Approximation\n\n    # Convert to numpy for OpenBoost\n    grad_np = grad.detach().numpy().astype(np.float32)\n    hess_np = hess.detach().numpy().astype(np.float32)\n\n    # Fit tree\n    tree = ob.fit_tree(X_binned, grad_np, hess_np, max_depth=6)\n\n    # Update predictions\n    tree_pred = torch.from_numpy(tree(X_binned))\n    pred = pred + 0.1 * tree_pred\n</code></pre>"},{"location":"tutorials/custom-loss/#tips-for-custom-losses","title":"Tips for Custom Losses","text":""},{"location":"tutorials/custom-loss/#1-ensure-numerical-stability","title":"1. Ensure Numerical Stability","text":"<pre><code>def stable_loss(pred, y):\n    # Clip values to avoid overflow\n    pred = np.clip(pred, -100, 100)\n\n    # Avoid division by zero in Hessian\n    hess = np.maximum(hess, 1e-6)\n\n    return grad.astype(np.float32), hess.astype(np.float32)\n</code></pre>"},{"location":"tutorials/custom-loss/#2-always-return-float32","title":"2. Always Return float32","text":"<p>OpenBoost uses float32 internally:</p> <pre><code>return grad.astype(np.float32), hess.astype(np.float32)\n</code></pre>"},{"location":"tutorials/custom-loss/#3-hessian-must-be-positive","title":"3. Hessian Must Be Positive","text":"<p>The Hessian should always be positive for the optimization to work properly:</p> <pre><code>hess = np.maximum(hess, 1e-6)\n</code></pre>"},{"location":"tutorials/custom-loss/#4-test-your-loss","title":"4. Test Your Loss","text":"<p>Verify gradients numerically:</p> <pre><code>def check_gradient(loss_fn, pred, y, eps=1e-5):\n    \"\"\"Check gradient with finite differences.\"\"\"\n    grad, _ = loss_fn(pred, y)\n\n    # Numerical gradient\n    numerical_grad = np.zeros_like(pred)\n    for i in range(len(pred)):\n        pred_plus = pred.copy()\n        pred_plus[i] += eps\n        pred_minus = pred.copy()\n        pred_minus[i] -= eps\n\n        loss_plus = np.mean((pred_plus - y) ** 2)  # Your loss\n        loss_minus = np.mean((pred_minus - y) ** 2)\n        numerical_grad[i] = (loss_plus - loss_minus) / (2 * eps)\n\n    print(f\"Max gradient error: {np.max(np.abs(grad - numerical_grad))}\")\n</code></pre>"},{"location":"tutorials/custom-loss/#built-in-loss-functions","title":"Built-in Loss Functions","text":"<p>OpenBoost includes these losses out of the box:</p> Loss String Use Case MSE <code>'mse'</code> Regression MAE <code>'mae'</code> Robust regression Huber <code>'huber'</code> Outlier-robust Quantile <code>'quantile'</code> Quantile regression LogLoss <code>'logloss'</code> Binary classification Softmax <code>'softmax'</code> Multi-class (use MultiClassGradientBoosting) Poisson <code>'poisson'</code> Count data Gamma <code>'gamma'</code> Positive continuous Tweedie <code>'tweedie'</code> Zero-inflated positive <pre><code># Using built-in losses\nmodel = ob.GradientBoosting(n_trees=100, loss='huber')\nmodel = ob.GradientBoosting(n_trees=100, loss='quantile', quantile_alpha=0.9)\nmodel = ob.GradientBoosting(n_trees=100, loss='tweedie', tweedie_rho=1.5)\n</code></pre>"},{"location":"tutorials/custom-loss/#next-steps","title":"Next Steps","text":"<ul> <li>Uncertainty Quantification - Probabilistic predictions</li> <li>Migration from XGBoost - Switching from XGBoost</li> </ul>"},{"location":"tutorials/uncertainty/","title":"Uncertainty Quantification with NaturalBoost","text":"<p>NaturalBoost predicts full probability distributions instead of just point estimates, giving you uncertainty bounds on your predictions.</p>"},{"location":"tutorials/uncertainty/#why-uncertainty-matters","title":"Why Uncertainty Matters","text":"<p>Traditional gradient boosting gives you a single number: \"the price will be $100\". But in reality, you might want to know:</p> <ul> <li>How confident is the model?</li> <li>What's the range of likely values?</li> <li>What's the probability of exceeding a threshold?</li> </ul> <p>NaturalBoost answers these questions by predicting distribution parameters (e.g., mean and variance for a Normal distribution).</p>"},{"location":"tutorials/uncertainty/#basic-usage","title":"Basic Usage","text":"<pre><code>import numpy as np\nimport openboost as ob\n\n# Generate heteroscedastic data (variance depends on X)\nnp.random.seed(42)\nX = np.random.randn(1000, 10).astype(np.float32)\nnoise_std = 0.5 + np.abs(X[:, 0])  # Noise increases with |X[:, 0]|\ny = (X[:, 0] * 2 + X[:, 1] + noise_std * np.random.randn(1000)).astype(np.float32)\n\nX_train, X_test = X[:800], X[800:]\ny_train, y_test = y[:800], y[800:]\n\n# Train NaturalBoost with Normal distribution\nmodel = ob.NaturalBoostNormal(\n    n_trees=100,\n    max_depth=4,\n    learning_rate=0.1,\n)\nmodel.fit(X_train, y_train)\n\n# Point prediction (mean)\nmean = model.predict(X_test)\n\n# Prediction interval (90% confidence)\nlower, upper = model.predict_interval(X_test, alpha=0.1)\n\nprint(f\"Coverage: {np.mean((y_test &gt;= lower) &amp; (y_test &lt;= upper)):.1%}\")\n# Should be close to 90%\n</code></pre>"},{"location":"tutorials/uncertainty/#distribution-output","title":"Distribution Output","text":"<p>For full control, use <code>predict_distribution()</code>:</p> <pre><code># Get full distribution output\noutput = model.predict_distribution(X_test)\n\n# Access distribution parameters\nprint(f\"Mean (loc):  {output.params['loc'][:5]}\")\nprint(f\"Std (scale): {output.params['scale'][:5]}\")\n\n# Convenience methods\nmean = output.mean()\nstd = output.std()\nvariance = output.variance()\n\n# Prediction intervals\nlower, upper = output.interval(alpha=0.1)  # 90% interval\nlower, upper = output.interval(alpha=0.05)  # 95% interval\n\n# Negative log-likelihood (useful for evaluation)\nnll = output.nll(y_test)\nprint(f\"Mean NLL: {np.mean(nll):.4f}\")\n</code></pre>"},{"location":"tutorials/uncertainty/#monte-carlo-sampling","title":"Monte Carlo Sampling","text":"<p>Sample from the predicted distribution for downstream analysis:</p> <pre><code># Draw samples from predicted distributions\nsamples = model.sample(X_test, n_samples=1000)  # Shape: (1000, n_test)\n\n# Use samples for risk analysis\nthreshold = 10.0\nprob_exceed = np.mean(samples &gt; threshold, axis=0)  # P(Y &gt; 10) for each sample\n\n# Quantile estimation\nq90 = np.percentile(samples, 90, axis=0)  # 90th percentile\n</code></pre>"},{"location":"tutorials/uncertainty/#available-distributions","title":"Available Distributions","text":""},{"location":"tutorials/uncertainty/#normal-gaussian","title":"Normal (Gaussian)","text":"<p>Best for: General regression with symmetric errors.</p> <pre><code>model = ob.NaturalBoostNormal(n_trees=100)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"tutorials/uncertainty/#log-normal","title":"Log-Normal","text":"<p>Best for: Positive data with right skew (prices, incomes, durations).</p> <pre><code>model = ob.NaturalBoostLogNormal(n_trees=100)\nmodel.fit(X_train, y_train)  # y must be positive\n\n# Predictions are in original scale\nmean = model.predict(X_test)  # E[Y], not E[log(Y)]\n</code></pre>"},{"location":"tutorials/uncertainty/#gamma","title":"Gamma","text":"<p>Best for: Positive continuous data (insurance claims, rainfall).</p> <pre><code>model = ob.NaturalBoostGamma(n_trees=100)\nmodel.fit(X_train, y_train)  # y must be positive\n</code></pre>"},{"location":"tutorials/uncertainty/#poisson","title":"Poisson","text":"<p>Best for: Count data (number of events, visitors, transactions).</p> <pre><code>model = ob.NaturalBoostPoisson(n_trees=100)\nmodel.fit(X_train, y_train)  # y: non-negative integers\n</code></pre>"},{"location":"tutorials/uncertainty/#student-t","title":"Student-t","text":"<p>Best for: Data with heavy tails and outliers.</p> <pre><code>model = ob.NaturalBoostStudentT(n_trees=100)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"tutorials/uncertainty/#tweedie","title":"Tweedie","text":"<p>Best for: Zero-inflated positive data (insurance claims, Kaggle competitions).</p> <pre><code>model = ob.NaturalBoostTweedie(n_trees=100)\nmodel.fit(X_train, y_train)  # y: non-negative, many zeros allowed\n\n# Default power=1.5 (compound Poisson-Gamma)\n# Use power closer to 1 for more zeros\n# Use power closer to 2 for Gamma-like behavior\n</code></pre>"},{"location":"tutorials/uncertainty/#negative-binomial","title":"Negative Binomial","text":"<p>Best for: Overdispersed count data (sales forecasting, Rossmann competition).</p> <pre><code>model = ob.NaturalBoostNegBin(n_trees=100)\nmodel.fit(X_train, y_train)  # y: non-negative integers\n\n# Better than Poisson when variance &gt; mean\n</code></pre>"},{"location":"tutorials/uncertainty/#comparing-distributions","title":"Comparing Distributions","text":"<p>To choose the right distribution, compare negative log-likelihood:</p> <pre><code>import openboost as ob\n\ndistributions = [\n    ('Normal', ob.NaturalBoostNormal),\n    ('LogNormal', ob.NaturalBoostLogNormal),\n    ('Gamma', ob.NaturalBoostGamma),\n]\n\nresults = []\nfor name, ModelClass in distributions:\n    model = ModelClass(n_trees=100, max_depth=4)\n    model.fit(X_train, y_train)\n    nll = model.score(X_test, y_test)  # Mean NLL\n    results.append((name, nll))\n\n# Lower NLL is better\nfor name, nll in sorted(results, key=lambda x: x[1]):\n    print(f\"{name}: NLL = {nll:.4f}\")\n</code></pre>"},{"location":"tutorials/uncertainty/#custom-distributions","title":"Custom Distributions","text":"<p>Define your own distribution:</p> <pre><code>import openboost as ob\nimport numpy as np\n\n# Method 1: Use create_custom_distribution (with autodiff)\nMyDist = ob.create_custom_distribution(\n    name='MyDist',\n    param_names=['loc', 'scale'],\n    nll_fn=lambda params, y: (\n        0.5 * np.log(2 * np.pi * params['scale']**2) +\n        0.5 * ((y - params['loc']) / params['scale'])**2\n    ),\n    mean_fn=lambda params: params['loc'],\n    param_transforms={'scale': 'softplus'},  # Ensure scale &gt; 0\n)\n\n# Use with NaturalBoost\nmodel = ob.NaturalBoost(distribution=MyDist(), n_trees=100)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"tutorials/uncertainty/#calibration","title":"Calibration","text":"<p>Check if your prediction intervals are well-calibrated:</p> <pre><code>def check_calibration(model, X_test, y_test, alphas=[0.1, 0.2, 0.3, 0.4, 0.5]):\n    \"\"\"Check if prediction intervals have correct coverage.\"\"\"\n    print(\"Alpha | Expected | Observed\")\n    print(\"-\" * 30)\n\n    for alpha in alphas:\n        lower, upper = model.predict_interval(X_test, alpha=alpha)\n        coverage = np.mean((y_test &gt;= lower) &amp; (y_test &lt;= upper))\n        expected = 1 - alpha\n        print(f\"{alpha:.2f}  | {expected:.0%}       | {coverage:.1%}\")\n\ncheck_calibration(model, X_test, y_test)\n</code></pre>"},{"location":"tutorials/uncertainty/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/uncertainty/#1-choose-the-right-distribution","title":"1. Choose the Right Distribution","text":"Data Type Recommended Distribution General continuous Normal Positive values LogNormal, Gamma Count data Poisson, Negative Binomial Heavy tails Student-t Zero-inflated positive Tweedie Overdispersed counts Negative Binomial"},{"location":"tutorials/uncertainty/#2-use-appropriate-tree-depth","title":"2. Use Appropriate Tree Depth","text":"<p>Shallower trees (max_depth=3-4) often work better for uncertainty estimation than deep trees.</p>"},{"location":"tutorials/uncertainty/#3-train-longer","title":"3. Train Longer","text":"<p>NaturalBoost learns two (or more) parameters per sample, so it may need more trees than standard GBDT.</p> <pre><code>model = ob.NaturalBoostNormal(\n    n_trees=500,      # More trees\n    max_depth=4,      # Shallower trees\n    learning_rate=0.05,  # Lower LR for stability\n)\n</code></pre>"},{"location":"tutorials/uncertainty/#4-evaluate-with-nll","title":"4. Evaluate with NLL","text":"<p>Use negative log-likelihood for model comparison, not just RMSE:</p> <pre><code>nll = model.score(X_test, y_test)  # Lower is better\n</code></pre>"},{"location":"tutorials/uncertainty/#example-insurance-claims-prediction","title":"Example: Insurance Claims Prediction","text":"<pre><code>import openboost as ob\n\n# Tweedie distribution for insurance claims (many zeros, heavy tail)\nmodel = ob.NaturalBoostTweedie(\n    n_trees=300,\n    max_depth=4,\n    learning_rate=0.05,\n)\nmodel.fit(X_train, claims)\n\n# Predict expected claim amount\nexpected_claim = model.predict(X_new)\n\n# Probability of large claim (risk assessment)\noutput = model.predict_distribution(X_new)\nsamples = model.sample(X_new, n_samples=10000)\nprob_large_claim = np.mean(samples &gt; 10000, axis=0)\n\nprint(f\"Expected claim: ${expected_claim[0]:.2f}\")\nprint(f\"P(claim &gt; $10k): {prob_large_claim[0]:.1%}\")\n</code></pre>"},{"location":"tutorials/uncertainty/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Loss Functions - Define your own objectives</li> <li>Migration from XGBoost - Switching from XGBoost</li> </ul>"},{"location":"user-guide/model-persistence/","title":"Model Persistence","text":"<p>Save and load trained OpenBoost models.</p>"},{"location":"user-guide/model-persistence/#basic-saveload","title":"Basic Save/Load","text":"<pre><code>import openboost as ob\n\n# Train\nmodel = ob.GradientBoosting(n_trees=100)\nmodel.fit(X_train, y_train)\n\n# Save\nmodel.save('my_model.joblib')\n\n# Load\nloaded_model = ob.GradientBoosting.load('my_model.joblib')\npredictions = loaded_model.predict(X_test)\n</code></pre>"},{"location":"user-guide/model-persistence/#supported-models","title":"Supported Models","text":"<p>All models support save/load:</p> <ul> <li><code>GradientBoosting</code></li> <li><code>MultiClassGradientBoosting</code></li> <li><code>DART</code></li> <li><code>OpenBoostGAM</code></li> <li><code>NaturalBoostNormal</code>, <code>NaturalBoostGamma</code>, etc.</li> <li><code>LinearLeafGBDT</code></li> </ul>"},{"location":"user-guide/model-persistence/#using-joblibpickle-directly","title":"Using joblib/pickle Directly","text":"<pre><code>import joblib\n\n# Save\njoblib.dump(model, 'model.joblib')\n\n# Load\nloaded = joblib.load('model.joblib')\n</code></pre> <p>Or with pickle:</p> <pre><code>import pickle\n\n# Save\nwith open('model.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\n# Load\nwith open('model.pkl', 'rb') as f:\n    loaded = pickle.load(f)\n</code></pre>"},{"location":"user-guide/model-persistence/#cross-platform-loading","title":"Cross-Platform Loading","text":"<p>Models trained on GPU can be loaded on CPU machines:</p> <pre><code># Train on GPU\nmodel = ob.GradientBoosting(n_trees=100)\nmodel.fit(X_train, y_train)  # Using CUDA\nmodel.save('model.joblib')\n\n# Load on CPU machine\nloaded = ob.GradientBoosting.load('model.joblib')\npredictions = loaded.predict(X_test)  # Works on CPU\n</code></pre>"},{"location":"user-guide/model-persistence/#versioning","title":"Versioning","text":"<p>Include version information when saving:</p> <pre><code>import openboost as ob\n\nmetadata = {\n    'openboost_version': ob.__version__,\n    'model_type': 'GradientBoosting',\n    'training_date': '2026-01-20',\n}\n\njoblib.dump({'model': model, 'metadata': metadata}, 'model_with_meta.joblib')\n</code></pre>"},{"location":"user-guide/model-persistence/#model-checkpointing","title":"Model Checkpointing","text":"<p>Save best models during training:</p> <pre><code>from openboost import ModelCheckpoint\n\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    callbacks=[\n        ModelCheckpoint(\n            filepath='checkpoints/best_model.joblib',\n            save_best_only=True,\n        )\n    ],\n)\n</code></pre>"},{"location":"user-guide/model-persistence/#file-formats","title":"File Formats","text":"Format Extension Pros Cons joblib <code>.joblib</code> Fast, compressed Python only pickle <code>.pkl</code> Standard library Slower, larger <p>Recommended: Use joblib for best performance.</p>"},{"location":"user-guide/sklearn-integration/","title":"sklearn Integration","text":"<p>OpenBoost provides sklearn-compatible wrappers for seamless integration with scikit-learn pipelines.</p>"},{"location":"user-guide/sklearn-integration/#available-wrappers","title":"Available Wrappers","text":"Wrapper Base Model Use Case <code>OpenBoostRegressor</code> GradientBoosting Regression <code>OpenBoostClassifier</code> GradientBoosting Classification <code>OpenBoostDistributionalRegressor</code> NaturalBoost Probabilistic regression <code>OpenBoostLinearLeafRegressor</code> LinearLeafGBDT Linear leaf regression"},{"location":"user-guide/sklearn-integration/#basic-usage","title":"Basic Usage","text":"<pre><code>from openboost import OpenBoostRegressor, OpenBoostClassifier\n\n# Regressor\nreg = OpenBoostRegressor(n_estimators=100, max_depth=6)\nreg.fit(X_train, y_train)\nprint(f\"R\u00b2 Score: {reg.score(X_test, y_test):.4f}\")\n\n# Classifier\nclf = OpenBoostClassifier(n_estimators=100, max_depth=6)\nclf.fit(X_train, y_train)\nprint(f\"Accuracy: {clf.score(X_test, y_test):.4f}\")\n</code></pre>"},{"location":"user-guide/sklearn-integration/#cross-validation","title":"Cross-Validation","text":"<pre><code>from sklearn.model_selection import cross_val_score\n\nreg = OpenBoostRegressor(n_estimators=100)\nscores = cross_val_score(reg, X, y, cv=5)\nprint(f\"CV Score: {scores.mean():.4f} \u00b1 {scores.std():.4f}\")\n</code></pre>"},{"location":"user-guide/sklearn-integration/#grid-search","title":"Grid Search","text":"<pre><code>from sklearn.model_selection import GridSearchCV\nfrom openboost import OpenBoostRegressor, get_param_grid\n\n# Get suggested parameter grid\nparam_grid = get_param_grid('regression')\n\n# Or define your own\nparam_grid = {\n    'n_estimators': [100, 300, 500],\n    'max_depth': [4, 6, 8],\n    'learning_rate': [0.01, 0.05, 0.1],\n}\n\nsearch = GridSearchCV(OpenBoostRegressor(), param_grid, cv=5)\nsearch.fit(X, y)\nprint(f\"Best params: {search.best_params_}\")\n</code></pre>"},{"location":"user-guide/sklearn-integration/#pipeline","title":"Pipeline","text":"<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom openboost import OpenBoostRegressor\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', OpenBoostRegressor(n_estimators=100)),\n])\n\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)\n</code></pre>"},{"location":"user-guide/sklearn-integration/#parameter-mapping","title":"Parameter Mapping","text":"<p>sklearn wrapper parameters map to OpenBoost parameters:</p> sklearn Parameter OpenBoost Parameter <code>n_estimators</code> <code>n_trees</code> <code>max_depth</code> <code>max_depth</code> <code>learning_rate</code> <code>learning_rate</code> <code>min_samples_leaf</code> <code>min_child_weight</code> <code>subsample</code> <code>subsample</code>"},{"location":"user-guide/sklearn-integration/#out-of-fold-predictions","title":"Out-of-Fold Predictions","text":"<pre><code>from openboost import cross_val_predict, cross_val_predict_proba\n\n# Regression\noof_pred = cross_val_predict(model, X, y, cv=5)\n\n# Classification\noof_proba = cross_val_predict_proba(classifier, X, y, cv=5)\n</code></pre>"},{"location":"user-guide/sklearn-integration/#distributional-sklearn-wrapper","title":"Distributional sklearn Wrapper","text":"<pre><code>from openboost import OpenBoostDistributionalRegressor\n\nmodel = OpenBoostDistributionalRegressor(\n    distribution='normal',\n    n_estimators=100,\n)\nmodel.fit(X_train, y_train)\n\n# Get prediction intervals\nlower, upper = model.predict_interval(X_test, alpha=0.1)\n</code></pre>"},{"location":"user-guide/evaluation/metrics/","title":"Evaluation Metrics","text":"<p>OpenBoost includes thin wrappers around sklearn metrics with full sample weight support.</p>"},{"location":"user-guide/evaluation/metrics/#regression-metrics","title":"Regression Metrics","text":"<pre><code>import openboost as ob\n\nmse = ob.mse_score(y_true, y_pred)\nmae = ob.mae_score(y_true, y_pred)\nrmse = ob.rmse_score(y_true, y_pred)\nr2 = ob.r2_score(y_true, y_pred)\n\n# With sample weights\nweighted_mse = ob.mse_score(y_true, y_pred, sample_weight=weights)\n</code></pre> Metric Function Description MSE <code>mse_score</code> Mean squared error MAE <code>mae_score</code> Mean absolute error RMSE <code>rmse_score</code> Root mean squared error R\u00b2 <code>r2_score</code> Coefficient of determination"},{"location":"user-guide/evaluation/metrics/#classification-metrics","title":"Classification Metrics","text":"<pre><code>import openboost as ob\n\nacc = ob.accuracy_score(y_true, y_pred)\nauc = ob.roc_auc_score(y_true, y_proba)\nlogloss = ob.log_loss_score(y_true, y_proba)\nf1 = ob.f1_score(y_true, y_pred)\nprecision = ob.precision_score(y_true, y_pred)\nrecall = ob.recall_score(y_true, y_pred)\n\n# With sample weights\nweighted_auc = ob.roc_auc_score(y_true, y_proba, sample_weight=weights)\n</code></pre> Metric Function Description Accuracy <code>accuracy_score</code> Classification accuracy ROC AUC <code>roc_auc_score</code> Area under ROC curve Log Loss <code>log_loss_score</code> Cross-entropy loss F1 <code>f1_score</code> Harmonic mean of precision/recall Precision <code>precision_score</code> True positives / predicted positives Recall <code>recall_score</code> True positives / actual positives"},{"location":"user-guide/evaluation/metrics/#probabilistic-metrics","title":"Probabilistic Metrics","text":"<p>For NaturalBoost and distributional models:</p> <pre><code>import openboost as ob\nimport numpy as np\n\n# Train probabilistic model\nmodel = ob.NaturalBoostNormal(n_trees=100)\nmodel.fit(X_train, y_train)\n\n# Get predictions\noutput = model.predict_distribution(X_test)\nmean = output.mean()\nstd = np.sqrt(output.variance())\n\n# CRPS - gold standard for probabilistic forecasting\ncrps = ob.crps_gaussian(y_test, mean, std)\n\n# For non-Gaussian, use Monte Carlo samples\nsamples = model.sample(X_test, n_samples=1000)\ncrps_mc = ob.crps_empirical(y_test, samples)\n\n# Negative log-likelihood\nnll = ob.negative_log_likelihood(y_test, mean, std)\n\n# Prediction interval evaluation\nlower, upper = model.predict_interval(X_test, alpha=0.1)\nscore = ob.interval_score(y_test, lower, upper, alpha=0.1)\n\n# Pinball loss for quantiles\nq90 = np.percentile(samples, 90, axis=0)\nloss = ob.pinball_loss(y_test, q90, quantile=0.9)\n</code></pre> Metric Function Use Case CRPS (Gaussian) <code>crps_gaussian</code> Probabilistic regression CRPS (Empirical) <code>crps_empirical</code> Any distribution Brier Score <code>brier_score</code> Probabilistic classification Pinball Loss <code>pinball_loss</code> Quantile regression Interval Score <code>interval_score</code> Prediction intervals NLL <code>negative_log_likelihood</code> Likelihood evaluation"},{"location":"user-guide/evaluation/metrics/#calibration-metrics","title":"Calibration Metrics","text":"<pre><code>import openboost as ob\n\n# For classifiers\nclassifier = ob.OpenBoostClassifier(n_estimators=100)\nclassifier.fit(X_train, y_train)\ny_proba = classifier.predict_proba(X_test)[:, 1]\n\n# Brier score\nbrier = ob.brier_score(y_test, y_proba)\n\n# Expected Calibration Error\nece = ob.expected_calibration_error(y_test, y_proba, n_bins=10)\n\n# Reliability diagram data\nfrac_pos, mean_pred, counts = ob.calibration_curve(y_test, y_proba, n_bins=10)\n</code></pre> Metric Function Description Brier Score <code>brier_score</code> Proper scoring rule for probabilities ECE <code>expected_calibration_error</code> Calibration quality Calibration Curve <code>calibration_curve</code> Data for reliability diagrams"},{"location":"user-guide/models/dart/","title":"DART","text":"<p>Dropout Additive Regression Trees - a regularization technique that randomly drops trees during training.</p>"},{"location":"user-guide/models/dart/#why-dart","title":"Why DART?","text":"<p>Standard gradient boosting can overfit, especially with many trees. DART addresses this by:</p> <ol> <li>Randomly dropping trees during each boosting iteration</li> <li>Normalizing the contribution of remaining trees</li> <li>Preventing individual trees from dominating</li> </ol>"},{"location":"user-guide/models/dart/#basic-usage","title":"Basic Usage","text":"<pre><code>import openboost as ob\n\nmodel = ob.DART(\n    n_trees=200,\n    max_depth=6,\n    dropout_rate=0.1,  # Drop 10% of trees\n)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"user-guide/models/dart/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>dropout_rate</code> float 0.1 Fraction of trees to drop <code>skip_drop</code> float 0.5 Probability of skipping dropout <p>Plus all standard <code>GradientBoosting</code> parameters.</p>"},{"location":"user-guide/models/dart/#when-to-use-dart","title":"When to Use DART","text":"Situation Recommendation Overfitting with many trees Try DART Small dataset DART can help Large dataset Standard GBDT usually fine Need interpretability Standard GBDT (DART trees interact)"},{"location":"user-guide/models/dart/#example","title":"Example","text":"<pre><code>import openboost as ob\nfrom sklearn.model_selection import cross_val_score\n\n# Compare standard GBDT vs DART\ngbdt = ob.GradientBoosting(n_trees=200, max_depth=6)\ndart = ob.DART(n_trees=200, max_depth=6, dropout_rate=0.1)\n\ngbdt_scores = cross_val_score(gbdt, X, y, cv=5)\ndart_scores = cross_val_score(dart, X, y, cv=5)\n\nprint(f\"GBDT: {gbdt_scores.mean():.4f} \u00b1 {gbdt_scores.std():.4f}\")\nprint(f\"DART: {dart_scores.mean():.4f} \u00b1 {dart_scores.std():.4f}\")\n</code></pre>"},{"location":"user-guide/models/gam/","title":"OpenBoostGAM","text":"<p>GPU-accelerated Generalized Additive Model - interpretable machine learning with feature-level explanations.</p>"},{"location":"user-guide/models/gam/#why-gam","title":"Why GAM?","text":"<p>GAMs decompose predictions into individual feature contributions:</p> <pre><code>prediction = f\u2081(x\u2081) + f\u2082(x\u2082) + ... + f\u2099(x\u2099) + intercept\n</code></pre> <p>This means you can visualize exactly how each feature affects the prediction.</p>"},{"location":"user-guide/models/gam/#basic-usage","title":"Basic Usage","text":"<pre><code>import openboost as ob\n\ngam = ob.OpenBoostGAM(\n    n_rounds=500,\n    learning_rate=0.05,\n    max_depth=3,  # Shallow trees for smoothness\n)\ngam.fit(X_train, y_train)\npredictions = gam.predict(X_test)\n\n# Get feature importance\nimportance = gam.get_feature_importance()\n</code></pre>"},{"location":"user-guide/models/gam/#visualizing-shape-functions","title":"Visualizing Shape Functions","text":"<pre><code># Plot how each feature affects predictions\ngam.plot_shape_function(\n    feature_idx=0,\n    feature_name=\"age\",\n)\n\n# Plot multiple features\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\nfor i, ax in enumerate(axes.flat):\n    gam.plot_shape_function(i, ax=ax, feature_name=feature_names[i])\nplt.tight_layout()\n</code></pre>"},{"location":"user-guide/models/gam/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>n_rounds</code> int 1000 Number of boosting rounds <code>learning_rate</code> float 0.01 Step size <code>max_depth</code> int 3 Tree depth (keep small for smoothness) <code>loss</code> str 'mse' Loss function"},{"location":"user-guide/models/gam/#performance-vs-interpretml-ebm","title":"Performance vs InterpretML EBM","text":"Samples EBM (CPU) OpenBoostGAM (GPU) Speedup 10,000 3.6s 0.14s 25x 50,000 6.3s 0.16s 39x 100,000 10.5s 0.25s 43x"},{"location":"user-guide/models/gam/#example-credit-risk","title":"Example: Credit Risk","text":"<pre><code>import openboost as ob\nimport matplotlib.pyplot as plt\n\n# Train interpretable model\ngam = ob.OpenBoostGAM(n_rounds=500, learning_rate=0.05)\ngam.fit(X_train, y_train)\n\n# Explain predictions\nfeature_names = ['age', 'income', 'debt_ratio', 'credit_history']\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfor i, (ax, name) in enumerate(zip(axes.flat, feature_names)):\n    gam.plot_shape_function(i, ax=ax, feature_name=name)\n    ax.set_title(f\"Effect of {name}\")\nplt.tight_layout()\nplt.savefig(\"gam_explanations.png\")\n</code></pre>"},{"location":"user-guide/models/gam/#best-practices","title":"Best Practices","text":"<ol> <li>Use shallow trees (<code>max_depth=3</code>) for smoother shape functions</li> <li>Use more rounds with lower learning rate for better fits</li> <li>Normalize features for easier interpretation</li> <li>Check shape functions for unexpected patterns (data issues)</li> </ol>"},{"location":"user-guide/models/gradient-boosting/","title":"Gradient Boosting","text":"<p>The core gradient boosting model for regression and binary classification.</p>"},{"location":"user-guide/models/gradient-boosting/#basic-usage","title":"Basic Usage","text":"<pre><code>import openboost as ob\n\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    max_depth=6,\n    learning_rate=0.1,\n    loss='mse',\n)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"user-guide/models/gradient-boosting/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>n_trees</code> int 100 Number of boosting iterations <code>max_depth</code> int 6 Maximum depth of each tree <code>learning_rate</code> float 0.1 Step size shrinkage <code>loss</code> str/callable 'mse' Loss function <code>min_child_weight</code> float 1.0 Minimum sum of hessian in a leaf <code>reg_lambda</code> float 1.0 L2 regularization <code>subsample</code> float 1.0 Row subsampling ratio <code>colsample_bytree</code> float 1.0 Column subsampling ratio <code>n_bins</code> int 256 Number of histogram bins"},{"location":"user-guide/models/gradient-boosting/#loss-functions","title":"Loss Functions","text":"Loss Use Case <code>'mse'</code> Regression (default) <code>'mae'</code> Robust regression <code>'huber'</code> Outlier-robust regression <code>'logloss'</code> Binary classification <code>'quantile'</code> Quantile regression Custom callable Your own loss"},{"location":"user-guide/models/gradient-boosting/#custom-loss-function","title":"Custom Loss Function","text":"<pre><code>def quantile_loss(pred, y, tau=0.9):\n    residual = y - pred\n    grad = np.where(residual &gt; 0, -tau, 1 - tau)\n    hess = np.ones_like(pred)\n    return grad, hess\n\nmodel = ob.GradientBoosting(n_trees=100, loss=quantile_loss)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"user-guide/models/gradient-boosting/#training-with-validation","title":"Training with Validation","text":"<pre><code>model = ob.GradientBoosting(n_trees=500, max_depth=6)\n\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    callbacks=[\n        ob.EarlyStopping(patience=10),\n        ob.Logger(every=10),\n    ],\n)\n</code></pre>"},{"location":"user-guide/models/gradient-boosting/#feature-importance","title":"Feature Importance","text":"<pre><code>model.fit(X_train, y_train)\n\n# Compute importance\nimportance = ob.compute_feature_importances(model.trees_)\n\n# Plot\nob.plot_feature_importances(model.trees_, feature_names)\n</code></pre>"},{"location":"user-guide/models/gradient-boosting/#growth-strategies","title":"Growth Strategies","text":"<pre><code># Level-wise (XGBoost-style, default)\nmodel = ob.GradientBoosting(growth='levelwise')\n\n# Leaf-wise (LightGBM-style)\nmodel = ob.GradientBoosting(growth='leafwise')\n\n# Symmetric/Oblivious (CatBoost-style)\nmodel = ob.GradientBoosting(growth='symmetric')\n</code></pre>"},{"location":"user-guide/models/gradient-boosting/#api-reference","title":"API Reference","text":""},{"location":"user-guide/models/gradient-boosting/#openboost.GradientBoosting","title":"GradientBoosting  <code>dataclass</code>","text":"<pre><code>GradientBoosting(\n    n_trees=100,\n    max_depth=6,\n    learning_rate=0.1,\n    loss=\"mse\",\n    min_child_weight=1.0,\n    reg_lambda=1.0,\n    reg_alpha=0.0,\n    gamma=0.0,\n    subsample=1.0,\n    colsample_bytree=1.0,\n    n_bins=256,\n    quantile_alpha=0.5,\n    tweedie_rho=1.5,\n    distributed=False,\n    n_workers=None,\n    subsample_strategy=\"none\",\n    goss_top_rate=0.2,\n    goss_other_rate=0.1,\n    batch_size=None,\n    n_gpus=None,\n    devices=None,\n)\n</code></pre> <p>               Bases: <code>PersistenceMixin</code></p> <p>Gradient Boosting ensemble model.</p> <p>A gradient boosting model that supports both built-in loss functions and custom loss functions. When using built-in losses with GPU, training is fully batched for maximum performance.</p> PARAMETER DESCRIPTION <code>n_trees</code> <p>Number of trees to train.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>max_depth</code> <p>Maximum depth of each tree.</p> <p> TYPE: <code>int</code> DEFAULT: <code>6</code> </p> <code>learning_rate</code> <p>Shrinkage factor applied to each tree.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>loss</code> <p>Loss function. Can be: - 'mse': Mean Squared Error (regression) - 'logloss': Binary cross-entropy (classification) - 'huber': Huber loss (robust regression) - 'mae': Mean Absolute Error (L1 regression) - 'quantile': Quantile regression (use with quantile_alpha) - Callable: Custom function(pred, y) -&gt; (grad, hess)</p> <p> TYPE: <code>str | LossFunction | Callable[..., tuple]</code> DEFAULT: <code>'mse'</code> </p> <code>min_child_weight</code> <p>Minimum sum of hessian in a leaf.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>reg_lambda</code> <p>L2 regularization on leaf values.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_bins</code> <p>Number of bins for histogram building.</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> <code>quantile_alpha</code> <p>Quantile level for 'quantile' loss (0 &lt; alpha &lt; 1). - 0.5: Median regression (default) - 0.9: 90th percentile - 0.1: 10th percentile</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>tweedie_rho</code> <p>Variance power for 'tweedie' loss (1 &lt; rho &lt; 2). - 1.5: Default (compound Poisson-Gamma)</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.5</code> </p> <code>subsample_strategy</code> <p>Sampling strategy for large-scale training (Phase 17): - 'none': No sampling (default) - 'random': Random subsampling - 'goss': Gradient-based One-Side Sampling (LightGBM-style)</p> <p> TYPE: <code>Literal['none', 'random', 'goss']</code> DEFAULT: <code>'none'</code> </p> <code>goss_top_rate</code> <p>Fraction of top-gradient samples to keep (for GOSS).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>goss_other_rate</code> <p>Fraction of remaining samples to sample (for GOSS).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>batch_size</code> <p>Mini-batch size for large datasets. If None, process all at once.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <p>Basic regression:</p> <pre><code>import openboost as ob\n\nmodel = ob.GradientBoosting(n_trees=100, loss='mse')\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n</code></pre> <p>Quantile regression (90th percentile):</p> <pre><code>model = ob.GradientBoosting(loss='quantile', quantile_alpha=0.9)\nmodel.fit(X_train, y_train)\n</code></pre> <p>GOSS for faster training:</p> <pre><code>model = ob.GradientBoosting(\n    n_trees=100,\n    subsample_strategy='goss',\n    goss_top_rate=0.2,\n    goss_other_rate=0.1,\n)\n</code></pre> <p>Multi-GPU training:</p> <pre><code>model = ob.GradientBoosting(n_trees=100, n_gpus=4)\nmodel.fit(X, y)  # Data parallel across 4 GPUs\n</code></pre>"},{"location":"user-guide/models/gradient-boosting/#openboost.GradientBoosting.fit","title":"fit","text":"<pre><code>fit(\n    X, y, callbacks=None, eval_set=None, sample_weight=None\n)\n</code></pre> <p>Fit the gradient boosting model.</p> PARAMETER DESCRIPTION <code>X</code> <p>Training features, shape (n_samples, n_features).</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Training targets, shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> <code>callbacks</code> <p>List of Callback instances for training hooks.        Use EarlyStopping for early stopping, Logger for progress.</p> <p> TYPE: <code>list[Callback] | None</code> DEFAULT: <code>None</code> </p> <code>eval_set</code> <p>List of (X, y) tuples for validation (used with callbacks).</p> <p> TYPE: <code>list[tuple[NDArray, NDArray]] | None</code> DEFAULT: <code>None</code> </p> <code>sample_weight</code> <p>Sample weights, shape (n_samples,).</p> <p> TYPE: <code>NDArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>self</code> <p>The fitted model.</p> <p> TYPE: <code>GradientBoosting</code> </p> Example <pre><code>from openboost import GradientBoosting, EarlyStopping, Logger\n\nmodel = GradientBoosting(n_trees=1000)\nmodel.fit(\n    X, y,\n    callbacks=[EarlyStopping(patience=50), Logger(period=10)],\n    eval_set=[(X_val, y_val)]\n)\n</code></pre>"},{"location":"user-guide/models/gradient-boosting/#openboost.GradientBoosting.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Generate predictions for X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Features to predict on, shape (n_samples, n_features). Can be raw numpy array or pre-binned BinnedArray.</p> <p> TYPE: <code>NDArray | BinnedArray</code> </p> RETURNS DESCRIPTION <code>predictions</code> <p>Shape (n_samples,).</p> <p> TYPE: <code>NDArray</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If model is not fitted or X has wrong shape.</p>"},{"location":"user-guide/models/linear-leaf/","title":"Linear Leaf GBDT","text":"<p>Trees with linear models in the leaves instead of constant values. Better for extrapolation and smooth relationships.</p>"},{"location":"user-guide/models/linear-leaf/#why-linear-leaves","title":"Why Linear Leaves?","text":"<p>Standard trees predict constant values in each leaf. Linear Leaf GBDT fits a linear model in each leaf, which:</p> <ul> <li>Captures linear trends within regions</li> <li>Extrapolates better outside training data</li> <li>Needs fewer trees for smooth relationships</li> </ul>"},{"location":"user-guide/models/linear-leaf/#basic-usage","title":"Basic Usage","text":"<pre><code>import openboost as ob\n\nmodel = ob.LinearLeafGBDT(\n    n_trees=100,\n    max_depth=4,\n    learning_rate=0.1,\n)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"user-guide/models/linear-leaf/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>ridge_lambda</code> float 1.0 Ridge regularization for leaf linear models <p>Plus all standard <code>GradientBoosting</code> parameters.</p>"},{"location":"user-guide/models/linear-leaf/#when-to-use","title":"When to Use","text":"Situation Recommendation Data has linear trends LinearLeafGBDT Need extrapolation LinearLeafGBDT Purely nonlinear data Standard GBDT Maximum speed Standard GBDT"},{"location":"user-guide/models/linear-leaf/#example","title":"Example","text":"<pre><code>import numpy as np\nimport openboost as ob\n\n# Data with linear trend + nonlinear pattern\nX = np.random.randn(1000, 5).astype(np.float32)\ny = 2 * X[:, 0] + np.sin(X[:, 1] * 3) + 0.1 * np.random.randn(1000)\ny = y.astype(np.float32)\n\n# Compare models\ngbdt = ob.GradientBoosting(n_trees=100, max_depth=6)\nlinear_leaf = ob.LinearLeafGBDT(n_trees=100, max_depth=4)\n\ngbdt.fit(X[:800], y[:800])\nlinear_leaf.fit(X[:800], y[:800])\n\n# Evaluate\ngbdt_rmse = np.sqrt(np.mean((gbdt.predict(X[800:]) - y[800:])**2))\nll_rmse = np.sqrt(np.mean((linear_leaf.predict(X[800:]) - y[800:])**2))\n\nprint(f\"GBDT RMSE: {gbdt_rmse:.4f}\")\nprint(f\"LinearLeaf RMSE: {ll_rmse:.4f}\")\n</code></pre>"},{"location":"user-guide/models/linear-leaf/#sklearn-wrapper","title":"sklearn Wrapper","text":"<pre><code>from openboost import OpenBoostLinearLeafRegressor\n\nmodel = OpenBoostLinearLeafRegressor(n_estimators=100, max_depth=4)\nmodel.fit(X_train, y_train)\nprint(f\"R\u00b2 Score: {model.score(X_test, y_test):.4f}\")\n</code></pre>"},{"location":"user-guide/models/multiclass/","title":"Multi-class Classification","text":"<p>For classification problems with more than 2 classes.</p>"},{"location":"user-guide/models/multiclass/#basic-usage","title":"Basic Usage","text":"<pre><code>import openboost as ob\n\nmodel = ob.MultiClassGradientBoosting(\n    n_classes=5,\n    n_trees=100,\n    max_depth=6,\n)\nmodel.fit(X_train, y_train)  # y_train: 0, 1, 2, 3, or 4\n\n# Get class probabilities\nprobabilities = model.predict_proba(X_test)  # Shape: (n_samples, n_classes)\n\n# Get class predictions\npredictions = model.predict(X_test)  # Shape: (n_samples,)\n</code></pre>"},{"location":"user-guide/models/multiclass/#parameters","title":"Parameters","text":"<p>Same as <code>GradientBoosting</code>, plus:</p> Parameter Type Default Description <code>n_classes</code> int required Number of classes"},{"location":"user-guide/models/multiclass/#example-iris-classification","title":"Example: Iris Classification","text":"<pre><code>import numpy as np\nimport openboost as ob\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load data\niris = load_iris()\nX, y = iris.data.astype(np.float32), iris.target\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Train\nmodel = ob.MultiClassGradientBoosting(\n    n_classes=3,\n    n_trees=100,\n    max_depth=4,\n)\nmodel.fit(X_train, y_train)\n\n# Evaluate\npredictions = model.predict(X_test)\naccuracy = np.mean(predictions == y_test)\nprint(f\"Accuracy: {accuracy:.2%}\")\n</code></pre>"},{"location":"user-guide/models/multiclass/#sklearn-wrapper","title":"sklearn Wrapper","text":"<p>For scikit-learn compatibility:</p> <pre><code>from openboost import OpenBoostClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf = OpenBoostClassifier(n_estimators=100, max_depth=6)\nscores = cross_val_score(clf, X, y, cv=5)\nprint(f\"CV Accuracy: {scores.mean():.2%}\")\n</code></pre>"},{"location":"user-guide/naturalboost/custom-distributions/","title":"Custom Distributions","text":"<p>Define your own probability distribution for NaturalBoost.</p>"},{"location":"user-guide/naturalboost/custom-distributions/#using-create_custom_distribution","title":"Using <code>create_custom_distribution</code>","text":"<pre><code>import openboost as ob\nimport numpy as np\n\n# Define a custom distribution\nMyDist = ob.create_custom_distribution(\n    name='MyDist',\n    param_names=['loc', 'scale'],\n    nll_fn=lambda params, y: (\n        0.5 * np.log(2 * np.pi * params['scale']**2) +\n        0.5 * ((y - params['loc']) / params['scale'])**2\n    ),\n    mean_fn=lambda params: params['loc'],\n    param_transforms={'scale': 'softplus'},  # Ensure scale &gt; 0\n)\n\n# Use with NaturalBoost\nmodel = ob.NaturalBoost(distribution=MyDist(), n_trees=100)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"user-guide/naturalboost/custom-distributions/#parameter-transforms","title":"Parameter Transforms","text":"<p>Transforms ensure parameters stay in valid ranges:</p> Transform Range Use For <code>'softplus'</code> (0, \u221e) Scale, variance <code>'exp'</code> (0, \u221e) Rate parameters <code>'sigmoid'</code> (0, 1) Probabilities <code>None</code> (-\u221e, \u221e) Location parameters"},{"location":"user-guide/naturalboost/custom-distributions/#full-example-laplace-distribution","title":"Full Example: Laplace Distribution","text":"<pre><code>import openboost as ob\nimport numpy as np\n\ndef laplace_nll(params, y):\n    \"\"\"Negative log-likelihood for Laplace distribution.\"\"\"\n    loc = params['loc']\n    scale = params['scale']\n    return np.log(2 * scale) + np.abs(y - loc) / scale\n\ndef laplace_mean(params):\n    return params['loc']\n\ndef laplace_std(params):\n    return params['scale'] * np.sqrt(2)\n\nLaplaceDist = ob.create_custom_distribution(\n    name='Laplace',\n    param_names=['loc', 'scale'],\n    nll_fn=laplace_nll,\n    mean_fn=laplace_mean,\n    std_fn=laplace_std,\n    param_transforms={'scale': 'softplus'},\n)\n\n# Train\nmodel = ob.NaturalBoost(distribution=LaplaceDist(), n_trees=100)\nmodel.fit(X_train, y_train)\n\n# Predict\nmean = model.predict(X_test)\nlower, upper = model.predict_interval(X_test, alpha=0.1)\n</code></pre>"},{"location":"user-guide/naturalboost/custom-distributions/#with-jax-autodiff","title":"With JAX Autodiff","text":"<p>For automatic gradient computation:</p> <pre><code>import jax.numpy as jnp\n\ndef custom_nll_jax(params, y):\n    \"\"\"JAX-compatible NLL for autodiff.\"\"\"\n    loc = params['loc']\n    scale = params['scale']\n    return 0.5 * jnp.log(2 * jnp.pi * scale**2) + 0.5 * ((y - loc) / scale)**2\n\n# Gradients computed automatically via JAX\nMyDist = ob.create_custom_distribution(\n    name='MyDist',\n    param_names=['loc', 'scale'],\n    nll_fn=custom_nll_jax,\n    autodiff='jax',  # Use JAX for gradients\n)\n</code></pre>"},{"location":"user-guide/naturalboost/custom-distributions/#available-built-in-distributions","title":"Available Built-in Distributions","text":"<pre><code>import openboost as ob\n\n# List all available distributions\nprint(ob.list_distributions())\n\n# Get a distribution by name\nnormal = ob.get_distribution('normal')\ngamma = ob.get_distribution('gamma')\n</code></pre>"},{"location":"user-guide/naturalboost/distributions/","title":"Distributions","text":"<p>NaturalBoost supports multiple probability distributions for different data types.</p>"},{"location":"user-guide/naturalboost/distributions/#choosing-a-distribution","title":"Choosing a Distribution","text":"Data Type Recommended Distribution General continuous Normal Positive values LogNormal, Gamma Count data Poisson, Negative Binomial Heavy tails Student-t Zero-inflated positive Tweedie Overdispersed counts Negative Binomial"},{"location":"user-guide/naturalboost/distributions/#normal-gaussian","title":"Normal (Gaussian)","text":"<p>Best for: General regression with symmetric errors.</p> <pre><code>model = ob.NaturalBoostNormal(n_trees=100)\nmodel.fit(X_train, y_train)\n\nmean = model.predict(X_test)\nlower, upper = model.predict_interval(X_test, alpha=0.1)\n</code></pre>"},{"location":"user-guide/naturalboost/distributions/#log-normal","title":"Log-Normal","text":"<p>Best for: Positive data with right skew (prices, incomes, durations).</p> <pre><code>model = ob.NaturalBoostLogNormal(n_trees=100)\nmodel.fit(X_train, y_train)  # y must be positive\n\n# Predictions are in original scale\nmean = model.predict(X_test)  # E[Y], not E[log(Y)]\n</code></pre>"},{"location":"user-guide/naturalboost/distributions/#gamma","title":"Gamma","text":"<p>Best for: Positive continuous data (insurance claims, rainfall).</p> <pre><code>model = ob.NaturalBoostGamma(n_trees=100)\nmodel.fit(X_train, y_train)  # y must be positive\n</code></pre>"},{"location":"user-guide/naturalboost/distributions/#poisson","title":"Poisson","text":"<p>Best for: Count data (number of events, visitors, transactions).</p> <pre><code>model = ob.NaturalBoostPoisson(n_trees=100)\nmodel.fit(X_train, y_train)  # y: non-negative integers\n</code></pre>"},{"location":"user-guide/naturalboost/distributions/#student-t","title":"Student-t","text":"<p>Best for: Data with heavy tails and outliers.</p> <pre><code>model = ob.NaturalBoostStudentT(n_trees=100)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"user-guide/naturalboost/distributions/#tweedie","title":"Tweedie","text":"<p>Best for: Zero-inflated positive data (insurance claims, Kaggle competitions).</p> <pre><code>model = ob.NaturalBoostTweedie(n_trees=100)\nmodel.fit(X_train, y_train)  # y: non-negative, many zeros allowed\n\n# Default power=1.5 (compound Poisson-Gamma)\n# Use power closer to 1 for more zeros\n# Use power closer to 2 for Gamma-like behavior\n</code></pre>"},{"location":"user-guide/naturalboost/distributions/#negative-binomial","title":"Negative Binomial","text":"<p>Best for: Overdispersed count data (sales forecasting, Rossmann competition).</p> <pre><code>model = ob.NaturalBoostNegBin(n_trees=100)\nmodel.fit(X_train, y_train)  # y: non-negative integers\n\n# Better than Poisson when variance &gt; mean\n</code></pre>"},{"location":"user-guide/naturalboost/distributions/#comparing-distributions","title":"Comparing Distributions","text":"<p>Use negative log-likelihood to choose the best distribution:</p> <pre><code>import openboost as ob\n\ndistributions = [\n    ('Normal', ob.NaturalBoostNormal),\n    ('LogNormal', ob.NaturalBoostLogNormal),\n    ('Gamma', ob.NaturalBoostGamma),\n]\n\nresults = []\nfor name, ModelClass in distributions:\n    model = ModelClass(n_trees=100, max_depth=4)\n    model.fit(X_train, y_train)\n    nll = model.score(X_test, y_test)  # Mean NLL\n    results.append((name, nll))\n\n# Lower NLL is better\nfor name, nll in sorted(results, key=lambda x: x[1]):\n    print(f\"{name}: NLL = {nll:.4f}\")\n</code></pre>"},{"location":"user-guide/naturalboost/overview/","title":"NaturalBoost Overview","text":"<p>NaturalBoost predicts full probability distributions instead of just point estimates, giving you uncertainty bounds on your predictions.</p>"},{"location":"user-guide/naturalboost/overview/#why-uncertainty-matters","title":"Why Uncertainty Matters","text":"<p>Traditional gradient boosting gives you a single number: \"the price will be $100\". But in reality, you might want to know:</p> <ul> <li>How confident is the model?</li> <li>What's the range of likely values?</li> <li>What's the probability of exceeding a threshold?</li> </ul> <p>NaturalBoost answers these questions by predicting distribution parameters (e.g., mean and variance for a Normal distribution).</p>"},{"location":"user-guide/naturalboost/overview/#quick-start","title":"Quick Start","text":"<pre><code>import numpy as np\nimport openboost as ob\n\n# Train probabilistic model\nmodel = ob.NaturalBoostNormal(n_trees=100, max_depth=4)\nmodel.fit(X_train, y_train)\n\n# Point prediction (mean)\nmean = model.predict(X_test)\n\n# 90% prediction interval\nlower, upper = model.predict_interval(X_test, alpha=0.1)\n\n# Check coverage\ncoverage = np.mean((y_test &gt;= lower) &amp; (y_test &lt;= upper))\nprint(f\"Coverage: {coverage:.1%}\")  # Should be ~90%\n</code></pre>"},{"location":"user-guide/naturalboost/overview/#available-models","title":"Available Models","text":"Model Distribution Use Case <code>NaturalBoostNormal</code> Gaussian General uncertainty <code>NaturalBoostLogNormal</code> Log-Normal Positive skewed (prices) <code>NaturalBoostGamma</code> Gamma Positive continuous <code>NaturalBoostPoisson</code> Poisson Count data <code>NaturalBoostStudentT</code> Student-t Heavy tails, outliers <code>NaturalBoostTweedie</code> Tweedie Insurance claims (Kaggle!) <code>NaturalBoostNegBin</code> Negative Binomial Sales forecasting (Kaggle!)"},{"location":"user-guide/naturalboost/overview/#distribution-output","title":"Distribution Output","text":"<p>For full control, use <code>predict_distribution()</code>:</p> <pre><code>output = model.predict_distribution(X_test)\n\n# Access distribution parameters\nmean = output.mean()\nstd = output.std()\nvariance = output.variance()\n\n# Prediction intervals\nlower, upper = output.interval(alpha=0.1)  # 90% interval\n\n# Negative log-likelihood\nnll = output.nll(y_test)\nprint(f\"Mean NLL: {np.mean(nll):.4f}\")\n</code></pre>"},{"location":"user-guide/naturalboost/overview/#monte-carlo-sampling","title":"Monte Carlo Sampling","text":"<p>Sample from the predicted distribution for downstream analysis:</p> <pre><code># Draw samples\nsamples = model.sample(X_test, n_samples=1000)  # Shape: (1000, n_test)\n\n# Risk analysis\nthreshold = 10.0\nprob_exceed = np.mean(samples &gt; threshold, axis=0)  # P(Y &gt; 10)\n\n# Quantile estimation\nq90 = np.percentile(samples, 90, axis=0)\n</code></pre>"},{"location":"user-guide/naturalboost/overview/#performance-vs-ngboost","title":"Performance vs NGBoost","text":"Samples NGBoost NaturalBoost (GPU) Speedup 5,000 5.2s 1.9s 2.8x 10,000 10.6s 1.9s 5.6x 20,000 22.2s 2.0s 11.3x"},{"location":"user-guide/naturalboost/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Use shallower trees (<code>max_depth=3-4</code>) - better for uncertainty estimation</li> <li>Train longer - NaturalBoost learns 2+ parameters per sample</li> <li>Evaluate with NLL - not just RMSE</li> </ol> <pre><code>model = ob.NaturalBoostNormal(\n    n_trees=500,      # More trees\n    max_depth=4,      # Shallower\n    learning_rate=0.05,  # Lower LR\n)\n</code></pre>"},{"location":"user-guide/training/callbacks/","title":"Callbacks","text":"<p>Control training with callbacks for early stopping, logging, checkpointing, and more.</p>"},{"location":"user-guide/training/callbacks/#available-callbacks","title":"Available Callbacks","text":"Callback Purpose <code>EarlyStopping</code> Stop when validation metric stops improving <code>Logger</code> Print training progress <code>ModelCheckpoint</code> Save best models during training <code>LearningRateScheduler</code> Dynamic learning rate"},{"location":"user-guide/training/callbacks/#early-stopping","title":"Early Stopping","text":"<pre><code>import openboost as ob\nfrom openboost import EarlyStopping\n\nmodel = ob.GradientBoosting(n_trees=500)\n\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    callbacks=[EarlyStopping(patience=10, min_delta=0.001)],\n)\n\nprint(f\"Stopped at {len(model.trees_)} trees\")\n</code></pre>"},{"location":"user-guide/training/callbacks/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>patience</code> int 10 Rounds without improvement before stopping <code>min_delta</code> float 0.0 Minimum change to qualify as improvement <code>restore_best</code> bool True Restore weights from best iteration"},{"location":"user-guide/training/callbacks/#logger","title":"Logger","text":"<pre><code>from openboost import Logger\n\nmodel.fit(\n    X_train, y_train,\n    callbacks=[Logger(every=10)],  # Print every 10 trees\n)\n</code></pre> <p>Output: <pre><code>[10] train_loss: 0.1234\n[20] train_loss: 0.0987\n[30] train_loss: 0.0856\n...\n</code></pre></p>"},{"location":"user-guide/training/callbacks/#model-checkpoint","title":"Model Checkpoint","text":"<pre><code>from openboost import ModelCheckpoint\n\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    callbacks=[\n        ModelCheckpoint(\n            filepath='best_model.joblib',\n            save_best_only=True,\n        )\n    ],\n)\n</code></pre>"},{"location":"user-guide/training/callbacks/#learning-rate-scheduler","title":"Learning Rate Scheduler","text":"<pre><code>from openboost import LearningRateScheduler\n\ndef lr_schedule(round_num, current_lr):\n    \"\"\"Reduce LR by 10% every 100 rounds.\"\"\"\n    if round_num &gt; 0 and round_num % 100 == 0:\n        return current_lr * 0.9\n    return current_lr\n\nmodel.fit(\n    X_train, y_train,\n    callbacks=[LearningRateScheduler(lr_schedule)],\n)\n</code></pre>"},{"location":"user-guide/training/callbacks/#combining-callbacks","title":"Combining Callbacks","text":"<pre><code>from openboost import EarlyStopping, Logger, ModelCheckpoint\n\ncallbacks = [\n    EarlyStopping(patience=20),\n    Logger(every=10),\n    ModelCheckpoint('checkpoints/model_{round}.joblib'),\n]\n\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    callbacks=callbacks,\n)\n</code></pre>"},{"location":"user-guide/training/callbacks/#custom-callbacks","title":"Custom Callbacks","text":"<pre><code>from openboost import Callback\n\nclass MyCallback(Callback):\n    def on_train_begin(self, state):\n        print(\"Training started!\")\n\n    def on_round_end(self, state):\n        if state.round % 50 == 0:\n            print(f\"Round {state.round}: loss = {state.train_loss:.4f}\")\n\n    def on_train_end(self, state):\n        print(f\"Training finished after {state.round} rounds\")\n\nmodel.fit(X_train, y_train, callbacks=[MyCallback()])\n</code></pre>"},{"location":"user-guide/training/callbacks/#callback-methods","title":"Callback Methods","text":"Method When Called <code>on_train_begin(state)</code> Before training starts <code>on_round_begin(state)</code> Before each boosting round <code>on_round_end(state)</code> After each boosting round <code>on_train_end(state)</code> After training completes"},{"location":"user-guide/training/large-scale/","title":"Large-Scale Training","text":"<p>Train on datasets that don't fit in memory or need faster training.</p>"},{"location":"user-guide/training/large-scale/#goss-sampling","title":"GOSS Sampling","text":"<p>Gradient-based One-Side Sampling (from LightGBM) - train 3x faster with minimal accuracy loss.</p> <pre><code>import openboost as ob\n\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    subsample_strategy='goss',\n    goss_top_rate=0.2,    # Keep top 20% high-gradient samples\n    goss_other_rate=0.1,  # Sample 10% of the rest\n)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"user-guide/training/large-scale/#how-goss-works","title":"How GOSS Works","text":"<ol> <li>Sort samples by gradient magnitude</li> <li>Keep top <code>goss_top_rate</code> samples (most informative)</li> <li>Randomly sample <code>goss_other_rate</code> from the rest</li> <li>Weight the random samples to maintain unbiased gradients</li> </ol>"},{"location":"user-guide/training/large-scale/#goss-parameters","title":"GOSS Parameters","text":"Parameter Default Description <code>goss_top_rate</code> 0.2 Fraction of high-gradient samples to keep <code>goss_other_rate</code> 0.1 Fraction of remaining samples to keep <p>Result: Train on ~28% of samples with similar accuracy.</p>"},{"location":"user-guide/training/large-scale/#memory-mapped-arrays","title":"Memory-Mapped Arrays","text":"<p>For datasets larger than RAM:</p> <pre><code>import openboost as ob\n\n# Create memory-mapped binned array (saves to disk)\nX_mmap = ob.create_memmap_binned('large_data.npy', X_large)\n\n# Load for training (no copy, uses disk)\nX_mmap = ob.load_memmap_binned('large_data.npy', n_features, n_samples)\n\n# Train as normal\nmodel = ob.GradientBoosting(n_trees=100)\nmodel.fit(X_mmap, y_train)\n</code></pre>"},{"location":"user-guide/training/large-scale/#mini-batch-training","title":"Mini-Batch Training","text":"<p>Accumulate histograms in batches:</p> <pre><code>from openboost import MiniBatchIterator, accumulate_histograms_minibatch\n\n# Process 100k samples at a time\nhist_grad, hist_hess = accumulate_histograms_minibatch(\n    X_mmap, grad, hess,\n    batch_size=100_000,\n    n_features=n_features,\n)\n</code></pre>"},{"location":"user-guide/training/large-scale/#multi-gpu-training","title":"Multi-GPU Training","text":"<p>Distribute training across multiple GPUs:</p> <pre><code>import openboost as ob\n\n# Automatic multi-GPU with Ray\nmodel = ob.GradientBoosting(n_trees=100, n_gpus=4)\nmodel.fit(X, y)\n\n# Or specify exact devices\nmodel = ob.GradientBoosting(n_trees=100, devices=[0, 2])\nmodel.fit(X, y)\n</code></pre>"},{"location":"user-guide/training/large-scale/#requirements","title":"Requirements","text":"<pre><code>pip install \"openboost[distributed]\"  # Installs Ray\n</code></pre>"},{"location":"user-guide/training/large-scale/#scaling-guidelines","title":"Scaling Guidelines","text":"Dataset Size Recommendation &lt;100K samples Standard training 100K-1M GOSS sampling 1M-10M GOSS + memory-mapped &gt;10M Multi-GPU + GOSS"},{"location":"user-guide/training/large-scale/#example-large-dataset","title":"Example: Large Dataset","text":"<pre><code>import numpy as np\nimport openboost as ob\n\n# Simulate large dataset (10M samples)\nn_samples = 10_000_000\nn_features = 100\n\n# Create memory-mapped data\nX_mmap = ob.create_memmap_binned(\n    'large_X.npy',\n    np.random.randn(n_samples, n_features).astype(np.float32)\n)\n\ny = np.random.randn(n_samples).astype(np.float32)\n\n# Train with GOSS\nmodel = ob.GradientBoosting(\n    n_trees=100,\n    subsample_strategy='goss',\n    goss_top_rate=0.1,\n    goss_other_rate=0.05,\n)\nmodel.fit(X_mmap, y)\n</code></pre>"}]}